{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIT_WIDTH = 8\n",
    "EPOCHS = 80\n",
    "Q_EPOCHS = 25\n",
    "BATCH_SIZE = 256\n",
    "MODEL_NAME = 'model'\n",
    "MINI = False\n",
    "FIT_MODEL = True\n",
    "MODEL_STRUCTURE = 2 # 0: short, 1: medium, 2: long\n",
    "mini = \"_mini\" if MINI else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=5, flat=True)\n",
    "data_zoom = mnist.MNISTData(size_initial=20, size_final=8, color_depth=8, flat=True)\n",
    "# todo: data_zoom.x_train = data_zoom.x_train*(2**(BIT_WIDTH))\n",
    "# todo: data_zoom.x_test = data_zoom.x_test*(2**(BIT_WIDTH))\n",
    "\n",
    "# x_train= data_zoom.x_train\n",
    "# y_train= data_zoom.y_train\n",
    "# x_test= data_zoom.x_test\n",
    "# y_test= data_zoom.y_test\n",
    "\n",
    "ax = plt.subplot(1, 1 , 1)\n",
    "plt.imshow(data_zoom.x_train[0].reshape(size_final,size_final), cmap='gray_r')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from callbacks import all_callbacks\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import os \n",
    "# import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class QAutoencoder:\n",
    "    def __init__(self, data: mnist.MNISTData, bit_width=8, EPOCHS=1, Q_EPOCHS=1, model_name='model'):\n",
    "        self.x_train = data.x_train\n",
    "        self.y_train = data.y_train\n",
    "        self.x_test = data.x_test\n",
    "        self.y_test = data.y_test\n",
    "        self.input_min = np.min(data.x_train)\n",
    "        self.input_max = np.max(data.x_train)\n",
    "        self.input_shape = (data.x_train.shape[-1],)\n",
    "        self.BIT_WIDTH = bit_width\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.Q_EPOCHS = Q_EPOCHS\n",
    "        self.MODEL_NAME = model_name\n",
    "        self.model = self.autoencoder_model_gen()\n",
    "        self.history = None\n",
    "        self.loss = None\n",
    "        self.float_model_predictions = None\n",
    "        self.path_to_model =    None\n",
    "\n",
    "        self.path_to_quantized_model = None\n",
    "        self.q_aware_model = None\n",
    "        self.q_aware_loss = None\n",
    "        self.quantized_tflite_model = None\n",
    "        self.interpreter = None\n",
    "        self.input_details = None\n",
    "        self.output_details = None\n",
    "        self.quantized_model_predictions = None\n",
    "    \n",
    "    def mini_model(self):\n",
    "        encoder_input = Input(shape=self.input_shape)\n",
    "        decoder_output = Dense(self.x_train.shape[-1], activation='linear')(encoder_input)\n",
    "        # Model\n",
    "        model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "        # refactor the code above to use the functional AP\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # model.compile(optimizer='adam', loss='binary_crossentropy') # classifier\n",
    "        return model\n",
    "    \n",
    "    def autoencoder_model_gen(self):\n",
    "        if MINI:\n",
    "            return self.mini_model()\n",
    "        # Encoder\n",
    "        # encoder_input = Input(shape=self.input_shape)\n",
    "        # encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "        # encoder_l2 = Dense(32, activation='relu')(encoder_l1)\n",
    "        # encoder_l3 = Dense(16, activation='relu')(encoder_l2)\n",
    "        # encoder_output = Dense(2, activation='relu')(encoder_l3)\n",
    "        \n",
    "        # # Decoder\n",
    "        # decoder_l1 = Dense(16, activation='relu')(encoder_output)\n",
    "        # decoder_l2 = Dense(32, activation='relu')(decoder_l1)\n",
    "        # decoder_l3 = Dense(32, activation='relu')(decoder_l2)\n",
    "        if MODEL_STRUCTURE == 0:\n",
    "            # Encoder\n",
    "            encoder_input = Input(shape=self.input_shape)\n",
    "            encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "            encoder_l3 = Dense(15, activation='relu')(encoder_l2)\n",
    "            encoder_output = Dense(2, activation='relu')(encoder_l4)\n",
    "\n",
    "            # Decoder\n",
    "            # decoder_l1 = Dense(16, activation='relu')(encoder_output)\n",
    "            decoder_l2 = Dense(32, activation='relu')(encoder_output)\n",
    "            # decoder_output = Dense(y_train.shape[-1], activation='sigmoid')(decoder_l3) # classifier\n",
    "            # decoder_output = Dense(\n",
    "            #     self.x_train.shape[-1], activation='sigmoid')(decoder_l3)  # autoencoder\n",
    "            decoder_output = Dense(self.x_train.shape[-1], activation='linear')(decoder_l3)\n",
    "\n",
    "        if MODEL_STRUCTURE == 1:\n",
    "            # Encoder\n",
    "            encoder_input = Input(shape=self.input_shape)\n",
    "            encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "            encoder_l2 = Dense(15, activation='relu')(encoder_l1)\n",
    "            encoder_l3 = Dense(15, activation='relu')(encoder_l2)\n",
    "            encoder_l4 = Dense(15, activation='relu')(encoder_l3)\n",
    "            encoder_output = Dense(2, activation='relu')(encoder_l4)\n",
    "\n",
    "            # Decoder\n",
    "            # decoder_l1 = Dense(16, activation='relu')(encoder_output)\n",
    "            decoder_l2 = Dense(32, activation='relu')(encoder_output)\n",
    "            decoder_l3 = Dense(32, activation='relu')(decoder_l2)\n",
    "            # decoder_output = Dense(y_train.shape[-1], activation='sigmoid')(decoder_l3) # classifier\n",
    "            # decoder_output = Dense(\n",
    "            #     self.x_train.shape[-1], activation='sigmoid')(decoder_l3)  # autoencoder\n",
    "            decoder_output = Dense(self.x_train.shape[-1], activation='linear')(decoder_l3)\n",
    "\n",
    "        if MODEL_STRUCTURE == 2:\n",
    "            # Encoder\n",
    "            encoder_input = Input(shape=self.input_shape)\n",
    "            encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "            encoder_l2 = Dense(50, activation='relu')(encoder_l1)\n",
    "            encoder_l3 = Dense(32, activation='relu')(encoder_l2)\n",
    "            encoder_l4 = Dense(16, activation='relu')(encoder_l3)\n",
    "            encoder_l5 = Dense(8, activation='relu')(encoder_l4)\n",
    "            encoder_l6 = Dense(4, activation='relu')(encoder_l5)\n",
    "            encoder_output = Dense(2, activation='relu')(encoder_l6)\n",
    "\n",
    "            # Decoder\n",
    "            decoder_l1 = Dense(16, activation='relu')(encoder_output)\n",
    "            decoder_l2 = Dense(32, activation='relu')(decoder_l1)\n",
    "            decoder_l3 = Dense(32, activation='relu')(decoder_l2)\n",
    "            # decoder_output = Dense(y_train.shape[-1], activation='sigmoid')(decoder_l3) # classifier\n",
    "            # decoder_output = Dense(\n",
    "            #     self.x_train.shape[-1], activation='sigmoid')(decoder_l3)  # autoencoder\n",
    "            decoder_output = Dense(self.x_train.shape[-1], activation='linear')(decoder_l3)\n",
    "\n",
    "        # Model\n",
    "        model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "        # refactor the code above to use the functional AP\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # model.compile(optimizer='adam', loss='binary_crossentropy') # classifier\n",
    "        return model\n",
    "\n",
    "    def fit_data(self, batch_size=256, epochs=EPOCHS):\n",
    "        \"\"\"Write the fit function for the autoencoder. \n",
    "        Storing the fit history in self.history to be able to plot the fitting scores.\"\"\"\n",
    "\n",
    "        callbacks = all_callbacks(stop_patience=1000,\n",
    "                                  lr_factor=0.5,\n",
    "                                  lr_patience=10,\n",
    "                                  lr_epsilon=0.000001,\n",
    "                                  # min_delta=0.000001,\n",
    "                                  lr_cooldown=2,\n",
    "                                  lr_minimum=0.0000001,\n",
    "                                outputDir=f'model{mini}/callbacks')\n",
    "\n",
    "        self.history = self.model.fit(self.x_train, self.x_train,\n",
    "                                      validation_data=(\n",
    "                                          self.x_test, self.x_test),\n",
    "                                      batch_size=batch_size, epochs=epochs,\n",
    "                                      shuffle=True, callbacks=callbacks.callbacks)\n",
    "        # self.model = strip_pruning(self.model)\n",
    "        self.loss = self.model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "        self.path_to_model =    f'model{mini}/model_{self.loss}loss/KERAS_check_best_model{mini}.model'\n",
    "        if not os.path.exists(self.path_to_model):\n",
    "            os.makedirs(self.path_to_model)\n",
    "\n",
    "        self.model.save(\n",
    "           self.path_to_model)\n",
    "        self.history = self.history.history\n",
    "        self.convert_to_Q_aware()\n",
    "\n",
    "    def plot_float_model(self, n=6):\n",
    "        \"\"\"Plot the float model\"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        self.float_model_predictions = self.model.predict(self.x_test)\n",
    "        self.plot_model_predictions(\n",
    "            n,\n",
    "            self.float_model_predictions,\n",
    "            f'./images/float_model/reconstructed images {self.MODEL_NAME}{mini}.png',\n",
    "        )\n",
    "\n",
    "    def representative_dataset(self):\n",
    "        for data in self.x_train:\n",
    "            # Scale the data using min and max values\n",
    "            scaled_data = (data - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))\n",
    "            yield [np.array([scaled_data], dtype=np.float32)]\n",
    "\n",
    "    def convert_to_Q_aware(self):\n",
    "        quantize_model = tfmot.quantization.keras.quantize_model\n",
    "        self.q_aware_model = quantize_model(self.model)\n",
    "        self.q_aware_model.compile(optimizer='adam', loss='mse')\n",
    "        self.fit_data_Q_aware()\n",
    "        # loss = self.q_aware_model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "\n",
    "        # # Save the quantized model\n",
    "        # with open(f'quantized_model.tflite', 'wb') as f:\n",
    "        #     f.write(self.q_aware_model)\n",
    "\n",
    "        self.convert_to_tflite()\n",
    "\n",
    "    def fit_data_Q_aware(self, batch_size=256, epochs=Q_EPOCHS):\n",
    "        \"\"\"Write the fit function for the autoencoder. \n",
    "        Storing the fit history in self.history to be able to plot the fitting scores.\"\"\"\n",
    "        \n",
    "        self.path_to_quantized_model = f'model{mini}/model_{self.loss}loss/QAE_model{self.BIT_WIDTH}bits/KERAS_check_best_model{mini}.model'\n",
    "        \n",
    "        if not os.path.exists(self.path_to_quantized_model):\n",
    "            os.makedirs(self.path_to_quantized_model)\n",
    "\n",
    "        callbacks = all_callbacks(stop_patience=1000,\n",
    "                                  lr_factor=0.5,\n",
    "                                  lr_patience=10,\n",
    "                                  lr_epsilon=0.000001,\n",
    "                                  # min_delta=0.000001,\n",
    "                                  lr_cooldown=2,\n",
    "                                  lr_minimum=0.0000001,\n",
    "                                  outputDir=f'model{mini}/model_{self.loss}loss/QAE_model{self.BIT_WIDTH}bits/callbacks')\n",
    "        # callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "\n",
    "        self.history_Q_aware = self.q_aware_model.fit(self.x_train, self.x_train,\n",
    "                                      validation_data=(\n",
    "                                          self.x_test, self.x_test),\n",
    "                                      batch_size=batch_size, epochs=epochs,\n",
    "                                      shuffle=True, callbacks=callbacks.callbacks)\n",
    "        # self.model = strip_pruning(self.model)\n",
    "\n",
    "        self.q_aware_model.save(\n",
    "            self.path_to_quantized_model)\n",
    "        self.history_Q_aware = self.history_Q_aware.history\n",
    "        self.q_aware_loss = self.q_aware_model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "        # self.convert_to_Q_aware()    \n",
    "\n",
    "    def convert_to_tflite(self):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(self.q_aware_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = self.representative_dataset\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "        self.quantized_tflite_model = converter.convert()\n",
    "\n",
    "        # Save the TensorFlow Lite model\n",
    "        path_to_tflite_model = f'model{mini}/model_{self.loss}loss/model_obj_tflite_model{mini}'\n",
    "        if not os.path.exists(path_to_tflite_model):\n",
    "            os.makedirs(path_to_tflite_model)\n",
    "        with open(f\"{path_to_tflite_model}.tflite\", \"wb\") as file:\n",
    "            file.write(self.quantized_tflite_model)\n",
    "\n",
    "        # Load the quantized model\n",
    "        self.interpreter = tf.lite.Interpreter(model_content=self.quantized_tflite_model)\n",
    "\n",
    "        self.interpreter.allocate_tensors()\n",
    "\n",
    "        # Get input and output details\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        self.quantized_predictions()\n",
    "\n",
    "    def quantized_predictions(self, n=50):\n",
    "        quantized_model_predictions = []\n",
    "\n",
    "        for i in range(n):\n",
    "            # Prepare input data\n",
    "            # input_data = np.array(\n",
    "            #     [self.x_test[i]*(2**(self.BIT_WIDTH-1))], dtype=np.int8)\n",
    "            # input_data = np.array([(self.x_test[i] - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))], dtype=np.int8)  \n",
    "            input_data = np.array([(self.x_test[i] - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH))], dtype=np.int8)  \n",
    "\n",
    "            self.interpreter.set_tensor(\n",
    "                self.input_details[0]['index'], input_data)\n",
    "            # print(f\"input_data: {input_data}\")\n",
    "\n",
    "            # Run inference\n",
    "            self.interpreter.invoke()\n",
    "\n",
    "            # Get output\n",
    "            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "            output_data = self.interpreter.get_tensor(\n",
    "                # self.output_details[0]['index']) / (2 ** (self.BIT_WIDTH - 1))\n",
    "                self.output_details[0]['index'])\n",
    "            # output_data = output_data * (self.input_max - self.input_min) / (2 ** (self.BIT_WIDTH - 1)) + self.input_min\n",
    "            # output_data = output_data / (2 ** (self.BIT_WIDTH - 1))\n",
    "            quantized_model_predictions.append(output_data)\n",
    "            # print(f\"output_data: {output_data}\")\n",
    "        \n",
    "        self.quantized_model_predictions = quantized_model_predictions\n",
    "        # self.compute_mse()\n",
    "\n",
    "    def quantized_predictions2(self, n=50):\n",
    "        quantized_model_predictions = []\n",
    "\n",
    "        for i in range(n):\n",
    "            # Prepare input data\n",
    "            # input_data = np.array(\n",
    "            #     [self.x_test[i]*(2**(self.BIT_WIDTH-1))], dtype=np.int8)\n",
    "            # input_data = np.array([(self.x_test[i] - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))], dtype=np.int8)  \n",
    "            input_data = np.array([(self.x_test[i] - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH))], dtype=np.int64)  \n",
    "\n",
    "            self.interpreter.set_tensor(\n",
    "                self.input_details[0]['index'], input_data)\n",
    "            # print(f\"input_data: {input_data}\")\n",
    "\n",
    "            # Run inference\n",
    "            self.interpreter.invoke()\n",
    "\n",
    "            # Get output\n",
    "            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "            output_data = self.interpreter.get_tensor(\n",
    "                # self.output_details[0]['index']) / (2 ** (self.BIT_WIDTH - 1))\n",
    "                self.output_details[0]['index'])\n",
    "            # output_data = output_data * (self.input_max - self.input_min) / (2 ** (self.BIT_WIDTH - 1)) + self.input_min\n",
    "            # output_data = output_data / (2 ** (self.BIT_WIDTH - 1))\n",
    "            quantized_model_predictions.append(output_data)\n",
    "            # print(f\"output_data: {output_data}\")\n",
    "        \n",
    "        self.quantized_model_predictions = quantized_model_predictions\n",
    "        # self.compute_mse()\n",
    "\n",
    "    def plot_quantized_model(self, n=6):\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        self.plot_model_predictions(\n",
    "            n,\n",
    "            self.quantized_model_predictions,\n",
    "            f'./images/QAE/reconstructed images{self.MODEL_NAME}{mini}.png',\n",
    "        )\n",
    "\n",
    "    def plot_model_predictions(self, n, quantized_model_predictions, imgs_path):\n",
    "        img_size = int(np.sqrt(self.input_shape[0]))\n",
    "        for i in range(n):\n",
    "            ax = plt.subplot(2, n, i + 1)\n",
    "            self.plot_imgs(\n",
    "                self.x_test, i, img_size, ax\n",
    "            )\n",
    "            ax = plt.subplot(2, n, i + n + 1)\n",
    "            self.plot_imgs(\n",
    "                quantized_model_predictions, i, img_size, ax\n",
    "            )\n",
    "        # if not os.path.exists(imgs_path):\n",
    "        #     os.makedirs(imgs_path)\n",
    "        plt.savefig(imgs_path.format(model_name=\" complete\"))\n",
    "        plt.show()\n",
    "\n",
    "    def plot_imgs(self, arg0, i, img_size, ax):\n",
    "        plt.imshow(arg0[i].reshape(img_size, img_size), cmap='gray_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    def compute_mse(self):\n",
    "        mse = mean_squared_error(self.float_model_predictions, self.quantized_model_predictions)\n",
    "        print(f'Mean Squared Error between floating point and quantized model predictions: {mse}')\n",
    "        self.mse = mse\n",
    "\n",
    "model_obj = QAutoencoder(data_zoom, bit_width=BIT_WIDTH, model_name=MODEL_NAME, EPOCHS=EPOCHS, Q_EPOCHS=Q_EPOCHS)\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32) # classifier\n",
    "model_obj.model.summary()\n",
    "# model.fit(x_train, x_train, epochs=EPOCHS, batch_size=BATCH_SIZE) #autoencoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_MODEL:\n",
    "    model_obj.fit_data(epochs=EPOCHS)  # batch_size=BATCH_SIZE, epochs=EPOCHS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_MODEL:\n",
    "    print(model_obj.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.plot_float_model()\n",
    "# model_obj.plot_quantized_model(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# MODEL_DIR = os.path.abspath(f\"model{mini}\")\n",
    "# # ASSETS_DIR = os.path.join(MODEL_DIR, \"assets\")\n",
    "\n",
    "# # if not os.path.exists(MODEL_DIR):\n",
    "# #     os.makedirs(MODEL_DIR)\n",
    "\n",
    "# # if not os.path.exists(ASSETS_DIR):\n",
    "# #     os.makedirs(ASSETS_DIR)\n",
    "# MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.path_to_model.split('/')[:-1]\n",
    "# join model path\n",
    "'/'.join(model_obj.path_to_model.split('/')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/'.join(model_obj.path_to_model.split('/')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def save_objects(model_obj):\n",
    "    save_path = '/'.join(model_obj.path_to_model.split('/')[:-1])\n",
    "    # # Save the Keras model\n",
    "    # model_obj.model.save(f'{model_obj.path_to_model}')\n",
    "\n",
    "    # # Save the .tflite model\n",
    "    # with open(f\"{model_obj.path_to_model}/model_obj_tflite_model{mini}.tflite\", \"wb\") as file:\n",
    "    #     file.write(model_obj.quantized_tflite_model)\n",
    "    # Save other attributes\n",
    "    attrs_to_save = {\n",
    "        'x_train': model_obj.x_train,\n",
    "        # 'y_train': model_obj.y_train,\n",
    "        'x_test': model_obj.x_test,\n",
    "        # 'y_test': model_obj.y_test,\n",
    "        'input_min': model_obj.input_min,\n",
    "        'input_max': model_obj.input_max,\n",
    "        'input_shape': model_obj.input_shape,\n",
    "        'BIT_WIDTH': model_obj.BIT_WIDTH,\n",
    "        'EPOCHS': model_obj.EPOCHS,\n",
    "        'Q_EPOCHS': model_obj.Q_EPOCHS,\n",
    "        'MODEL_NAME': model_obj.MODEL_NAME,\n",
    "        'history': model_obj.history,\n",
    "        'loss': model_obj.loss,\n",
    "        'float_model_predictions': model_obj.float_model_predictions,\n",
    "        'path_to_model': model_obj.path_to_model,\n",
    "\n",
    "        'path_to_quantized_model': model_obj.path_to_quantized_model,\n",
    "        # 'q_aware_model': model_obj.q_aware_model,\n",
    "        'q_aware_loss': model_obj.q_aware_loss,\n",
    "        'quantized_tflite_model': model_obj.quantized_tflite_model,\n",
    "        # 'interpreter': model_obj.interpreter,\n",
    "        # 'input_details': model_obj.input_details,\n",
    "        # 'output_details': model_obj.output_details,\n",
    "        'quantized_model_predictions': model_obj.quantized_model_predictions\n",
    "        # 'history_Q_aware': model_obj.history_Q_aware,\n",
    "        # 'mse': model_obj.mse,\n",
    "    }\n",
    "    with open(f'{save_path}/model_obj_attributes{mini}.pickle', 'wb') as f:\n",
    "        pickle.dump(attrs_to_save, f)\n",
    "\n",
    "\n",
    "# Save the objects of model_obj\n",
    "if FIT_MODEL:\n",
    "    save_objects(model_obj)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load it "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIT_WIDTH = 8\n",
    "# EPOCHS = 80\n",
    "# Q_EPOCHS = 40\n",
    "# BATCH_SIZE = 256\n",
    "# MODEL_NAME = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all folders in the given directory\n",
    "import os\n",
    "\n",
    "def get_best_model_path(path):\n",
    "    if directories := [\n",
    "    d\n",
    "    for d in os.listdir(path)\n",
    "    if d.startswith('model_') and d.endswith('loss')\n",
    "]:\n",
    "    # Extract loss value from each directory name\n",
    "        losses = [float(d.split('_')[1].replace('loss', '')) for d in directories]\n",
    "\n",
    "    # Select directory with lowest loss value\n",
    "        lowest_loss_index = losses.index(min(losses))\n",
    "        lowest_loss_dir = directories[lowest_loss_index]\n",
    "        print(f\"The directory with the lowest loss is {lowest_loss_dir}\")\n",
    "        return f\"{path}/{lowest_loss_dir}\"\n",
    "\n",
    "    else:\n",
    "        print(\"No directories found with name pattern 'model_*loss'\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "path = f'./model{mini}'  # Replace with the path to the directory you want to list    \n",
    "MODEL_DIR = get_best_model_path(path)\n",
    "\n",
    "MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{MODEL_DIR}/KERAS_check_best_model{mini}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_objects():\n",
    "    \n",
    "    # model_obj.model.save('model_obj_keras_model.h5')\n",
    "    # Load the TensorFlow model\n",
    "    loaded_model = tf.keras.models.load_model(f\"{MODEL_DIR}/KERAS_check_best_model{mini}.model\")\n",
    "\n",
    "    # Load the Keras model\n",
    "    # loaded_keras_model = tf.keras.models.load_model(f'{MODEL_DIR}/model_obj_keras_model{mini}.h5')\n",
    "\n",
    "    # Load the .tflite model\n",
    "    with open(f\"{MODEL_DIR}/model_obj_tflite_model{mini}.tflite\", \"rb\") as file:\n",
    "        loaded_tflite_model = file.read()\n",
    "\n",
    "    # Load other attributes\n",
    "    with open(f'{MODEL_DIR}/model_obj_attributes{mini}.pickle', 'rb') as f:\n",
    "        loaded_attributes = pickle.load(f)\n",
    "    \n",
    "    # Create a new QAutoencoder object and set the attributes\n",
    "    loaded_model_obj = QAutoencoder(data_zoom, bit_width=loaded_attributes['BIT_WIDTH'], model_name=loaded_attributes['MODEL_NAME'],\n",
    "                                    EPOCHS=loaded_attributes['EPOCHS'], Q_EPOCHS=loaded_attributes['Q_EPOCHS'])\n",
    "    \n",
    "    loaded_model_obj.model = loaded_model\n",
    "    # loaded_model_obj.model = loaded_keras_model\n",
    "    loaded_model_obj.quantized_tflite_model = loaded_tflite_model\n",
    "\n",
    "    loaded_model_obj.x_train = loaded_attributes['x_train']\n",
    "    # loaded_model_obj.y_train = loaded_attributes['y_train']\n",
    "    loaded_model_obj.x_test = loaded_attributes['x_test']\n",
    "    # loaded_model_obj.y_test = loaded_attributes['y_test']\n",
    "    loaded_model_obj.input_min = loaded_attributes['input_min']\n",
    "    loaded_model_obj.input_max = loaded_attributes['input_max']\n",
    "    loaded_model_obj.input_shape = loaded_attributes['input_shape']\n",
    "    loaded_model_obj.BIT_WIDTH = loaded_attributes['BIT_WIDTH']\n",
    "    loaded_model_obj.EPOCHS = loaded_attributes['EPOCHS']\n",
    "    loaded_model_obj.Q_EPOCHS = loaded_attributes['Q_EPOCHS']\n",
    "    loaded_model_obj.MODEL_NAME = loaded_attributes['MODEL_NAME']\n",
    "    loaded_model_obj.history = loaded_attributes['history']\n",
    "    loaded_model_obj.loss = loaded_attributes['loss']\n",
    "    loaded_model_obj.float_model_predictions = loaded_attributes['float_model_predictions']\n",
    "    loaded_model_obj.path_to_model = loaded_attributes['path_to_model']\n",
    "\n",
    "    loaded_model_obj.path_to_quantized_model = loaded_attributes['path_to_quantized_model']\n",
    "    # loaded_model_obj.q_aware_model = loaded_attributes['q_aware_model']\n",
    "    loaded_model_obj.q_aware_loss = loaded_attributes['q_aware_loss']\n",
    "    loaded_model_obj.quantized_tflite_model = loaded_attributes['quantized_tflite_model']\n",
    "    # loaded_model_obj.interpreter = loaded_attributes['interpreter']\n",
    "    # loaded_model_obj.input_details = loaded_attributes['input_details']\n",
    "    # loaded_model_obj.output_details = loaded_attributes['output_details']\n",
    "    loaded_model_obj.quantized_model_predictions = loaded_attributes['quantized_model_predictions']\n",
    "    \n",
    "    # loaded_model_obj.interpreter = loaded_tflite_model\n",
    "    # loaded_model_obj.mse = loaded_attributes['mse']\n",
    "\n",
    "    # Recreate the TFLite interpreter\n",
    "    loaded_model_obj.interpreter = tf.lite.Interpreter(model_content=loaded_tflite_model)\n",
    "    loaded_model_obj.interpreter.allocate_tensors()\n",
    "    loaded_model_obj.input_details = loaded_model_obj.interpreter.get_input_details()\n",
    "    loaded_model_obj.output_details = loaded_model_obj.interpreter.get_output_details()\n",
    "    # loaded_model_obj.convert_to_Q_aware()\n",
    "\n",
    "    return loaded_model_obj\n",
    "\n",
    "# # Load the objects and create a new model_obj\n",
    "# loaded_model_obj = load_objects()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the objects and create a new model_obj\n",
    "loaded_model_obj = load_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.plot_float_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.quantized_predictions(n=20)\n",
    "loaded_model_obj.plot_quantized_model(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.quantized_predictions2(n=20)\n",
    "# loaded_model_obj.plot_quantized_model(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.quantized_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, img in enumerate(loaded_model_obj.quantized_model_predictions):\n",
    "#     print(f' ------------ Image {i} ------------')\n",
    "#     for j, row in enumerate(img):\n",
    "#         for k, pixel in enumerate(row):\n",
    "#             print(pixel, \" : \", Fxp(pixel, signed=is_signed, n_word=BIT_WIDTH, n_frac=0).bin())\n",
    "#             # print(Fxp(pixel, signed=is_signed, n_word=BIT_WIDTH, n_frac=0).bin())\n",
    "#             print(' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fxpmath import Fxp\n",
    "# BIT_WIDTH = 8\n",
    "is_signed = True\n",
    "TB_FILES_DIR = 'model/testbench_files'\n",
    "if not os.path.exists(TB_FILES_DIR):\n",
    "    os.makedirs(TB_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_predictions = []\n",
    "pred_len = len(loaded_model_obj.quantized_model_predictions)\n",
    "\n",
    "with open('model/testbench_files/inputs_string.txt', 'w') as f:\n",
    "\n",
    "    for img in range(pred_len):\n",
    "        # Prepare input data\n",
    "        input_data = np.array([(loaded_model_obj.x_test[img] - loaded_model_obj.input_min) / (loaded_model_obj.input_max - loaded_model_obj.input_min) * (2 ** (loaded_model_obj.BIT_WIDTH))], dtype=np.int8)\n",
    "\n",
    "        for pixel in input_data[0]:\n",
    "            fxp_item = Fxp(pixel, signed=True, n_word=loaded_model_obj.BIT_WIDTH, n_frac=0)\n",
    "            # print(pixel, \" = \", fxp_item, \" : \", fxp_item.bin())\n",
    "\n",
    "            f.write(fxp_item.bin())\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # print(f\"input_data: {input_data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  generating expected results for testbench\n",
    "with open(f'{TB_FILES_DIR}/expected_results.txt', 'w') as f:\n",
    "    for img in loaded_model_obj.quantized_model_predictions:\n",
    "        for row in img:\n",
    "            for pixel in row:\n",
    "                # print(pixel, \" : \", Fxp(pixel, signed=is_signed, n_word=BIT_WIDTH, n_frac=0).bin())\n",
    "                f.write(Fxp(pixel, signed=is_signed, n_word=loaded_model_obj.BIT_WIDTH, n_frac=0).bin())\n",
    "        # print(\" \")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  generating expected results for testbench\n",
    "with open(f'{TB_FILES_DIR}/expected_results_dec.txt', 'w') as f:\n",
    "    for img in loaded_model_obj.quantized_model_predictions:\n",
    "        for row in img:\n",
    "            for pixel in row:\n",
    "                # print(pixel, \" : \", Fxp(pixel, signed=is_signed, n_word=BIT_WIDTH, n_frac=0).bin())\n",
    "                f.write(f\"{str(pixel)} \")\n",
    "        # print(\" \")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.quantized_tflite_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone and fine-tune pre-trained model with quantization aware training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_model_optimization as tfmot\n",
    "# model = model_obj.model\n",
    "# quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# # q_aware stands for for quantization aware.\n",
    "# q_aware_model = quantize_model(model)\n",
    "\n",
    "# # `quantize_model` requires a recompile.\n",
    "# # q_aware_model.compile(optimizer='adam',\n",
    "# #               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "# #               metrics=['accuracy'])\n",
    "# q_aware_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# q_aware_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model against baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.x_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_subset = model_obj.x_train[0:1000] # out of 60000\n",
    "# train_labels_subset = model_obj.x_train[0:1000]\n",
    "\n",
    "# q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "#                   batch_size=500, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model_accuracy = model_obj.model.evaluate(\n",
    "#     model_obj.x_test, model_obj.x_test, verbose=0)\n",
    "\n",
    "# q_aware_model_accuracy = model_obj.q_aware_model.evaluate(\n",
    "#    model_obj.x_test, model_obj.x_test, verbose=0)\n",
    "\n",
    "# print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "# print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create quantized model for TFLite backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, you have an actually quantized model with int8 weights and uint8 activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model_obj.q_aware_model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# q_aware_model = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See persistence of accuracy from TF to TFLite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to evaluate the TF Lite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def evaluate_model(interpreter, x_test, n=6):\n",
    "# #   input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "# #   output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "#     input_details = interpreter.get_input_details()\n",
    "#     output_details = interpreter.get_output_details()\n",
    "\n",
    "# #   # Run predictions on every image in the \"test\" dataset.\n",
    "# #   prediction_digits = []\n",
    "# #   for i, test_image in enumerate(test_images):\n",
    "# #     if i % 1000 == 0:\n",
    "# #       print('Evaluated on {n} results so far.'.format(n=i))\n",
    "# #     # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "# #     # the model's input data format.\n",
    "# #     test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "# #     interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "# #     # Run inference.\n",
    "# #     interpreter.invoke()\n",
    "\n",
    "# #     # Post-processing: remove batch dimension and find the digit with highest\n",
    "# #     # probability.\n",
    "# #     output = interpreter.tensor(output_index)\n",
    "# #     digit = np.argmax(output()[0])\n",
    "# #     prediction_digits.append(digit)\n",
    "\n",
    "# #   print('\\n')\n",
    "# #   # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "# #   prediction_digits = np.array(prediction_digits)\n",
    "# #   accuracy = (prediction_digits == test_labels).mean()\n",
    "# #   return accuracy\n",
    "\n",
    "#     quantized_model_predictions = []\n",
    "\n",
    "#     for i in range(n):\n",
    "#         # Prepare input data\n",
    "#         # input_data = np.array(\n",
    "#         #     [x_test[i]*(2**(BIT_WIDTH-1))], dtype=np.int8)\n",
    "#         # input_data = np.array([(x_test[i] - input_min) / (input_max - input_min) * (2 ** (BIT_WIDTH - 1))], dtype=np.int8)  \n",
    "#         input_data = np.expand_dims(x_test[i], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "#         interpreter.set_tensor(\n",
    "#             input_details[0]['index'], input_data)\n",
    "#         # print(f\"input_data: {input_data}\")\n",
    "#         # Run inference\n",
    "#         interpreter.invoke()\n",
    "\n",
    "#         # Get output\n",
    "#         output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "#         # output_data = interpreter.get_tensor(\n",
    "#         #     output_details[0]['index']) / (2 ** (BIT_WIDTH - 1))\n",
    "#         # output_data = output_data * (input_max - input_min) / (2 ** (BIT_WIDTH - 1)) + input_min\n",
    "#         # output_data = output_data / (2 ** (BIT_WIDTH - 1))\n",
    "#         # print(f\"output_data: {output_data}\")\n",
    "#         quantized_model_predictions.append(output_data)\n",
    "#     accuracy = (input_data == quantized_model_predictions).mean()\n",
    "    \n",
    "#     # quantized_model_predictions = quantized_model_predictions\n",
    "#     return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter = tf.lite.Interpreter(model_content=q_aware_model)\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # test_accuracy = evaluate_model(interpreter, test_images=model_obj.x_test, test_labels=model_obj.x_test)\n",
    "# test_accuracy = evaluate_model(interpreter, model_obj.x_test, n=6)\n",
    "\n",
    "# print('Quant TF test accuracy:', q_aware_model_accuracy)\n",
    "# print('Quant TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.q_aware_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model_obj.q_aware_model.layers:\n",
    "#   if hasattr(layer, 'quantize_config'):\n",
    "#     for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "#         quantized_and_dequantized = quantizer(weight, training=False, weights=quantizer_vars)\n",
    "#         min_var = quantizer_vars['min_var']\n",
    "#         max_var = quantizer_vars['max_var']\n",
    "#         print(quantized_and_dequantized*(2**(BIT_WIDTH)))\n",
    "#         # quantized = dequantize(quantized_and_dequantized, min_var, max_var, quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.quantized_tflite_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.interpreter.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.plot_float_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obj = model_obj\n",
    "# # plot_quantized_model(obj, n=6)\n",
    "# model_obj.plot_quantized_model(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See 4x smaller model from quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create a float TFLite model and then see that the quantized TFLite model\n",
    "is 4x smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create float TFLite model.\n",
    "# float_converter = tf.lite.TFLiteConverter.from_keras_model(model_obj.model)\n",
    "# float_tflite_model = float_converter.convert()\n",
    "\n",
    "# # Measure sizes of models.\n",
    "# _, float_file = tempfile.mkstemp('.tflite')\n",
    "# _, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "# with open(quant_file, 'wb') as f:\n",
    "#   f.write(model_obj.quantized_tflite_model)\n",
    "\n",
    "# with open(float_file, 'wb') as f:\n",
    "#   f.write(float_tflite_model)\n",
    "\n",
    "# print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "# print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))\n",
    "# print(float_converter.get_tensor_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b75f63a51ab1e44c10e89cf3b718812d9c5e2447d39cb402b946ba7653bfcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
