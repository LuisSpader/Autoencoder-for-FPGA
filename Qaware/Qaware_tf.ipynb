{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHA0lEQVR4nO3ZvU5VWQCG4X3wxMaAPYHGjtpWEqOJhVdjZ21p4YV4BV6AtZWFFwChNBHiTwxhT+VbDZnDDmfW6DxPe1bIRwL7ZbFX8zzPEwBM07QzegAA/x2iAEBEAYCIAgARBQAiCgBEFADIepNDV1dX09nZ2bS7uzutVqttbwLgls3zPF1cXEz7+/vTzs7194GNonB2djYdHh7e2jgAxjg5OZkODg6u/XyjKOzu7vbF9vb2bmcZAP+a8/Pz6fDwsOf5dTaKwq9/Ge3t7YkCwG/sn14BeNEMQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAWY8esG1fvnwZPWGRZ8+ejZ6w2MePH0dPWOT79++jJyzy5MmT0RMWefTo0egJi718+XL0hBv79u3bRufcFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCsRw/Ytvv374+esMjr169HT1js+fPnoycsMs/z6AmLfPr0afSERY6OjkZP+F+5vLzc6JybAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACDr0QP4e8fHx6MnLPbw4cPRExb5/Pnz6AmLHB0djZ7AH8RNAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMh69AD+3p07d0ZPWOz9+/ejJyzy9OnT0RMWWa9/z1/jd+/ejZ6w2M7On/v39J/7nQFwY6IAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAsh49AP4rXr16NXrCIsfHx6MnLHJ5eTl6wmJ3794dPWFr3BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGArG9yeJ7naZ7nbW3ZitVqNXrCIqenp6MnLPbgwYPRExb53X62f3n79u3oCYus1zd6/PAvcVMAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAZH2Twx8+fJju3bu3rS1b8fjx49ETFvn58+foCYu9efNm9IRFXrx4MXoCDOemAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFALLe5NA8z9M0TdPXr1+3OmYbfm3/3fyuu6dpmn78+DF6wiLn5+ejJ8DW/Pr5/qdny2re4Olzeno6HR4e3s4yAIY5OTmZDg4Orv18oyhcXV1NZ2dn0+7u7rRarW51IADbN8/zdHFxMe3v7087O9e/OdgoCgD8P3jRDEBEAYCIAgARBQAiCgBEFACIKACQvwDMppXmb6HR+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from class_QAutoencoder import model_obj, data_zoom, size_final\n",
    "from class_QAutoencoder import *\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "plt.imshow(data_zoom.x_train[0].reshape(size_final, size_final), cmap='gray_r')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 15)                975       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 32        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 15)                45        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                1024      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,076\n",
      "Trainable params: 2,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BIT_WIDTH = 8\n",
    "# EPOCHS = 80\n",
    "# Q_EPOCHS = 25\n",
    "EPOCHS = 1\n",
    "Q_EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "MODEL_NAME = 'model'\n",
    "MINI = False\n",
    "FIT_MODEL = True\n",
    "MODEL_SIZE = 0  # 0: short, 1: medium, 2: long, 3: very long\n",
    "mini = \"_mini\" if MINI else \"\"\n",
    "\n",
    "model_obj = QAutoencoder(data_zoom, bit_width=BIT_WIDTH,\n",
    "                         EPOCHS=EPOCHS, Q_EPOCHS=Q_EPOCHS, model_name=MODEL_NAME)\n",
    "\n",
    "model_obj.model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "  1/235 [..............................] - ETA: 2:50 - loss: 0.1770WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0030s). Check your callbacks.\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.1117\n",
      "***callbacks***\n",
      "saving losses to model/callbacks\\losses.log\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08694, saving model to model/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08694, saving model to model/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00001: saving model to model/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00001: saving model to model/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 13ms/step - loss: 0.1097 - val_loss: 0.0869 - lr: 0.0010\n",
      "INFO:tensorflow:Assets written to: model/model_15_2_15_64_0.08694loss/KERAS_check_best_model.model\\assets\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "  1/235 [..............................] - ETA: 8:09 - loss: 0.0865WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0376s). Check your callbacks.\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0849\n",
      "***callbacks***\n",
      "saving losses to model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08346, saving model to model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn, dense_5_layer_call_and_return_conditional_losses, dense_6_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08346, saving model to model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00001: saving model to model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn, dense_5_layer_call_and_return_conditional_losses, dense_6_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to model/model_15_2_15_64_0.08694loss/QAE8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 10s 35ms/step - loss: 0.0849 - val_loss: 0.0835 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn, dense_5_layer_call_and_return_conditional_losses, dense_6_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_15_2_15_64_0.08694loss/QAE8bits/KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_15_2_15_64_0.08694loss/QAE8bits/KERAS_check_best_model.model\\assets\n",
      "WARNING:absl:Found untraced functions such as dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn, dense_5_layer_call_and_return_conditional_losses, dense_6_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmpfohemrvl\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmpfohemrvl\\assets\n"
     ]
    }
   ],
   "source": [
    "if FIT_MODEL:\n",
    "    model_obj.fit_data(epochs=EPOCHS)  # batch_size=BATCH_SIZE, epochs=EPOCHS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08694251626729965"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj.loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load it "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIT_WIDTH = 8\n",
    "# EPOCHS = 80\n",
    "# Q_EPOCHS = 40\n",
    "# BATCH_SIZE = 256\n",
    "# model_name = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory with the lowest loss is model_15_2_15_64_0.08694loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./model/model_15_2_15_64_0.08694loss'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all folders in the given directory\n",
    "import os\n",
    "\n",
    "def get_best_model_path(path):\n",
    "    if directories := [\n",
    "    d\n",
    "    for d in os.listdir(path)\n",
    "    if d.startswith('model_') and d.endswith('loss')\n",
    "]:\n",
    "    # Extract loss value from each directory name\n",
    "        losses = [float(d.split('_')[1].replace('loss', '')) for d in directories]\n",
    "\n",
    "    # Select directory with lowest loss value\n",
    "        lowest_loss_index = losses.index(min(losses))\n",
    "        lowest_loss_dir = directories[lowest_loss_index]\n",
    "        print(f\"The directory with the lowest loss is {lowest_loss_dir}\")\n",
    "        return f\"{path}/{lowest_loss_dir}\"\n",
    "\n",
    "    else:\n",
    "        print(\"No directories found with name pattern 'model_*loss'\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "path = f'./model{mini}'  # Replace with the path to the directory you want to list    \n",
    "MODEL_DIR = get_best_model_path(path)\n",
    "\n",
    "MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/model_15_2_15_64_0.08694loss/KERAS_check_best_model.model'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{MODEL_DIR}/KERAS_check_best_model{mini}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KERAS_check_best_model.model', 'QAE8bits', 'saved_objects']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assets', 'keras_metadata.pb', 'saved_model.pb', 'variables']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f\"{MODEL_DIR}/KERAS_check_best_model{mini}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_objects/tflite_model.tflite.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_objects():\n",
    "    \n",
    "    # model_obj.model.save('model_obj_keras_model.h5')\n",
    "    # Load the TensorFlow model\n",
    "    loaded_model = tf.keras.models.load_model(f\"{MODEL_DIR}/KERAS_check_best_model{mini}.model\")\n",
    "\n",
    "    # Load the Keras model\n",
    "    # loaded_keras_model = tf.keras.models.load_model(f'{MODEL_DIR}/model_obj_keras_model{mini}.h5')\n",
    "\n",
    "    # Load the .tflite model\n",
    "    with open(f\"{MODEL_DIR}/saved_objects{mini}/tflite_model.tflite\", \"rb\") as file:\n",
    "        loaded_tflite_model = file.read()\n",
    "\n",
    "    # Load other attributes\n",
    "    with open(f'{MODEL_DIR}/saved_objects{mini}/model_obj_attributes{mini}.pickle', 'rb') as f:\n",
    "        loaded_attributes = pickle.load(f)\n",
    "    \n",
    "    # Create a new QAutoencoder object and set the attributes\n",
    "    loaded_model_obj = QAutoencoder(data_zoom, bit_width=loaded_attributes['BIT_WIDTH'], model_name=loaded_attributes['model_name'],\n",
    "                                    EPOCHS=loaded_attributes['EPOCHS'], Q_EPOCHS=loaded_attributes['Q_EPOCHS'])\n",
    "    \n",
    "    loaded_model_obj.model = loaded_model\n",
    "    # loaded_model_obj.model = loaded_keras_model\n",
    "    loaded_model_obj.quantized_tflite_model = loaded_tflite_model\n",
    "\n",
    "    loaded_model_obj.x_train = loaded_attributes['x_train']\n",
    "    # loaded_model_obj.y_train = loaded_attributes['y_train']\n",
    "    loaded_model_obj.x_test = loaded_attributes['x_test']\n",
    "    # loaded_model_obj.y_test = loaded_attributes['y_test']\n",
    "    loaded_model_obj.input_min = loaded_attributes['input_min']\n",
    "    loaded_model_obj.input_max = loaded_attributes['input_max']\n",
    "    loaded_model_obj.input_shape = loaded_attributes['input_shape']\n",
    "    loaded_model_obj.BIT_WIDTH = loaded_attributes['BIT_WIDTH']\n",
    "    loaded_model_obj.EPOCHS = loaded_attributes['EPOCHS']\n",
    "    loaded_model_obj.Q_EPOCHS = loaded_attributes['Q_EPOCHS']\n",
    "    loaded_model_obj.model_name = loaded_attributes['model_name']\n",
    "    loaded_model_obj.history = loaded_attributes['history']\n",
    "    loaded_model_obj.loss = loaded_attributes['loss']\n",
    "    loaded_model_obj.float_model_predictions = loaded_attributes['float_model_predictions']\n",
    "    loaded_model_obj.path_to_model = loaded_attributes['path_to_model']\n",
    "\n",
    "    loaded_model_obj.path_to_quantized_model = loaded_attributes['path_to_quantized_model']\n",
    "    # loaded_model_obj.q_aware_model = loaded_attributes['q_aware_model']\n",
    "    loaded_model_obj.q_aware_loss = loaded_attributes['q_aware_loss']\n",
    "    loaded_model_obj.quantized_tflite_model = loaded_attributes['quantized_tflite_model']\n",
    "    # loaded_model_obj.interpreter = loaded_attributes['interpreter']\n",
    "    # loaded_model_obj.input_details = loaded_attributes['input_details']\n",
    "    # loaded_model_obj.output_details = loaded_attributes['output_details']\n",
    "    loaded_model_obj.quantized_model_predictions = loaded_attributes['quantized_model_predictions']\n",
    "    \n",
    "    # loaded_model_obj.interpreter = loaded_tflite_model\n",
    "    # loaded_model_obj.mse = loaded_attributes['mse']\n",
    "\n",
    "    # Recreate the TFLite interpreter\n",
    "    loaded_model_obj.interpreter = tf.lite.Interpreter(model_content=loaded_tflite_model)\n",
    "    loaded_model_obj.interpreter.allocate_tensors()\n",
    "    loaded_model_obj.input_details = loaded_model_obj.interpreter.get_input_details()\n",
    "    loaded_model_obj.output_details = loaded_model_obj.interpreter.get_output_details()\n",
    "    # loaded_model_obj.convert_to_Q_aware()\n",
    "\n",
    "    return loaded_model_obj\n",
    "\n",
    "# # Load the objects and create a new model_obj\n",
    "# loaded_model_obj = load_objects()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/model_15_2_15_64_0.08694loss/model_obj_attributes.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load the objects and create a new model_obj\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loaded_model_obj \u001b[39m=\u001b[39m load_objects()\n",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m, in \u001b[0;36mload_objects\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     loaded_tflite_model \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[0;32m     16\u001b[0m \u001b[39m# Load other attributes\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mMODEL_DIR\u001b[39m}\u001b[39;49;00m\u001b[39m/model_obj_attributes\u001b[39;49m\u001b[39m{\u001b[39;49;00mmini\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     18\u001b[0m     loaded_attributes \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m     20\u001b[0m \u001b[39m# Create a new QAutoencoder object and set the attributes\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/model_15_2_15_64_0.08694loss/model_obj_attributes.pickle'"
     ]
    }
   ],
   "source": [
    "# Load the objects and create a new model_obj\n",
    "loaded_model_obj = load_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.plot_float_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_obj.quantized_predictions(n=20)\n",
    "loaded_model_obj.plot_quantized_model(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.quantized_predictions2(n=20)\n",
    "# loaded_model_obj.plot_quantized_model(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.quantized_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, img in enumerate(loaded_model_obj.quantized_model_predictions):\n",
    "#     print(f' ------------ Image {i} ------------')\n",
    "#     for j, row in enumerate(img):\n",
    "#         for k, pixel in enumerate(row):\n",
    "#             print(pixel, \" : \", Fxp(pixel, signed=is_signed, n_word=BIT_WIDTH, n_frac=0).bin())\n",
    "#             # print(Fxp(pixel, signed=is_signed, n_word=BIT_WIDTH, n_frac=0).bin())\n",
    "#             print(' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fxpmath import Fxp\n",
    "# BIT_WIDTH = 8\n",
    "is_signed = True\n",
    "TB_FILES_DIR = 'model/testbench_files'\n",
    "if not os.path.exists(TB_FILES_DIR):\n",
    "    os.makedirs(TB_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_predictions = []\n",
    "pred_len = len(loaded_model_obj.quantized_model_predictions)\n",
    "\n",
    "with open('model/testbench_files/inputs_string.txt', 'w') as f:\n",
    "\n",
    "    for img in range(pred_len):\n",
    "        # Prepare input data\n",
    "        input_data = np.array([(loaded_model_obj.x_test[img] - loaded_model_obj.input_min) / (loaded_model_obj.input_max - loaded_model_obj.input_min) * (2 ** (loaded_model_obj.BIT_WIDTH))], dtype=np.int8)\n",
    "\n",
    "        for pixel in input_data[0]:\n",
    "            fxp_item = Fxp(pixel, signed=True, n_word=loaded_model_obj.BIT_WIDTH, n_frac=0)\n",
    "            # print(pixel, \" = \", fxp_item, \" : \", fxp_item.bin())\n",
    "\n",
    "            f.write(fxp_item.bin())\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # print(f\"input_data: {input_data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  generating expected results for testbench\n",
    "with open(f'{TB_FILES_DIR}/expected_results.txt', 'w') as f:\n",
    "    for img in loaded_model_obj.quantized_model_predictions:\n",
    "        for row in img:\n",
    "            for pixel in row:\n",
    "                # print(pixel, \" : \", Fxp(pixel, signed=is_signed, n_word=BIT_WIDTH, n_frac=0).bin())\n",
    "                f.write(Fxp(pixel, signed=is_signed, n_word=loaded_model_obj.BIT_WIDTH, n_frac=0).bin())\n",
    "        # print(\" \")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  generating expected results for testbench\n",
    "with open(f'{TB_FILES_DIR}/expected_results_dec.txt', 'w') as f:\n",
    "    for img in loaded_model_obj.quantized_model_predictions:\n",
    "        for row in img:\n",
    "            for pixel in row:\n",
    "                # print(pixel, \" : \", Fxp(pixel, signed=is_signed, n_word=BIT_WIDTH, n_frac=0).bin())\n",
    "                f.write(f\"{str(pixel)} \")\n",
    "        # print(\" \")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.quantized_tflite_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone and fine-tune pre-trained model with quantization aware training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_model_optimization as tfmot\n",
    "# model = model_obj.model\n",
    "# quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# # q_aware stands for for quantization aware.\n",
    "# q_aware_model = quantize_model(model)\n",
    "\n",
    "# # `quantize_model` requires a recompile.\n",
    "# # q_aware_model.compile(optimizer='adam',\n",
    "# #               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "# #               metrics=['accuracy'])\n",
    "# q_aware_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# q_aware_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model against baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.x_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_subset = model_obj.x_train[0:1000] # out of 60000\n",
    "# train_labels_subset = model_obj.x_train[0:1000]\n",
    "\n",
    "# q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "#                   batch_size=500, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model_accuracy = model_obj.model.evaluate(\n",
    "#     model_obj.x_test, model_obj.x_test, verbose=0)\n",
    "\n",
    "# q_aware_model_accuracy = model_obj.q_aware_model.evaluate(\n",
    "#    model_obj.x_test, model_obj.x_test, verbose=0)\n",
    "\n",
    "# print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "# print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create quantized model for TFLite backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, you have an actually quantized model with int8 weights and uint8 activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model_obj.q_aware_model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# q_aware_model = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See persistence of accuracy from TF to TFLite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to evaluate the TF Lite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def evaluate_model(interpreter, x_test, n=6):\n",
    "# #   input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "# #   output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "#     input_details = interpreter.get_input_details()\n",
    "#     output_details = interpreter.get_output_details()\n",
    "\n",
    "# #   # Run predictions on every image in the \"test\" dataset.\n",
    "# #   prediction_digits = []\n",
    "# #   for i, test_image in enumerate(test_images):\n",
    "# #     if i % 1000 == 0:\n",
    "# #       print('Evaluated on {n} results so far.'.format(n=i))\n",
    "# #     # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "# #     # the model's input data format.\n",
    "# #     test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "# #     interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "# #     # Run inference.\n",
    "# #     interpreter.invoke()\n",
    "\n",
    "# #     # Post-processing: remove batch dimension and find the digit with highest\n",
    "# #     # probability.\n",
    "# #     output = interpreter.tensor(output_index)\n",
    "# #     digit = np.argmax(output()[0])\n",
    "# #     prediction_digits.append(digit)\n",
    "\n",
    "# #   print('\\n')\n",
    "# #   # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "# #   prediction_digits = np.array(prediction_digits)\n",
    "# #   accuracy = (prediction_digits == test_labels).mean()\n",
    "# #   return accuracy\n",
    "\n",
    "#     quantized_model_predictions = []\n",
    "\n",
    "#     for i in range(n):\n",
    "#         # Prepare input data\n",
    "#         # input_data = np.array(\n",
    "#         #     [x_test[i]*(2**(BIT_WIDTH-1))], dtype=np.int8)\n",
    "#         # input_data = np.array([(x_test[i] - input_min) / (input_max - input_min) * (2 ** (BIT_WIDTH - 1))], dtype=np.int8)  \n",
    "#         input_data = np.expand_dims(x_test[i], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "#         interpreter.set_tensor(\n",
    "#             input_details[0]['index'], input_data)\n",
    "#         # print(f\"input_data: {input_data}\")\n",
    "#         # Run inference\n",
    "#         interpreter.invoke()\n",
    "\n",
    "#         # Get output\n",
    "#         output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "#         # output_data = interpreter.get_tensor(\n",
    "#         #     output_details[0]['index']) / (2 ** (BIT_WIDTH - 1))\n",
    "#         # output_data = output_data * (input_max - input_min) / (2 ** (BIT_WIDTH - 1)) + input_min\n",
    "#         # output_data = output_data / (2 ** (BIT_WIDTH - 1))\n",
    "#         # print(f\"output_data: {output_data}\")\n",
    "#         quantized_model_predictions.append(output_data)\n",
    "#     accuracy = (input_data == quantized_model_predictions).mean()\n",
    "    \n",
    "#     # quantized_model_predictions = quantized_model_predictions\n",
    "#     return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter = tf.lite.Interpreter(model_content=q_aware_model)\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # test_accuracy = evaluate_model(interpreter, test_images=model_obj.x_test, test_labels=model_obj.x_test)\n",
    "# test_accuracy = evaluate_model(interpreter, model_obj.x_test, n=6)\n",
    "\n",
    "# print('Quant TF test accuracy:', q_aware_model_accuracy)\n",
    "# print('Quant TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.q_aware_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model_obj.q_aware_model.layers:\n",
    "#   if hasattr(layer, 'quantize_config'):\n",
    "#     for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "#         quantized_and_dequantized = quantizer(weight, training=False, weights=quantizer_vars)\n",
    "#         min_var = quantizer_vars['min_var']\n",
    "#         max_var = quantizer_vars['max_var']\n",
    "#         print(quantized_and_dequantized*(2**(BIT_WIDTH)))\n",
    "#         # quantized = dequantize(quantized_and_dequantized, min_var, max_var, quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.quantized_tflite_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.interpreter.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.plot_float_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obj = model_obj\n",
    "# # plot_quantized_model(obj, n=6)\n",
    "# model_obj.plot_quantized_model(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See 4x smaller model from quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create a float TFLite model and then see that the quantized TFLite model\n",
    "is 4x smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create float TFLite model.\n",
    "# float_converter = tf.lite.TFLiteConverter.from_keras_model(model_obj.model)\n",
    "# float_tflite_model = float_converter.convert()\n",
    "\n",
    "# # Measure sizes of models.\n",
    "# _, float_file = tempfile.mkstemp('.tflite')\n",
    "# _, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "# with open(quant_file, 'wb') as f:\n",
    "#   f.write(model_obj.quantized_tflite_model)\n",
    "\n",
    "# with open(float_file, 'wb') as f:\n",
    "#   f.write(float_tflite_model)\n",
    "\n",
    "# print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "# print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))\n",
    "# print(float_converter.get_tensor_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b75f63a51ab1e44c10e89cf3b718812d9c5e2447d39cb402b946ba7653bfcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
