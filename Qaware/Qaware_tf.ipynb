{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=5, flat=True)\n",
    "data_zoom = mnist.MNISTData(size_initial=20, size_final=8, color_depth=8, flat=True)\n",
    "\n",
    "# x_train= data_zoom.x_train\n",
    "# y_train= data_zoom.y_train\n",
    "# x_test= data_zoom.x_test\n",
    "# y_test= data_zoom.y_test\n",
    "\n",
    "ax = plt.subplot(1, 1 , 1)\n",
    "plt.imshow(data_zoom.x_train[0].reshape(size_final,size_final), cmap='gray_r')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIT_WIDTH = 8\n",
    "EPOCHS = 40\n",
    "Q_EPOCHS = 80\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = 'model'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,562\n",
      "Trainable params: 10,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from callbacks import all_callbacks\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import os \n",
    "# import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=8, flat=True)\n",
    "\n",
    "class QAutoencoder:\n",
    "    def __init__(self, data: mnist.MNISTData, bit_width=8, EPOCHS=1, Q_EPOCHS=1, model_name='model'):\n",
    "\n",
    "        self.x_train = data.x_train\n",
    "        self.y_train = data.y_train\n",
    "        self.x_test = data.x_test\n",
    "        self.y_test = data.y_test\n",
    "        self.input_min = np.min(data.x_train)\n",
    "        self.input_max = np.max(data.x_train)\n",
    "        self.input_shape = (data.x_train.shape[-1],)\n",
    "        self.BIT_WIDTH = bit_width\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.Q_EPOCHS = Q_EPOCHS\n",
    "        self.MODEL_NAME = model_name\n",
    "        self.model = self.autoencoder_model_gen()\n",
    "        self.history = None\n",
    "        self.loss = None\n",
    "        self.float_model_predictions = None\n",
    "\n",
    "        self.q_aware_model = None\n",
    "        self.quantized_tflite_model = None\n",
    "        self.interpreter = None\n",
    "        self.input_details = None\n",
    "        self.output_details = None\n",
    "        self.quantized_model_predictions = None\n",
    "\n",
    "    def autoencoder_model_gen(self):\n",
    "\n",
    "        # Encoder\n",
    "        encoder_input = Input(shape=self.input_shape)\n",
    "        encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "        encoder_l2 = Dense(32, activation='relu')(encoder_l1)\n",
    "        encoder_l3 = Dense(16, activation='relu')(encoder_l2)\n",
    "        encoder_output = Dense(2, activation='relu')(encoder_l3)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_l1 = Dense(16, activation='relu')(encoder_output)\n",
    "        decoder_l2 = Dense(32, activation='relu')(decoder_l1)\n",
    "        decoder_l3 = Dense(32, activation='relu')(decoder_l2)\n",
    "        # decoder_output = Dense(y_train.shape[-1], activation='sigmoid')(decoder_l3) # classifier\n",
    "        # decoder_output = Dense(\n",
    "        #     self.x_train.shape[-1], activation='sigmoid')(decoder_l3)  # autoencoder\n",
    "        decoder_output = Dense(self.x_train.shape[-1], activation='linear')(decoder_l3)\n",
    "\n",
    "        # Model\n",
    "        model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "        # refactor the code above to use the functional AP\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # model.compile(optimizer='adam', loss='binary_crossentropy') # classifier\n",
    "        return model\n",
    "\n",
    "    def fit_data(self, batch_size=256, epochs=Q_EPOCHS):\n",
    "        \"\"\"Write the fit function for the autoencoder. \n",
    "        Storing the fit history in self.history to be able to plot the fitting scores.\"\"\"\n",
    "\n",
    "        callbacks = all_callbacks(stop_patience=1000,\n",
    "                                  lr_factor=0.5,\n",
    "                                  lr_patience=10,\n",
    "                                  lr_epsilon=0.000001,\n",
    "                                  # min_delta=0.000001,\n",
    "                                  lr_cooldown=2,\n",
    "                                  lr_minimum=0.0000001,\n",
    "                                  outputDir=f'model/QAE_model{self.BIT_WIDTH}bits/callbacks')\n",
    "        # callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "\n",
    "        self.history = self.model.fit(self.x_train, self.x_train,\n",
    "                                      validation_data=(\n",
    "                                          self.x_test, self.x_test),\n",
    "                                      batch_size=batch_size, epochs=epochs,\n",
    "                                      shuffle=True, callbacks=callbacks.callbacks)\n",
    "        # self.model = strip_pruning(self.model)\n",
    "        self.loss = self.model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "        self.model.save(\n",
    "            # f'model/QAE_model{self.BIT_WIDTH}bits/KERAS_check_best_model.h5')\n",
    "            f'model/model_{self.loss}loss/KERAS_check_best_model.model')\n",
    "        self.history = self.history.history\n",
    "        self.convert_to_Q_aware()\n",
    "\n",
    "    def plot_float_model(self, n=6):\n",
    "        \"\"\"Plot the float model\"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        self.float_model_predictions = self.model.predict(self.x_test)\n",
    "        self._extracted_from_plot_quantized_model_8(\n",
    "            n,\n",
    "            self.float_model_predictions,\n",
    "            './images/QAE/reconstructed images {model_name}.png',\n",
    "        )\n",
    "\n",
    "    # def representative_dataset(self):\n",
    "    #     for data in self.x_train:\n",
    "    #         # yield [np.array([data], dtype=np.float32)]\n",
    "    #         yield [np.array([data * (2 ** (self.BIT_WIDTH - 1))], dtype=np.float32)]\n",
    "\n",
    "    def representative_dataset(self):\n",
    "        for data in self.x_train:\n",
    "            # Scale the data using min and max values\n",
    "            scaled_data = (data - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))\n",
    "            yield [np.array([scaled_data], dtype=np.float32)]\n",
    "\n",
    "    def convert_to_Q_aware(self):\n",
    "\n",
    "        # converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n",
    "        # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # converter.representative_dataset = self.representative_dataset\n",
    "        # converter.target_spec.supported_ops = [\n",
    "        #     tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        # converter.inference_input_type = tf.int8\n",
    "        # converter.inference_output_type = tf.int8\n",
    "\n",
    "        # self.q_aware_model = converter.convert()\n",
    "\n",
    "        # # Load the quantized model\n",
    "        # self.interpreter = tf.lite.Interpreter(\n",
    "        #     model_path='quantized_model.tflite')\n",
    "        # self.interpreter.allocate_tensors()\n",
    "        # ------------\n",
    "        quantize_model = tfmot.quantization.keras.quantize_model\n",
    "        self.q_aware_model = quantize_model(self.model)\n",
    "        self.q_aware_model.compile(optimizer='adam', loss='mse')\n",
    "        self.fit_data_Q_aware()\n",
    "        # loss = self.q_aware_model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "\n",
    "        # # Save the quantized model\n",
    "        # with open(f'quantized_model.tflite', 'wb') as f:\n",
    "        #     f.write(self.q_aware_model)\n",
    "\n",
    "        self.convert_to_tflite()\n",
    "\n",
    "    def fit_data_Q_aware(self, batch_size=256, epochs=Q_EPOCHS):\n",
    "        \"\"\"Write the fit function for the autoencoder. \n",
    "        Storing the fit history in self.history to be able to plot the fitting scores.\"\"\"\n",
    "\n",
    "        callbacks = all_callbacks(stop_patience=1000,\n",
    "                                  lr_factor=0.5,\n",
    "                                  lr_patience=10,\n",
    "                                  lr_epsilon=0.000001,\n",
    "                                  # min_delta=0.000001,\n",
    "                                  lr_cooldown=2,\n",
    "                                  lr_minimum=0.0000001,\n",
    "                                  outputDir=f'model/QAE_model{self.BIT_WIDTH}bits/callbacks')\n",
    "        # callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "\n",
    "        self.history_Q_aware = self.q_aware_model.fit(self.x_train, self.x_train,\n",
    "                                      validation_data=(\n",
    "                                          self.x_test, self.x_test),\n",
    "                                      batch_size=batch_size, epochs=epochs,\n",
    "                                      shuffle=True, callbacks=callbacks.callbacks)\n",
    "        # self.model = strip_pruning(self.model)\n",
    "        self.q_aware_model.save(\n",
    "            # f'model/QAE_model{self.BIT_WIDTH}bits/KERAS_check_best_model.h5')\n",
    "            f'model/QAE_model{self.BIT_WIDTH}bits/KERAS_check_best_model.model')\n",
    "        self.history_Q_aware = self.history_Q_aware.history\n",
    "        self.loss = self.q_aware_model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "        # self.convert_to_Q_aware()    \n",
    "\n",
    "    def convert_to_tflite(self):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(self.q_aware_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = self.representative_dataset\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "        self.quantized_tflite_model = converter.convert()\n",
    "\n",
    "        # Save the TensorFlow Lite model\n",
    "        with open(\"model_obj_tflite_model.tflite\", \"wb\") as file:\n",
    "            file.write(self.quantized_tflite_model)\n",
    "\n",
    "        # Load the quantized model\n",
    "        # self.interpreter = tf.lite.Interpreter(model_path='quantized_model.tflite')\n",
    "        self.interpreter = tf.lite.Interpreter(model_content=self.quantized_tflite_model)\n",
    "\n",
    "        self.interpreter.allocate_tensors()\n",
    "\n",
    "        # Get input and output details\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        self.quantized_predictions()\n",
    "\n",
    "\n",
    "\n",
    "    def quantized_predictions(self, n=6):\n",
    "        quantized_model_predictions = []\n",
    "\n",
    "        for i in range(n):\n",
    "            # Prepare input data\n",
    "            # input_data = np.array(\n",
    "            #     [self.x_test[i]*(2**(self.BIT_WIDTH-1))], dtype=np.int8)\n",
    "            input_data = np.array([(self.x_test[i] - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))], dtype=np.int8)  \n",
    "\n",
    "            self.interpreter.set_tensor(\n",
    "                self.input_details[0]['index'], input_data)\n",
    "            print(f\"input_data: {input_data}\")\n",
    "            # Run inference\n",
    "            self.interpreter.invoke()\n",
    "\n",
    "            # Get output\n",
    "            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "            output_data = self.interpreter.get_tensor(\n",
    "                # self.output_details[0]['index']) / (2 ** (self.BIT_WIDTH - 1))\n",
    "                self.output_details[0]['index'])\n",
    "            # output_data = output_data * (self.input_max - self.input_min) / (2 ** (self.BIT_WIDTH - 1)) + self.input_min\n",
    "            # output_data = output_data / (2 ** (self.BIT_WIDTH - 1))\n",
    "            print(f\"output_data: {output_data}\")\n",
    "            quantized_model_predictions.append(output_data)\n",
    "        \n",
    "        self.quantized_model_predictions = quantized_model_predictions\n",
    "        # self.compute_mse()\n",
    "\n",
    "    def plot_quantized_model(self, n=6):\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        self._extracted_from_plot_quantized_model_8(\n",
    "            n,\n",
    "            self.quantized_model_predictions,\n",
    "            './images/QAE/reconstructed images{model_name}.png',\n",
    "        )\n",
    "\n",
    "    # TODO Rename this here and in `plot_float_model` and `plot_quantized_model`\n",
    "    def _extracted_from_plot_quantized_model_8(self, n, quantized_model_predictions, imgs_path):\n",
    "        img_size = int(np.sqrt(self.input_shape[0]))\n",
    "        for i in range(n):\n",
    "            ax = plt.subplot(2, n, i + 1)\n",
    "            self.plot_imgs(\n",
    "                self.x_test, i, img_size, ax\n",
    "            )\n",
    "            ax = plt.subplot(2, n, i + n + 1)\n",
    "            self.plot_imgs(\n",
    "                quantized_model_predictions, i, img_size, ax\n",
    "            )\n",
    "        if not os.path.exists(imgs_path):\n",
    "            os.makedirs(imgs_path)\n",
    "        plt.savefig(imgs_path.format(model_name=\" complete\"))\n",
    "        plt.show()\n",
    "\n",
    "    # TODO Rename this here and in `plot_float_model` and `plot_quantized_model`\n",
    "    def plot_imgs(self, arg0, i, img_size, ax):\n",
    "        plt.imshow(arg0[i].reshape(img_size, img_size), cmap='gray_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    def compute_mse(self):\n",
    "        mse = mean_squared_error(self.float_model_predictions, self.quantized_model_predictions)\n",
    "        print(f'Mean Squared Error between floating point and quantized model predictions: {mse}')\n",
    "        self.mse = mse\n",
    "\n",
    "model_obj = QAutoencoder(data_zoom, bit_width=BIT_WIDTH, model_name=MODEL_NAME, EPOCHS=EPOCHS, Q_EPOCHS=Q_EPOCHS)\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32) # classifier\n",
    "model_obj.model.summary()\n",
    "# model.fit(x_train, x_train, epochs=EPOCHS, batch_size=BATCH_SIZE) #autoencoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/40\n",
      "  1/235 [..............................] - ETA: 8:39 - loss: 0.1838WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.1089\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09351, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09351, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00001: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00001: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 8s 25ms/step - loss: 0.1083 - val_loss: 0.0935 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0888\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09351 to 0.08307, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09351 to 0.08307, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00002: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00002: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 28ms/step - loss: 0.0887 - val_loss: 0.0831 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0811\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08307 to 0.07835, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08307 to 0.07835, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00003: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00003: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 28ms/step - loss: 0.0810 - val_loss: 0.0783 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0770\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07835 to 0.07556, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07835 to 0.07556, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00004: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00004: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0770 - val_loss: 0.0756 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0745\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07556 to 0.07318, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07556 to 0.07318, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00005: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00005: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 30ms/step - loss: 0.0745 - val_loss: 0.0732 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0727\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07318 to 0.07181, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07318 to 0.07181, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00006: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00006: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.0727 - val_loss: 0.0718 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0713\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07181 to 0.07070, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07181 to 0.07070, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00007: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00007: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.0713 - val_loss: 0.0707 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0701\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07070 to 0.06988, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07070 to 0.06988, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00008: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00008: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.0701 - val_loss: 0.0699 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0690\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06988 to 0.06873, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06988 to 0.06873, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00009: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00009: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.0690 - val_loss: 0.0687 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0683\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06873 to 0.06818, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06873 to 0.06818, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00010: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00010: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00010: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_epoch10.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.0683 - val_loss: 0.0682 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0676\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06818 to 0.06738, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06818 to 0.06738, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00011: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00011: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.0676 - val_loss: 0.0674 - lr: 0.0010\n",
      "Epoch 12/40\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0671\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06738 to 0.06734, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06738 to 0.06734, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00012: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00012: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0671 - val_loss: 0.0673 - lr: 0.0010\n",
      "Epoch 13/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0665\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06734 to 0.06637, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06734 to 0.06637, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00013: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00013: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.0665 - val_loss: 0.0664 - lr: 0.0010\n",
      "Epoch 14/40\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0660\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.06637 to 0.06598, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.06637 to 0.06598, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00014: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00014: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0660 - val_loss: 0.0660 - lr: 0.0010\n",
      "Epoch 15/40\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0657\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.06598 to 0.06552, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.06598 to 0.06552, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00015: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00015: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.0657 - val_loss: 0.0655 - lr: 0.0010\n",
      "Epoch 16/40\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0653\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.06552 to 0.06536, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.06552 to 0.06536, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00016: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00016: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.0653 - val_loss: 0.0654 - lr: 0.0010\n",
      "Epoch 17/40\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0651\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.06536 to 0.06506, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.06536 to 0.06506, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00017: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00017: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.0651 - val_loss: 0.0651 - lr: 0.0010\n",
      "Epoch 18/40\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0648\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06506 to 0.06501, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06506 to 0.06501, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00018: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00018: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0648 - val_loss: 0.0650 - lr: 0.0010\n",
      "Epoch 19/40\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0645\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06501 to 0.06453, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06501 to 0.06453, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00019: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00019: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.0645 - val_loss: 0.0645 - lr: 0.0010\n",
      "Epoch 20/40\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0643\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.06453 to 0.06410, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.06453 to 0.06410, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00020: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00020: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00020: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_epoch20.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.0642 - val_loss: 0.0641 - lr: 0.0010\n",
      "Epoch 21/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0639\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06410 to 0.06384, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06410 to 0.06384, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00021: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00021: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0639 - val_loss: 0.0638 - lr: 0.0010\n",
      "Epoch 22/40\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0637\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.06384 to 0.06369, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.06384 to 0.06369, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00022: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00022: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0636 - val_loss: 0.0637 - lr: 0.0010\n",
      "Epoch 23/40\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0634\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06369 to 0.06356, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06369 to 0.06356, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00023: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00023: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0634 - val_loss: 0.0636 - lr: 0.0010\n",
      "Epoch 24/40\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0631\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06356 to 0.06321, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06356 to 0.06321, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00024: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00024: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0631 - val_loss: 0.0632 - lr: 0.0010\n",
      "Epoch 25/40\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0630\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06321 to 0.06309, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06321 to 0.06309, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00025: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00025: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0629 - val_loss: 0.0631 - lr: 0.0010\n",
      "Epoch 26/40\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0626\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06309 to 0.06296, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06309 to 0.06296, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00026: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00026: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0626 - val_loss: 0.0630 - lr: 0.0010\n",
      "Epoch 27/40\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0625\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.06296 to 0.06259, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.06296 to 0.06259, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00027: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00027: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.0625 - val_loss: 0.0626 - lr: 0.0010\n",
      "Epoch 28/40\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0623\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.06259 to 0.06235, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.06259 to 0.06235, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00028: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00028: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.0623 - val_loss: 0.0624 - lr: 0.0010\n",
      "Epoch 29/40\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0620\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.06235\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.06235\n",
      "\n",
      "Epoch 00029: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00029: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.0620 - val_loss: 0.0624 - lr: 0.0010\n",
      "Epoch 30/40\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0619\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.06235 to 0.06229, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.06235 to 0.06229, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00030: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00030: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00030: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_epoch30.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0619 - val_loss: 0.0623 - lr: 0.0010\n",
      "Epoch 31/40\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0619\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.06229 to 0.06196, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.06229 to 0.06196, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00031: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00031: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0619 - val_loss: 0.0620 - lr: 0.0010\n",
      "Epoch 32/40\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0616\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.06196 to 0.06190, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.06196 to 0.06190, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00032: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00032: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0617 - val_loss: 0.0619 - lr: 0.0010\n",
      "Epoch 33/40\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0613\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.06190 to 0.06161, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.06190 to 0.06161, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00033: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00033: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.0614 - val_loss: 0.0616 - lr: 0.0010\n",
      "Epoch 34/40\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0613\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.06161\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.06161\n",
      "\n",
      "Epoch 00034: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00034: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 3s 11ms/step - loss: 0.0613 - val_loss: 0.0621 - lr: 0.0010\n",
      "Epoch 35/40\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0613\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.06161 to 0.06134, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.06161 to 0.06134, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00035: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00035: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0613 - val_loss: 0.0613 - lr: 0.0010\n",
      "Epoch 36/40\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0610\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.06134\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.06134\n",
      "\n",
      "Epoch 00036: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00036: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 3s 11ms/step - loss: 0.0610 - val_loss: 0.0615 - lr: 0.0010\n",
      "Epoch 37/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0610\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.06134 to 0.06126, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.06134 to 0.06126, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00037: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00037: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0610 - val_loss: 0.0613 - lr: 0.0010\n",
      "Epoch 38/40\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0606\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.06126 to 0.06107, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.06126 to 0.06107, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00038: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00038: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.0607 - val_loss: 0.0611 - lr: 0.0010\n",
      "Epoch 39/40\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0606\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.06107\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.06107\n",
      "\n",
      "Epoch 00039: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00039: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.0606 - val_loss: 0.0612 - lr: 0.0010\n",
      "Epoch 40/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0605\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.06107 to 0.06077, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.06107 to 0.06077, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00040: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00040: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00040: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_epoch40.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.0605 - val_loss: 0.0608 - lr: 0.0010\n",
      "INFO:tensorflow:Assets written to: model/model_0.060774143785238266loss/KERAS_check_best_model.model\\assets\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/40\n",
      "  1/235 [..............................] - ETA: 8:22 - loss: 0.0747WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0075s). Check your callbacks.\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0648\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06278, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06278, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00001: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 16s 58ms/step - loss: 0.0648 - val_loss: 0.0628 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0627\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06278 to 0.06268, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss improved from 0.06278 to 0.06268, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00002: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 13s 54ms/step - loss: 0.0628 - val_loss: 0.0627 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0622\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06268 to 0.06226, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 0.06268 to 0.06226, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00003: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 11s 45ms/step - loss: 0.0623 - val_loss: 0.0623 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0618\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06226\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06226\n",
      "\n",
      "Epoch 00004: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0618 - val_loss: 0.0624 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0617\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06226 to 0.06174, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss improved from 0.06226 to 0.06174, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00005: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0617 - val_loss: 0.0617 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0616\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06174\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06174\n",
      "\n",
      "Epoch 00006: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0616 - val_loss: 0.0624 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0616\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.06174\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.06174\n",
      "\n",
      "Epoch 00007: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0616 - val_loss: 0.0624 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0611\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06174 to 0.06149, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss improved from 0.06174 to 0.06149, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00008: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 14s 59ms/step - loss: 0.0612 - val_loss: 0.0615 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0612\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06149\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06149\n",
      "\n",
      "Epoch 00009: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0612 - val_loss: 0.0617 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0616\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06149\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06149\n",
      "\n",
      "Epoch 00010: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00010: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_epoch10.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0616 - val_loss: 0.0617 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0612\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06149 to 0.06126, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss improved from 0.06149 to 0.06126, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00011: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0612 - val_loss: 0.0613 - lr: 0.0010\n",
      "Epoch 12/40\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0613\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.06126\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.06126\n",
      "\n",
      "Epoch 00012: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0612 - val_loss: 0.0615 - lr: 0.0010\n",
      "Epoch 13/40\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0612\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06126 to 0.06114, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_loss improved from 0.06126 to 0.06114, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00013: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 12s 50ms/step - loss: 0.0611 - val_loss: 0.0611 - lr: 0.0010\n",
      "Epoch 14/40\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0610\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06114\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06114\n",
      "\n",
      "Epoch 00014: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00014: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0610 - val_loss: 0.0621 - lr: 0.0010\n",
      "Epoch 15/40\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0609\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06114\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06114\n",
      "\n",
      "Epoch 00015: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0609 - val_loss: 0.0624 - lr: 0.0010\n",
      "Epoch 16/40\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0609\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06114\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06114\n",
      "\n",
      "Epoch 00016: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00016: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0609 - val_loss: 0.0614 - lr: 0.0010\n",
      "Epoch 17/40\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0607\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.06114 to 0.06108, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: val_loss improved from 0.06114 to 0.06108, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00017: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 12s 50ms/step - loss: 0.0607 - val_loss: 0.0611 - lr: 0.0010\n",
      "Epoch 18/40\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0607\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06108 to 0.06074, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: val_loss improved from 0.06108 to 0.06074, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00018: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 14s 60ms/step - loss: 0.0606 - val_loss: 0.0607 - lr: 0.0010\n",
      "Epoch 19/40\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0604\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06074 to 0.06047, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00019: val_loss improved from 0.06074 to 0.06047, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00019: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00019: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 11s 47ms/step - loss: 0.0604 - val_loss: 0.0605 - lr: 0.0010\n",
      "Epoch 20/40\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0606\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06047\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06047\n",
      "\n",
      "Epoch 00020: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00020: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_epoch20.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 8s 33ms/step - loss: 0.0606 - val_loss: 0.0617 - lr: 0.0010\n",
      "Epoch 21/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0606\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.06047\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.06047\n",
      "\n",
      "Epoch 00021: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00021: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 32ms/step - loss: 0.0606 - val_loss: 0.0610 - lr: 0.0010\n",
      "Epoch 22/40\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0604\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06047\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06047\n",
      "\n",
      "Epoch 00022: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00022: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0604 - val_loss: 0.0612 - lr: 0.0010\n",
      "Epoch 23/40\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0604\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.06047\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.06047\n",
      "\n",
      "Epoch 00023: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0604 - val_loss: 0.0606 - lr: 0.0010\n",
      "Epoch 24/40\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0606\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06047 to 0.06033, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_loss improved from 0.06047 to 0.06033, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00024: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 12s 50ms/step - loss: 0.0606 - val_loss: 0.0603 - lr: 0.0010\n",
      "Epoch 25/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0603\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00025: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00025: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 28ms/step - loss: 0.0604 - val_loss: 0.0606 - lr: 0.0010\n",
      "Epoch 26/40\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0605\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00026: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 29ms/step - loss: 0.0605 - val_loss: 0.0605 - lr: 0.0010\n",
      "Epoch 27/40\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0603\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00027: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00027: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 28ms/step - loss: 0.0603 - val_loss: 0.0605 - lr: 0.0010\n",
      "Epoch 28/40\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0602\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00028: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 29ms/step - loss: 0.0602 - val_loss: 0.0606 - lr: 0.0010\n",
      "Epoch 29/40\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0605\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00029: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 28ms/step - loss: 0.0605 - val_loss: 0.0617 - lr: 0.0010\n",
      "Epoch 30/40\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0602\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00030: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00030: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_epoch30.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 28ms/step - loss: 0.0602 - val_loss: 0.0614 - lr: 0.0010\n",
      "Epoch 31/40\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0601\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00031: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 31ms/step - loss: 0.0601 - val_loss: 0.0607 - lr: 0.0010\n",
      "Epoch 32/40\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0601\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00032: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 8s 33ms/step - loss: 0.0601 - val_loss: 0.0608 - lr: 0.0010\n",
      "Epoch 33/40\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0600\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00033: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 29ms/step - loss: 0.0600 - val_loss: 0.0607 - lr: 0.0010\n",
      "Epoch 34/40\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0604\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.06033\n",
      "\n",
      "Epoch 00034: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 30ms/step - loss: 0.0603 - val_loss: 0.0606 - lr: 0.0010\n",
      "Epoch 35/40\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0598\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.06033 to 0.05992, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss improved from 0.06033 to 0.05992, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00035: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 12s 49ms/step - loss: 0.0598 - val_loss: 0.0599 - lr: 5.0000e-04\n",
      "Epoch 36/40\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0598\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05992\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05992\n",
      "\n",
      "Epoch 00036: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 30ms/step - loss: 0.0599 - val_loss: 0.0602 - lr: 5.0000e-04\n",
      "Epoch 37/40\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0599\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.05992\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.05992\n",
      "\n",
      "Epoch 00037: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0599 - val_loss: 0.0607 - lr: 5.0000e-04\n",
      "Epoch 38/40\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0598\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.05992\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.05992\n",
      "\n",
      "Epoch 00038: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 29ms/step - loss: 0.0598 - val_loss: 0.0604 - lr: 5.0000e-04\n",
      "Epoch 39/40\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0597\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.05992 to 0.05976, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss improved from 0.05992 to 0.05976, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00039: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 15s 65ms/step - loss: 0.0597 - val_loss: 0.0598 - lr: 5.0000e-04\n",
      "Epoch 40/40\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0594\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.05976\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.05976\n",
      "\n",
      "Epoch 00040: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00040: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_epoch40.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 7s 32ms/step - loss: 0.0594 - val_loss: 0.0603 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits/KERAS_check_best_model.model\\assets\n",
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmp3sapg88w\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmp3sapg88w\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data: [[   0    0    0    0    0    0    0    0    0   56   34    0    0    0\n",
      "     0    0    0   24   31   89   92   71  118    0    0    0    0    0\n",
      "     0   76   18    0    0    0    0    0    0  127    1    0    0    0\n",
      "     0    0   92   15    0    0    0    0    0   13  102    1    0    0\n",
      "     0    0    0 -128    0    0    0    0]]\n",
      "output_data: [[-91 -88 -92 -81 -18  -1 -65 -90 -91 -95 -94 -44  27 -50 -88 -86 -93 -99\n",
      "  -80  27 -40 -71 -98 -95 -91 -91 -23  29 -51 -39 -77 -92 -90 -82  16  -4\n",
      "   17  21 -36 -87 -91 -62  15  -1 -25  26 -48 -92 -89 -70  14  50  34 -20\n",
      "  -89 -94 -90 -91 -87 -90 -90 -88 -92 -93]]\n",
      "input_data: [[   0    2  111  126  126    0    0    0    0   20   83    0  107    9\n",
      "     0    0    0    1    0   23 -128    0    0    0    0    0    0 -128\n",
      "     7    0    0    0    0    0   72   88    0    0    0    0    0    0\n",
      "  -128    0    2    1    2    0    0    0  126 -128 -128   97 -128  124\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "output_data: [[-90 -80 -76 -66 -77 -86 -86 -93 -83 -31  39  45  37  21 -61 -82 -83 -33\n",
      "  -16 -68 -43 -25 -72 -93 -88 -49   7  20  21 -15 -73 -94 -92 -75 -21  11\n",
      "   -8   0 -29 -88 -88 -83 -69 -52 -76 -40   2 -76 -88 -66 -30 -36 -70  12\n",
      "   14 -81 -93 -81 -28  38  62  29 -68 -94]]\n",
      "input_data: [[   0    0    0    0    1   99    1    0    0    0    0    0    8   32\n",
      "     0    0    0    0    0    0  104    0    0    0    0    0    0    0\n",
      "  -128    0    0    0    0    0    0    7   60    0    0    0    0    0\n",
      "     1   81    7    0    0    0    0    0    0 -128    0    0    0    0\n",
      "     0    0    0   76    0    0    0    0]]\n",
      "output_data: [[-90 -79 -75 -63 -67 -76 -81 -93 -86 -47  26  45  41  34 -43 -83 -87 -52\n",
      "  -21 -52 -41  -2 -72 -94 -92 -63  -3  51  46 -15 -76 -89 -93 -73 -33   5\n",
      "  -15   7 -33 -85 -87 -73 -64 -79 -89   2 -10 -79 -85 -45  -9 -60 -29  46\n",
      "  -21 -89 -91 -60  12  52  61 -16 -81 -95]]\n",
      "input_data: [[   0    0    0   12  110    0    0    0    0    0   13  126 -128   22\n",
      "     0    0    0    1  125  126   90  114    0    0    0    3  125    0\n",
      "     0   18 -128    0    0   40   70    0    0   18 -128    0    0   78\n",
      "    73    0   39 -128   79    0    0   22 -128  123  123  112    0    0\n",
      "     0    0   11  124   36    0    0    0]]\n",
      "output_data: [[-90 -77 -69 -59 -75 -85 -84 -93 -80 -13  66  51  44  28 -53 -81 -82 -25\n",
      "  -17 -66 -30  -6 -66 -91 -90 -46   7  62  52   0 -60 -91 -92 -64 -28   1\n",
      "  -19   5   5 -82 -88 -69 -57 -81 -84 -50  41 -69 -89 -43   4 -56 -49  22\n",
      "   35 -82 -92 -65   3  59  90  24 -70 -95]]\n",
      "input_data: [[   0    0    0    0    0    0    0    0    0    0   31    0    0    0\n",
      "    80    0    0    0  120    1    0    0   97    0    0   61   16    0\n",
      "     0   78    9    0    0   78    0    0    0 -128    0    0    0    0\n",
      "    79   89   45 -128    4    0    0    0    0    0    0 -128    3    0\n",
      "     0    0    0    0    0   85    1    0]]\n",
      "output_data: [[-90 -79 -75 -62 -65 -77 -80 -93 -86 -49  24  44  37  32 -42 -83 -87 -54\n",
      "  -22 -54 -44  -4 -72 -95 -91 -62  -7  50  46 -17 -75 -89 -93 -71 -34   4\n",
      "  -16   6 -33 -85 -87 -73 -63 -79 -91   4 -10 -79 -86 -45 -10 -63 -28  47\n",
      "  -24 -90 -91 -59  12  52  59 -21 -82 -95]]\n",
      "input_data: [[   0    0    0    0    0    0    0    0    0    0    0    0   17  119\n",
      "     2    0    0    0    0    0  120   45    0    0    0    0    0    1\n",
      "  -128    0    0    0    0    0    0   39  122    2    0    0    0    0\n",
      "     2  120   19    0    0    0    0    0    0 -128    0    0    0    0\n",
      "     0    0    0  126    0    0    0    0]]\n",
      "output_data: [[-90 -80 -81 -73 -72 -71 -78 -93 -90 -66   9  54  55  56 -14 -76 -89 -57\n",
      "    3 -32 -45  10 -40 -89 -93 -58  -1  47  39   1 -64 -88 -93 -64 -24  34\n",
      "   16  16 -49 -85 -86 -67 -47 -66 -66  30 -35 -84 -85 -36   5 -50  -3  62\n",
      "  -53 -93 -91 -53  33  67  57 -39 -88 -96]]\n"
     ]
    }
   ],
   "source": [
    "model_obj.fit_data(epochs=EPOCHS)  # batch_size=BATCH_SIZE, epochs=EPOCHS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06032105162739754"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_obj.plot_float_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obj = model_obj\n",
    "# plot_quantized_model(obj, n=6)\n",
    "model_obj.plot_quantized_model(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retraining Q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.fit_data_Q_aware(epochs=Q_EPOCHS)\n",
    "# model_obj.convert_to_tflite()\n",
    "# model_obj.plot_quantized_model(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "def save_objects(model_obj, MODEL_DIR):\n",
    "    # Save the TensorFlow model\n",
    "    model_obj.model.save(MODEL_DIR)\n",
    "\n",
    "    # Save the TensorFlow Lite model\n",
    "    with open(\"model_obj_tflite_model.tflite\", \"wb\") as file:\n",
    "        file.write(model_obj.quantized_tflite_model)\n",
    "\n",
    "    # Save the other attributes of the custom object\n",
    "    model_obj_dict = model_obj.__dict__.copy()\n",
    "    del model_obj_dict[\"model\"]\n",
    "    del model_obj_dict[\"interpreter\"]\n",
    "\n",
    "    with open(\"model_obj_attributes.pickle\", \"wb\") as file:\n",
    "        pickle.dump(model_obj_dict, file)\n",
    "\n",
    "def load_objects(MODEL_DIR):\n",
    "    # Load the TensorFlow model\n",
    "    loaded_model = tf.keras.models.load_model(MODEL_DIR)\n",
    "\n",
    "    # Load the TensorFlow Lite model\n",
    "    with open(\"model_obj_tflite_model.tflite\", \"rb\") as file:\n",
    "        loaded_tflite_model = file.read()\n",
    "\n",
    "    # Load the other attributes of the custom object\n",
    "    with open(\"model_obj_attributes.pickle\", \"rb\") as file:\n",
    "        model_obj_attributes = pickle.load(file)\n",
    "\n",
    "    # Create a new custom object and set its attributes\n",
    "    loaded_model_obj = QAutoencoder(model_obj_attributes[\"data_zoom\"], bit_width=model_obj_attributes[\"BIT_WIDTH\"], model_name=model_obj_attributes[\"MODEL_NAME\"])\n",
    "    for key, value in model_obj_attributes.items():\n",
    "        setattr(loaded_model_obj, key, value)\n",
    "\n",
    "    # Set the model attribute of the custom object\n",
    "    loaded_model_obj.model = loaded_model\n",
    "    loaded_model_obj.quantized_tflite_model = loaded_tflite_model\n",
    "\n",
    "    # Load the interpreter\n",
    "    loaded_model_obj.interpreter = tf.lite.Interpreter(model_content=loaded_tflite_model)\n",
    "    loaded_model_obj.interpreter.allocate_tensors()\n",
    "\n",
    "    return loaded_model_obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_DIR = os.path.abspath(\"model_obj_tf_model\")\n",
    "ASSETS_DIR = os.path.join(MODEL_DIR, \"assets\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "if not os.path.exists(ASSETS_DIR):\n",
    "    os.makedirs(ASSETS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the objects of model_obj\n",
    "# save_objects(model_obj, MODEL_DIR)\n",
    "# # # Load the objects and create a new model_obj\n",
    "# # loaded_model_obj = load_objects(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save_objects(model_obj):\n",
    "    # Save the TensorFlow model\n",
    "    model_obj.model.save(os.path.join(MODEL_DIR, \"tf_model\"))\n",
    "\n",
    "    # Save the TensorFlow Lite model\n",
    "    with open(\"model_obj_tflite_model.tflite\", \"wb\") as file:\n",
    "        file.write(model_obj.quantized_tflite_model)\n",
    "\n",
    "    # Save the other attributes of the custom object\n",
    "    model_obj_dict = model_obj.__dict__.copy()\n",
    "    del model_obj_dict[\"model\"]\n",
    "    del model_obj_dict[\"interpreter\"]\n",
    "\n",
    "    # Convert the list of arrays in masked_array to a single 2D array\n",
    "    masked_array = model_obj_dict[\"masked_array\"]\n",
    "    model_obj_dict[\"masked_array_shapes\"] = [arr.shape for arr in masked_array]\n",
    "    model_obj_dict[\"masked_array\"] = np.vstack(masked_array)\n",
    "\n",
    "    with h5py.File(\"model_obj_attributes.h5\", \"w\") as file:\n",
    "        for key, value in model_obj_dict.items():\n",
    "            file.create_dataset(key, data=value)\n",
    "\n",
    "def load_objects():\n",
    "    # Load the TensorFlow model\n",
    "    loaded_model = tf.keras.models.load_model(os.path.join(MODEL_DIR, \"tf_model\"))\n",
    "\n",
    "    # Load the TensorFlow Lite model\n",
    "    with open(\"model_obj_tflite_model.tflite\", \"rb\") as file:\n",
    "        loaded_tflite_model = file.read()\n",
    "\n",
    "    # Load the other attributes of the custom object\n",
    "    with h5py.File(\"model_obj_attributes.h5\", \"r\") as file:\n",
    "        model_obj_attributes = {key: file[key][()] for key in file.keys()}\n",
    "\n",
    "    # Convert the single 2D array back to a list of arrays for masked_array\n",
    "    loaded_masked_array = np.split(model_obj_attributes[\"masked_array\"], np.cumsum(model_obj_attributes[\"masked_array_shapes\"][:-1]))\n",
    "    model_obj_attributes[\"masked_array\"] = [arr.reshape(shape) for arr, shape in zip(loaded_masked_array, model_obj_attributes[\"masked_array_shapes\"])]\n",
    "\n",
    "    # Create a new custom object and set its attributes\n",
    "    loaded_model_obj = QAutoencoder(model_obj_attributes[\"data_zoom\"], bit_width=model_obj_attributes[\"BIT_WIDTH\"], model_name=model_obj_attributes[\"MODEL_NAME\"])\n",
    "    for key, value in model_obj_attributes.items():\n",
    "        setattr(loaded_model_obj, key, value)\n",
    "\n",
    "    # Set the model attribute of the custom object\n",
    "    loaded_model_obj.model = loaded_model\n",
    "    loaded_model_obj.quantized_tflite_model = loaded_tflite_model\n",
    "\n",
    "    # Load the interpreter\n",
    "    loaded_model_obj.interpreter = tf.lite.Interpreter(model_content=loaded_tflite_model)\n",
    "    loaded_model_obj.interpreter.allocate_tensors()\n",
    "\n",
    "    return loaded_model_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\luisa\\OneDrive\\Documentos\\GitHub\\Autoencoder-for-FPGA\\Qaware\\model_obj_tf_model\\tf_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\luisa\\OneDrive\\Documentos\\GitHub\\Autoencoder-for-FPGA\\Qaware\\model_obj_tf_model\\tf_model\\assets\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'masked_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Save the objects of model_obj\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m save_objects(model_obj)\n",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m, in \u001b[0;36msave_objects\u001b[1;34m(model_obj)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdel\u001b[39;00m model_obj_dict[\u001b[39m\"\u001b[39m\u001b[39minterpreter\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[39m# Convert the list of arrays in masked_array to a single 2D array\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m masked_array \u001b[39m=\u001b[39m model_obj_dict[\u001b[39m\"\u001b[39;49m\u001b[39mmasked_array\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     18\u001b[0m model_obj_dict[\u001b[39m\"\u001b[39m\u001b[39mmasked_array_shapes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m masked_array]\n\u001b[0;32m     19\u001b[0m model_obj_dict[\u001b[39m\"\u001b[39m\u001b[39mmasked_array\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(masked_array)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'masked_array'"
     ]
    }
   ],
   "source": [
    "# Save the objects of model_obj\n",
    "save_objects(model_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the objects and create a new model_obj\n",
    "loaded_model_obj = load_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_objects(model_obj):\n",
    "    # Save the Keras model\n",
    "    model_obj.model.save('model_obj_keras_model.h5')\n",
    "\n",
    "    # Save the .tflite model\n",
    "    with open(\"model_obj_tflite_model.tflite\", \"wb\") as file:\n",
    "        file.write(model_obj.quantized_tflite_model)\n",
    "\n",
    "    # Save other attributes\n",
    "    attrs_to_save = {\n",
    "        'x_train': model_obj.x_train,\n",
    "        'y_train': model_obj.y_train,\n",
    "        'x_test': model_obj.x_test,\n",
    "        'y_test': model_obj.y_test,\n",
    "        'input_shape': model_obj.input_shape,\n",
    "        'BIT_WIDTH': model_obj.BIT_WIDTH,\n",
    "        'EPOCHS': model_obj.EPOCHS,\n",
    "        'Q_EPOCHS': model_obj.Q_EPOCHS,\n",
    "        'MODEL_NAME': model_obj.MODEL_NAME,\n",
    "        'history': model_obj.history,\n",
    "        'loss': model_obj.loss,\n",
    "        'float_model_predictions': model_obj.float_model_predictions,\n",
    "        'quantized_model_predictions': model_obj.quantized_model_predictions,\n",
    "        'input_min': model_obj.input_min,\n",
    "        'input_max': model_obj.input_max,\n",
    "        # 'history_Q_aware': model_obj.history_Q_aware,\n",
    "        # 'mse': model_obj.mse,\n",
    "    }\n",
    "    with open('model_obj_attributes.pickle', 'wb') as f:\n",
    "        pickle.dump(attrs_to_save, f)\n",
    "\n",
    "\n",
    "def load_objects():\n",
    "    \n",
    "    # model_obj.model.save('model_obj_keras_model.h5')\n",
    "    # Load the TensorFlow model\n",
    "    loaded_model = tf.keras.models.load_model(os.path.join(MODEL_DIR, \"tf_model\"))\n",
    "\n",
    "    # Load the Keras model\n",
    "    loaded_keras_model = tf.keras.models.load_model('model_obj_keras_model.h5')\n",
    "\n",
    "    # Load the .tflite model\n",
    "    with open(\"model_obj_tflite_model.tflite\", \"rb\") as file:\n",
    "        loaded_tflite_model = file.read()\n",
    "\n",
    "    # Load other attributes\n",
    "    with open('model_obj_attributes.pickle', 'rb') as f:\n",
    "        loaded_attributes = pickle.load(f)\n",
    "    \n",
    "    # Create a new QAutoencoder object and set the attributes\n",
    "    loaded_model_obj = QAutoencoder(data_zoom, bit_width=loaded_attributes['BIT_WIDTH'], model_name=loaded_attributes['MODEL_NAME'],\n",
    "                                    EPOCHS=loaded_attributes['EPOCHS'], Q_EPOCHS=loaded_attributes['Q_EPOCHS'])\n",
    "    \n",
    "    loaded_model_obj.model = loaded_keras_model\n",
    "    loaded_model_obj.quantized_tflite_model = loaded_tflite_model\n",
    "    loaded_model_obj.input_min = loaded_attributes['input_min']\n",
    "    loaded_model_obj.input_max = loaded_attributes['input_max']\n",
    "    loaded_model_obj.input_shape = loaded_attributes['input_shape']\n",
    "    loaded_model_obj.history = loaded_attributes['history']\n",
    "    loaded_model_obj.history_Q_aware = loaded_attributes['history_Q_aware']\n",
    "    loaded_model_obj.loss = loaded_attributes['loss']\n",
    "    # loaded_model_obj.mse = loaded_attributes['mse']\n",
    "\n",
    "    loaded_model_obj.x_train = loaded_attributes['x_train']\n",
    "    loaded_model_obj.y_train = loaded_attributes['y_train']\n",
    "    loaded_model_obj.x_test = loaded_attributes['x_test']\n",
    "    loaded_model_obj.y_test = loaded_attributes['y_test']\n",
    "    loaded_model_obj.model = loaded_model\n",
    "    loaded_model_obj.history = loaded_attributes['history']\n",
    "    loaded_model_obj.loss = loaded_attributes['loss']\n",
    "    loaded_model_obj.float_model_predictions = loaded_attributes['float_model_predictions']\n",
    "    loaded_model_obj.quantized_model_predictions = loaded_attributes['quantized_model_predictions']\n",
    "    loaded_model_obj.interpreter = loaded_tflite_model\n",
    "    loaded_model_obj.input_min = loaded_attributes['input_min']\n",
    "    loaded_model_obj.input_max = loaded_attributes['input_max']\n",
    "\n",
    "\n",
    "    # Recreate the TFLite interpreter\n",
    "    loaded_model_obj.interpreter = tf.lite.Interpreter(model_content=loaded_tflite_model)\n",
    "    loaded_model_obj.interpreter.allocate_tensors()\n",
    "    loaded_model_obj.input_details = loaded_model_obj.interpreter.get_input_details()\n",
    "    loaded_model_obj.output_details = loaded_model_obj.interpreter.get_output_details()\n",
    "    # loaded_model_obj.convert_to_Q_aware()\n",
    "\n",
    "    return loaded_model_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the objects of model_obj\n",
    "save_objects(model_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the objects and create a new model_obj\n",
    "loaded_model_obj = load_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,562\n",
      "Trainable params: 10,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model_obj.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' \\x00\\x00\\x00TFL3\\x00\\x00\\x00\\x00\\x14\\x00 \\x00\\x1c\\x00\\x18\\x00\\x14\\x00\\x10\\x00\\x0c\\x00\\x00\\x00\\x08\\x00\\x04\\x00\\x14\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x94\\x00\\x00\\x00\\xc4\\x00\\x00\\x00\\x84.\\x00\\x00\\x94.\\x00\\x00\\x8c>\\x00\\x00\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x10\\x00\\x0c\\x00\\x08\\x00\\x04\\x00\\n\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00@\\x00\\x00\\x00\\x0f\\x00\\x00\\x00serving_default\\x00\\x01\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xbc\\xff\\xff\\xff\\x18\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\r\\x00\\x00\\x00quant_dense_7\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x0e\\xe2\\xff\\xff\\x04\\x00\\x00\\x00\\x07\\x00\\x00\\x00input_1\\x00\\x01\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x08\\x00\\x0c\\x00\\x08\\x00\\x04\\x00\\x08\\x00\\x00\\x00\\x1a\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x13\\x00\\x00\\x00min_runtime_version\\x00\\x1b\\x00\\x00\\x00\\xbc-\\x00\\x00\\xb4-\\x00\\x00\\xa4\\x1d\\x00\\x00\\x8c\\x1c\\x00\\x00\\x84\\x1c\\x00\\x00t\\x14\\x00\\x00\\xe4\\x13\\x00\\x00\\xdc\\x13\\x00\\x00\\xcc\\x11\\x00\\x00|\\x11\\x00\\x00t\\x11\\x00\\x00D\\x11\\x00\\x00,\\x11\\x00\\x00$\\x11\\x00\\x00\\xf4\\x10\\x00\\x00\\xa4\\x10\\x00\\x00\\x9c\\x10\\x00\\x00\\x8c\\x0e\\x00\\x00\\xfc\\r\\x00\\x00\\xf4\\r\\x00\\x00\\xe4\\t\\x00\\x00T\\t\\x00\\x00L\\t\\x00\\x00<\\x01\\x00\\x00,\\x00\\x00\\x00$\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xc6\\xe2\\xff\\xff\\x04\\x00\\x00\\x00\\x10\\x00\\x00\\x001.14.0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\xd2\\xff\\xff\\xe6\\xe2\\xff\\xff\\x04\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\xf9\\xff\\xff\\xff\\xf2\\x00\\x00\\x00\\x98\\x03\\x00\\x00P\\x08\\x00\\x00\\t\\x05\\x00\\x00r\\xf7\\xff\\xff\\xcb\\xfe\\xff\\xffZ\\x01\\x00\\x00C\\x01\\x00\\x00=\\x04\\x00\\x00[\\x06\\x00\\x00\\'\\x05\\x00\\x00c\\x02\\x00\\x00\\x99\\xff\\xff\\xff\\xe7\\xfd\\xff\\xff\\xed\\x01\\x00\\x00\\x1e\\x01\\x00\\x00\\xae\\xfe\\xff\\xffo\\x03\\x00\\x00\\x9c\\x03\\x00\\x00U\\x00\\x00\\x00\\x0e\\x03\\x00\\x00\\xe2\\xfd\\xff\\xfft\\xf9\\xff\\xff~\\x00\\x00\\x00R\\xf4\\xff\\xffT\\t\\x00\\x00x\\x05\\x00\\x00\\xb4\\x07\\x00\\x00\\xc2\\x13\\x00\\x00O\\x05\\x00\\x00q\\xf4\\xff\\xff{\\xfe\\xff\\xff\\x8e\\xf0\\xff\\xff;\\t\\x00\\x00\\xf7\\xfe\\xff\\xff\\x82\\x00\\x00\\x00\\xfd\\x05\\x00\\x00\\x0e\\x04\\x00\\x004\\xf6\\xff\\xff\\xed\\xff\\xff\\xff&\\xf4\\xff\\xffW\\x0c\\x00\\x00x\\xfb\\xff\\xff\\x10\\xfb\\xff\\xff\\x16\\x00\\x00\\x00\\xe1\\x03\\x00\\x00}\\xfe\\xff\\xff\\x85\\x01\\x00\\x00\\x06\\xfb\\xff\\xff\\xf7\\xfc\\xff\\xff\\xec\\xf9\\xff\\xff\\xfc\\xf9\\xff\\xff\\xfa\\x07\\x00\\x00.\\x05\\x00\\x00\\xcb\\xff\\xff\\xffu\\x00\\x00\\x00\\xc8\\x02\\x00\\x00\\xa4\\xfd\\xff\\xff%\\x06\\x00\\x00\\xcb\\x04\\x00\\x00\\xec\\x08\\x00\\x00\\x1f\\x03\\x00\\x00H\\x00\\x00\\x00\\xf2\\xe3\\xff\\xff\\x04\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\xff\\x00\\x01\\x00\\x01\\x00\\x00\\x00\\x00\\xe7\\x01\\xfe\\x00\\x00\\x00\\x01\\x00\\x00\\xff\\x00\\x02\\x01\\xfd\\xff\\xf0\\x01\\xfe\\x00\\x00\\x01\\xfc\\x04\\xff\\x02\\x06\\xfa\\xfe\\x04\\x03\\x01\\x00\\x02\\x01\\xf8\\x01\\xff\\x02\\x00\\xff\\xfc\\xf9\\xfe\\x06\\x05\\xee\\xf8\\xf0\\x02\\xf9\\xfd\\xff\\x03\\xec\\x02\\x05\\x03\\x10\\xe4\\xec\\x10\\x06\\x03\\x04\\x05\\xf0\\xe7\\n\\x03\\n\\xff\\xfb\\x01\\xf2\\x01\\n\\xfe\\xe6\\x01\\x05\\x04\\xfa\\xfa\\x00\\x0b\\xf8\\xf5\\x11\\xfa\\x1b\\xd6\\xd1\\x10\\xf3\\xf8\\x04\\x02\\xcc\\x14\\x13\\x16\\x00\\x1d\\xf5\\t\\xfe\\t\\x04\\xf9\\xe8\\xf9\\x11\\x0c\\x0e\\x03\\x06@O\\xf5\\xe4\\xf4\\x12\\xeb\\x00\\xf2\\xeb\\x05\\x0b\\xf6\\xf0\\xfb\\x16\\x16\\xe2:\\xfa\\xeb\\n\\xfa\\xfa!\\xf2\\xf7\\xfa\\x17\\x05\\xe8\\x13U\\x03.\\xdf\\x1c\\xf5\\xf3)\\xfd\\xef\\x12\\x0b\\x12\\x18\\xf3\\xec\\t\\x0f\\xfd\\x08\\xe3\\x1f\\xec\\xe1\\x07\\x03\\x17\\x0f\\xef\\xf1\\xfd\\xdc\\x1f\\xf6\\r\\x0b\\x08\\xfe\\xfa\\t\\xea\\x06\\x0f\\x00\\xe6\\x0b\\x05\\x11\\x15\\x0b\\xfc\\xf3\\xfd\\x05\\x08\\xf3\\xf5\\xf5\\x00\\xf0\\x03\\xf7\\x03\\x06\\xfe\\x01\\xff\\xfe\\x03\\x04\\x00\\x00\\xfe\\x02\\x00\\x06\\xf5\\x01\\x18\\x01\\x05\\x01\\x01\\xfa\\xff\\xfd\\x01\\x06\\xfc\\x0e\\xff\\xed\\x02\\xfa\\x03\\x04\\x04\\x01\\x05\\x03\\x00\\x07\\xf8\\xff\\xff\\x01\\x03\\xfc\\x17\\x01\\x14\\x01\\xfa\\x06\\x00\\x01\\xf9\\xfe\\xfe\\x02\\x01\\x06\\xfa\\x12\\xfe\\xfe\\x02\\xff\\x0c\\xe7\\x06\\t\\x00\\x1b\\xd2\\xf8\\x02\\x02\\x0c\\xfb\\x02\\x01\\xe6\\x01\\xe6\\x13\\xec\\n\\xf8\\x00\\xf6\\x1f\\xef\\xf0\\x0c\\xfe\\xf8\\xf1\\x15\\x0b\\x0e\\xf8\\xf4#\\xea\\x1f\\xd8\\xe7\\x04\\xe1\\xfb\\x04\\x1b\\xf4\\xf2\\x0e\\xf9\\x02\\xef\\xf8\\x05!\\xfc5\\x01\\x07\\x06\\x0b\\xef\\xf6,\\n\"\\x15\\xdc\\xeb\\xe1\\x01*\\x06\\x0b\\xa7\\xcb\\xee\\n\\xfb\\xfa\\xd3\\x04\\xd1\\'\\x19\\x0eD\\xf3\"\\x04\\x08\\x05\\x0e!\\x14\\x18\\x14I\\x04\\xee\\xea\\xf6\\xd8\\x16V\\x17\\x10\\n\\x0b\\xeb)\\x18\\xe6\\xf5\\xcb\\xea\\x1f\\xb7\\x1d\\xf2\\xe8\\x17\\xf5\\x19\\x1bH\\x07\\x02\\x13J\\xe2\\x15\\x1a\\x12\\xd0\"5-\\xc1\\xed\\xf2\\xe6\\xf4\\x12\\xf4\\x1c\\xd2\\xe2\\xf4\\xdf&\\r\\n\\xf0\\x18\\xff\\xee\\xda\\x05G\\xebL\\x0c\\xe7\\x1c\\x08\\xfb0\\xec\\xf2\\x10\\x06!\\x02$\\x10\\x06\\x10%\\xf1\\xea\\x18\\xd5@\\x0c\\xf3\\xfd\\xf7\\xee\\x10\\xf6\\x18\\x1e\\xf3\\x07\\xeb\\x08\\x00\\x0c\\x0e\\xf9\\x10\\t\\x04$\\xe7\\t\\x13\\xf6\\xf6\\x05\\x00\\xfa\\x03\\xdc\\x17\\x13\\xff\\x10\\xeb\\xf0\\x08\\xfd\\r\\xfd\\t\\xfd\\xfb\\x02\\xf8\\x04\\xfc\\xf9\\xfe\\xff\\x02\\xff\\t\\xff\\x0c\\x03\\xfb.\\xfb\\x06\\xfb\\x04\\x03\\xfc\\xff\\xf5\\x04\\x1b\\xe8\\n\\x02\\xfa\\x08\\xef\\xfd\\x18\\xf2\\xf9\\xfd\\xf2\\xf5\\x03\\x0f\\x07\\x01\\x07\\xf0\\xf7\\xefi\\xd0\\x0e\\x00\"\\x03\"\\r\\x04\\x16\\x16\\xb3\\xf21\\xf8\\xda\\x16\\xe0\\x04\\xc8\\xdaL\\xd5\\x05\\xcc\\xc4\\x1e\\xf5\\xf0\\x12\\x03\\xeeE\\t\\x0f\\x17*\\x0c\\x13\\'\\xef\\x12\\xf8\\x9eC1\\x13\\xfe\\xe6\\x08\\xca\\x16\\xce*,\\x0b\\x1c\\x1e\\xf9\\xe7\\x17\\x06\\xd5\\xd6A\\x16\\x0c\\xe9\\xcf\\x02\\x11\\x01\\x0e+\\xf6\\xe8\\x12\\xe8\\x11\\xd4\\xe9\\xdc\\x12\\xfe\\x07\\xe1%!\\xff\\x12L\\xf4\\xf7\\xf5)\\xd7f\\xdc\\x00\\xfd\\xe2:\\x00\\x12\\x0b\\x17\\x16\\xe6\\x15\\x0f\\xe1\\xfa\\xe9\\x0f!\\xd9\\xca%D\\xf4\\xd1\\x11\\xdc\\x16\\xe7\\xee2\\xf4\\xc7\\xf8\\xef \\x12\\x05+\\x14\\xea\\xf3\\x1b\\x81\\x1e<#0\\x1c\\xe2\\x01\\r\\xe2<\\xc1\\x0c!\\x03k\\t&\\x04\\xe6\\xd3i\\xdc\\xfc!\\xc9A\\x0f!\\x11\\x12\\xe9\\xd8\\xfc!\\x0e\\xe3\\xf2\\xe8\\t\\x00\\x03\\r\\n\\n\\n\\x02\\x1b\\x08\\x07\\t\\xf2\\xe7\\n\\xfa\\x0e\\x1f\\xe8(\\x11\\xf9\\x00\\x00\\x05\\xf7\\xff\\x02\\x07\\t\\xfe\\x05\\x03\\x04\\x02\\x00\\x02\\xfe\\xff\\xff\\xfe\\xf6\\x04\\x13\\xfc\\x03\\x13\\xf6\\x01\\xfb\\x02\\xfe\\x05\\x07\\xf4\\xfc\\r\\xf8\\xf3\\xff\\xfd\\xee\\x0f$\\x16\\xef\\xf0\\xf2\\xe4\\xf7\\x0f.\\x10\\xf2\\x05\\x05\\xee\\x083\\xc7\\x1f\\x12\\x1c\\x12\\x16\\'\\x10\\x06\\x03\\xd2\\xeb%\\x01\\xd1\\xe2\\xfb\\x03\\xf5\\t\\x14\\x05\\x19\\xe9\\xc7\\xd7\\x06\\x06\\xf3\\x07\\xe2\\xd2+\\x17\\xfc\\xd2\\x11\\x0b\\xe2\\xe7\\xfd\\xe8\\x119\\xe7\\x1d\\xe34\\x0b\\x03\\x00/\\xb12\"\\xf0\\\\\\x1e\\x05\\xfd\\x00\\xe8\\x05\\xfe\\t\\xde\\xee\\xdd\\x07%\\xe0\\xf4\\xec\\x00\\x1d\\xf1\\xf1\\xf3\\xe0\\xf6\\x07\\x19\\x00*\\xc9\\x06\\x0b\\xd1\\x04\\xf6\\xf4\\xe5\\x053\\x11\\xe7\\xfe\\xe8\\x19\\x0f\\x04\\x0b\\xdc\\x14\\x11\\xefC\\x0c\\xf4\\xecL\\x01\\x05.\\xea\\xcd2\\x01\\xec\\x14\\xf8\\x00\\x0b\\xf3\\x0e\\x1b\\xd4\\xcb\\xfa\\xef\\xe5\\xe8\\xef\\x1c\\x13\\x12\\x0f\\xea\\xca)&$M\\x15\\x13\\x11\\x16\\x04\\x04\\xa2\\t\\x08\\xea%\\xf4\\x0e\\xec\\xfc\\xe2^\\xe6\\x04\\xf3\\xd7!\\xf5#\\t\\x1b\\xfe\\xf4\\xf7,\\xf8\\xe1\\xe5\\x0c\\r\\x0f\\xfa\\xff\\x12\\xf5\\x17\\n\\xef\\x19\\xfb\\xff\\xf4\\t\\x10\\xfe\\x08\\x16\\x04$\\x12\\xff\\x11\\x00\\x11\\xf2\\xf0\\xef\\x05\\x06\\x01\\t\\x05\\x05\\xff\\xff\\x03\\xfe\\x02\\x02\\x01\\t\\x02\\x0f\\xfc\\x05\\x06\\xf8\\x01\\xfd\\x02\\x04\\x02\\n\\x15\\xf7\\x13\\xff\\xf4\\xfd\\xff\\x15\\xf8=\\x06\\x12\\x01\\xdf\\xdb\\xfb\\x1e \\x0b\\xf1\\t\\xe7\\xed\\x0b \\xc5$#\\xee%\\r\\x10\\x13$\\x1a\\x01\\xe0\\x07\\x0e\\x16\\xfa\\x15\\xf1\\xe7\\x11\\xfd\\xf1\\x0e\\x03\\xfb\\x0e\\xee\\x17\\xfd\\xfd\\x02\\xd0\\x04\\x16\\xfb\\xe4\\xf9\\xf6\\xd8\\t\\x13\\xf7$/\\xf0\\x03)>\\x0b\\xf2\\xea\\xe8%\\xf8/\\xcc.(\\x18\\x12\\x04\\xd1\\x1d\\x06\\xf6\\x0b#\\x15\\xea\\x16\\xc9\\x17\\xf2\\x00\\x18\\x1d\\xe9\\xe30(\\x07\\x05\\x0e\\xb0N\\x17\\xfe\\xe7\\x15!\\xed\\x17\\x10\\xcc\\xfc\\xb2\\xff\\x05\\'\\x1e\\xdc\\x15\\xfc\\xf0\\x0b\\xe8C\\x00\\x1c\\xfan#\\x00\\x03\\xdd\\t\\xf7\\x0e\\xe0\\xfc\\x17N\\x17\\x08\\xf5(\\x184\\xec\\xe1\\xea\\xfa\\x0f\\x06%\\xfc \\xff\\xb9/\\r\\xef/\\xf98\\t\\x1a/\\xb9\\xd6\\xfb\\xf1\\xe8\\xeb\\x15\\x07\\xe7\\r\\xdd\\x1d\\xfa\\x1b\\xe7\\xe7\\x02\\xfd\\xfc\\x0c\\x1e\\x18\\x12\\xe8\\n!\\xd5\\xe8\\x13\\x1c\\x11\\xf5\\xf5\\x02\\xf6\\x16\\x05\\xfd\\x05\\xe7\\x18\\xf8\\x0c\\x11\\x04\\x04\\x13\\x10!\\x08\\x07\\xf8\\xf7\\x01\\xf1\\xf4\\xe5\\x02)\\x05\\xfc\\x08\\x06\\t\\x01\\x01\\xfe\\xfc\\xfb\\x03\\x02\\x08\\x02\\x02\\xfe\\r\\xfb\\xfe\\xff\\xf6\\x18\\xfb\\x03\\xe8\\xfd\\x0b\\r\\xfd\\xf2\\xf3C\\xe5\\x15\\x19\\x1e\\x04\\xf6\\x16\\xf6\\x0c\\xf9\\n\\xf4\\t\\x18\\xf3\\xfd\\r\\xe8\\x0e\\x17\\xf4;\\x07\\xff\\xf1\\r\\x08\\x02\\xde\\xf8\\x0b\\'\\xdf\\x14\\xf9\\x0f\\xf8\\x12\\xe8\\x07\\n\\xe70\\x05\\r\\xfe\\xf4\\x01\\xfd\\x05\\xfe\\xec\\x08\\xd0\\x06\\xfb\\xfd\\x0f\\x14\\t\\xdb\\r\\x1cP-\\xee\\x00\\x17\\xd94\\x0e\\n\\xe6\\x0b\\x0f\\x17$\\xf3\\xe7\\xff\\x16\\xfd.\\x1c\\x1c\\xf1\\x01\\x04\\x06\\xf8\\x0e\\xf0\\xd8\\xfa\\xeba\\xfb\\x137?\\x8d0\\r\\r\\x13\\x15!\\xe5\\xd3\\xf2\\xa3\\xd8\\x07\\xfd\\x0f\"\\xe5\\x1a\\xfe\\xf5\\x0b\\x03\\x0f\\'\\x11\\x1f\\x05f\\x1e\\xed\\xea\\xbf\\'/\\x1f\\xeb\\xe3\\xfe\\xf3\\x07?\\x13\\x17f,\\xe9\\xdf\\x07\\x16\\x02\\x08\\x18\\xe8\"\\r\\xc8\\x05C\\xe2\\x1e\\xf1+\\x11\\x18+\\xac\\xe3\\x03\\xe3\\xe9\\xf3\\xea\\xf8\\xf6\\r\\xd7\\x19\\x02\\x1c\\xf1\\t\\xfe\\t\\x06\\x13\\n\\x19\\x0f\\xdc\\xd9\"\\xed\\xdc\\x0c\\x16\\x0b\\x05\\xec\\xfa\\x05\\x18\\x04\\x01\\x05\\xf0\\x17\\xfe\\x00\\x13\\x00\\xff\\x01\\x01\\x0c\\x0b\\x00\\x0e\\xf4\\x0c\\xf9\\xf6\\xef\\xfd:\\x0e\\xf4\\x08\\x06\\x10\\x04\\x01\\xfd\\xf4\\xf7\\t\\x12\\x08\\x16\\x05\\x01\\r\\xfe\\xf6\\xff\\xf3\\x1d\\x01\\x01\\xff\\x00\\x19\\x13\\xfe\\xf2\\xeeQ\\xff\\xe2%\\x00\\x1f\\xff)\\xfa\\xf4\\xf7*\\xf5\\x01\\x1a\\x12\\xfa\\xf1\\x00\\xf4\\r\\xe1T\\x16\\xf8\\x01\\xec\\xf8\\n\\xf5\\x08\\x19\\x12\\xcf\\x08\\x14\\xfb\\x01\\xf7\\x13\\x14\\xed\\x1a8\\x03\\x06\\xfa\\x17\\xe6\\xf8\\x16\\xf2\\xff\\x05\\x1b\\x00\\xf5\\n\\xef\\xed\\xe7\\x08\\x1a\\xfb\\xeb0\\xc4 ,\\xa9)\\x05),\".\\xe8&\\xf4\\xff\\xd6\\xff\\x15\\xee2\\xd6\\x1d\\x00\\r\\x0e(\\xec\\xbb\\xf4*\\xef\\xe5\\xdf1\\xd6\\x03\\x0c\\xf8\\xfe\\xfb\\x0b\\r\\x18\\xfe\\xaf\\x1a\\xbd4?Z\\x01\\xee\\xf8\\x08$\\xd5\\xe8\\xf4\\t\\x06DM\\x1e\\x075\\xe5\\xe5\\xa4\\x1b\\x16\\xec\\x1c\\xb2\\x00\\xe9\\x10&\\xf72%\\xf6\\xfe\\xe4\\x1b,\\xf1\\x160\\x12.\\x06\\xf0\\x19\\xef\\xe4%\\xd9\\x06\\x1f\\x08#\\xb7\\xfa\\x12\\xef\\x05\\x08\\x06\\xeb\\xf7\\x01\\xe3\\x19\\xee\\x16\\xde\\x13\\x04,\\xf2\\xf1\\xf2\\xfa\\n\\xe4\\xf8\\x08\\x0b\\xea\\xf9\\xfa\\x01\\x08\\xfa\\x01\\x0b\\x00\\xff\\x06\\x07\\x00\\xf0\\xfe\\xfd\\x07\\xfe\\x06\\xfe\\xfc\\xfe\\x13\\xfc\\x00\\x01\\x11\\x00\\xf8\\x04\\x03\\x06\\x05\\x02\\x03\\x00\\x03\\x00\\x00\\xff\\x01\\x02\\xff\\x0e\\x01\\x10\\x02\\x03\\x00\\x02\\xfc\\xfe\\xfe\\x02\\xff\\x04\\x16\\xfa\\xf2\\x02\\x00\\xff\\xfe\\x1f\\x02\\xf0\\x0c\\xf8\\x0b\\x01\\x00\\xec\\n\\x0e\\xf7\\x06\\x02\\x15 \\x08\\xf8\\x00\\xf3\\x06\\xeb\\x18\\t\\xf5\\x05\\x0b\\x11\\x10\\xfe\\xfe\"\\x17\\xec\\x0e\\x16\\x15\\xea\\x01\\xf8/\\xe5\\x15\\xee\\x07\\xee\\xee\\x05\\xf7\\xf7\\xef\\xf6\\xf4\\xea\\x18\\xf8\\x14\\x08\\x0c\\xec\\xf5\\x04\\'\\xf3\\xecf\\xf3\\xf4\\x1a\\xcf\\xfa\\xd9>\\t\\xf6 \\x15\\xf1\\xf8\\xdf\\xf6\\x1c\\xf5\\xf9\\xee\\xf9\\xf4\\xde%\\x00\\x10\\xe6\\xc6\\x11#,\\xf4\\x13\\x08\\xd9\\xca8\\xeb\\x0b\\xf2\\x91\\xce1\\x03\\xd6\\xf8\\xf8\\x1253\\x0b\\xceV\\xfc\\x13\\xf7\\x16\\xc5\\x12!Q\\x04\\x14\\x1d\\r\\xda\\x1a\\xb8\\x14\\xf0\\x0f\\xfe\\xd4\\x1a\\xd3\\x0c\\x17\\xfe\\xfe\\xe9\\xef\\xd08\\xee=\\xfc\\x0f\\xe4\\x18\\n\\xff\\x04\\x19%\\xf0\\x1c\\xfc\\x07\\x0b\\x05\\x02\\xf8\\x02\\x00\\xfe\\x06\\xef\\x11\\r\\xe7\\xf4\\x00\\xfe\\xe1\\n\\xf9\\t\\xf4\\x0f\\xfd\\x05\\x06\\xf4\\x08\\xe0\\x11\\xff\\x05\\xfe\\x03\\x00\\x04\\x00\\x00\\x02\\x00\\xff\\xff\\xfe\\xfc\\x03\\x13\\xfd\\x02\\x00\\xfd\\xff\\xfe\\xff\\xfc\\x05\\x03\\xf6\\xfd\\x17\\x01\\xf6\\x01\\x01\\\\\\xdb\\xff\\xff\\x02\\xec\\xff\\xff\\x04\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\xcc\\x06\\x00\\x00\\xf1\\xfe\\xff\\xff1\\x00\\x00\\x00x\\xff\\xff\\xff:\\x05\\x00\\x00\\x19\\xfe\\xff\\xff\\x1e\\xfe\\xff\\xff|\\x02\\x00\\x00\\x97\\x02\\x00\\x00\\xbd\\xfe\\xff\\xff\\x07\\x06\\x00\\x00g\\x02\\x00\\x00\\x88\\xff\\xff\\xff2\\xfd\\xff\\xffD\\xff\\xff\\xff\\xd3\\xff\\xff\\xff\\x92\\x03\\x00\\x00\\x8e\\x01\\x00\\x00+\\x01\\x00\\x008\\xfe\\xff\\xff\\xac\\x04\\x00\\x00{\\x00\\x00\\x00\\xca\\xfb\\xff\\xffj\\xfc\\xff\\xff\\xa5\\xfe\\xff\\xff\\x86\\xff\\xff\\xff\\x10\\xff\\xff\\xffu\\xff\\xff\\xffA\\x01\\x00\\x00\\x1d\\x03\\x00\\x00p\\x03\\x00\\x00V\\x00\\x00\\x00\\x8e\\xec\\xff\\xff\\x04\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x12\\xe8\\x0b\\x16\\xe2\\xcf\\n\\x0f\\xe1\\x16\\xc0\\xd8\\x08\\xd1\\x0f\\xf9\\x11\\xf5\\x0b\\xeb\\xe5\\xfb\\x0c\\x1a\\x05\\x14\\xe5\\x13\\x16\\xf4\\xe2\\x0b\\xf0\\xbf\\xf1\\xff\\x19\\x0b\\x0c\\x02%\\xdb\\n!\\xf9\\xc1\\xf9\\x12\\x10\\x15\\xf3\\xf3\\xff\\x0e\"\\r\\xf0\\xee\\xeb\\n\\xfa\\x19\\xcc\\r\\xf7A\\xf8\\x0b\\x13\\xed\\x12\\x0b\\xfd\\xf7\\x08\\xf7\\xf1\\xf7\\x06\\xfb\\x07\\x14\\x07\\xd2\\xf9\\xfe\\x05\\xf9\\xf5\\x0b\\x03\\x0e\\x10\\x0e\\xf7\\xef\\xfe\\x19\\x12\\xe2\\x06\\xf7\\x07\\x07\\x15\\xf5\\xee\\x11\\x03\\xeb\\x03\\xf6\\x11\\xfe\\x01\\x1f\\x11\\xdf\\x0e\\x00\\xf5\\xe7\\x18\\x0c\\x11\\x07\\xf9\\xf7\\x05\\x1a\\xf6\\xfb\\x0c\\x02\\n\\t\\x00\\xff\\x1f\\x03\\n\\x89!\\x01\\t\\x0b\\x97\\x05\\xff\\x18\"\\x11\\xfc\\x02\\xfc\\x02\\xfd\\xf9\\xf8\\xf9\\x12\\xd8\\xf4\\r\\x18\\xe7\\x05\\xf4\\xeb\\xcd\\x0e\\x07\\xfd\\r\\xe1\\x0e\\x08\\xe3\\xef&\\xf3\\xf3\\xdd2\\xfa\\x1b\\xf1\\xfa\\xff\\xe6\\x13\\x02\\x06:\\x0b\\x1c\\x0b\\xec\\x10\\x04\\xf3\\xde\\x03\\xff\\xee\\xcf\\xe4\\x0c\\x0c\\xd5\\x1f\\r\\x05\\xe3\\x02\\x17\\x10\\x08\\xee\\xf4!\\xea\\x0b\\x06\\t\\x05\\n\\x0e\\xdf\\x03\\xfb\\x03\\x07\\xfc\\x15\\xfe\\x00\\xda\\n\\x0e\\x01\\xf8 \\xea\\xff\"\\xe3\\x00\\x10(\\x05\\x11\\xe4\\xf8\\xdf\\xf3\\xf3\\xfe\\xff \\xf9\\xff\\xfe\\x0c\\x07\\x00\\xfa\\xff\\xfe\\x17\\x07\\n\\xe1\\xfd\\xff\\x1e\\r6\\xef\"\\x00\\xec\\x02\\t\\xfa\\x03\\xf8\\x0f\\x07\\x81\\xf6\\x14\\x0e\\x07\\x06\\x07\\xe9\\xf0\\x0c\\xed\\xf5\\x10\\xf0\\x18\\x04\\xfa\\x1a\\x18\\xee\\r\\x0b\\xeb\\x07\\xf5\\x02\\x12\\x15\\xfa\\xdf\\xfb\\x0e\\x1e\\xfb\\r\\xf0\\xf5\\xfd\\xfb\\xf3\\xf7\\xf6\\xf6\\x0e\\xbd\\x1a\\x0b\\x0b\\x15\\xe0\\x1a\\x08\\xc8&\\xff\\n\\xff\\xff\\x11\\xf9\\x0c\\xe9\\x04\\x0f\\x12\\xf0\\x13\\xfb\\xdc\\x0f\\xff\\x0c\\xf7\\xec\\xfd\\xfa\\xd0\\r\\xfb\\xe1\\xf7\\xff\\xef\\x15\\xf2&\\xea\\xfd\\x13\\x05\\x15+\\x11\\xf5\\xff\\xf4\\xf9\\xf0\\xfd\\xf4\\x04\\x11\\x06\\xf3\\x01\\xf4\\xfc\\x0b\\xef\\x07\\xfe\\xf6\\x01\\xfa\\x02\\x05\\xf4\\x0b\\xf2\\x0f\\x01\\xf9\\xf7\\xfe\\xfc\\xf6\\xfc\\xf4\\xd6\\xf9\\xee\\xe4\\xef\\t\\x02\\xf8\\x0b\\x04\\x05\\n\\xf1\\x12\\x16\\xeb\\x12\\xff\\x1d\\xf9\\xe4\\xdd\\xed\\xfa#\\xfc\\r\\x0c\\t\\xfb\\t\\x00\\xfb\\xee\\xf8\\r\\xf4\\x08\\xf6\\x04\\xff\\xf7\\xf8\\xf2\\xf0\\xf6\\x01\\x06\\xfa\\x0f\\xf5\\xff\\xef\\r\\x06\\xf4\\xfe\\xf7\\xfc\\xfa\\xfe\\xee\\x12\\xfb\\x07\\r\\x1a\\x0b\\x00\\x00\\t\\xd4\\xf0\\xfd\\xe0\\xf5\\xf7\\xf8\\x03\\x1d\\xf4+\\x17\\xf9\\x15\\x1b\\x1a\\xf5\\xfe\\xe0\\xf3\\x00\\xd6\\xe8\\xf9\\x0c\\xd6\\t_\\t\\x07\\r\\xf0\\x03\\n\\xe4\\x01\\x07\\xf1\\xfa\\x0e\\xec\\x07\\xfa\\xf5\\x16\"\\xce\\xf1\\x10\\x07\\xff\\n\\xc9\\xfb\\xf9\\x01\\x02.\\xff\\xf2\\x04\\x13\\x00\\xf5\\x0f\\xf6\\x18\\xf2\\x05\\xf3\\xf4\\xc8L\\x06\\x0b\\xec\\x062\\xf9\\x06\\xf8\\xf5\\xfa\\xfd\\xd9\\xfc\\xf5\\x08\\x05\\n\\x0f\\xf1\\xc2\\x1d\\x03\\x06\\x18\\x0e\\xc0\\xfc\\xf6\\xf4\\x08\\x00P\\x0e\\x05\\xe9\\xf4\\x12\\r5\\x0b\\xbc\\xfc\\x08\\x0f\\xf6\\x10\\x06\\xf3\\xec\\xfd\\n\\xf2\\n\\xff\\xf2\\xf8\\x12\\x01\\xf4\\xfe\\xec\\xfa\\x01\\xf3\\x0b\\x02\\xed\\t\\x1cN\\x04\\xf2\\x05\\x07\\xfd\\x0b\\x02\\x14\\x0c\\n\\x03\\xef\\x15\\x11\\x0b\\xf7\\xf7\\x06\\x05\\xf3\\x06\\x04\\xf6\\r\\xfa\\n\\x04\\xe7\\n\\xfd\\xf3\\xc7\\xfc\\xf0\\xe9\\x0c\\x13/\\xf1\\xf5\\xf9\\x05\\xd8\\n\\xde\\x16\\xe8\\xfc\\xff\\xef\\xfa\\x04\\xf8\\x05\\x1d\\xf6\\x19\\xfc\\x00\\x04(\\x01\\xf1\\xfc\\x1d\\xfb\\x18\\xe5\\xfb%\\x0b\\xef\\xf7\\xfa\\x08\\xfe\\x05\\xd3\\xdc\\xf5\\xf9\\xee\\x0c\\x18\\x0b\\xf4\\x07\\xf4\\xde\\xf5\\x12\\x0f\\xff\\x01\\xcc\\x07\\x0e\\xf9\\xe9\\x10\\x15\\x1b\\x0b\\x95\\xfa\\xf1\\xf8\\x0c\\xd1\\xfd\\x07\\x03\\xfc\\x11\\xef\\xf9\\x14\\n\\x1a\\xf6\\r\\x16\\xf4\\x0f\\xf0\\xf9\\x1a\\x0f\\xf4\\xfc\\x0c\\x0f\\x10\\t\\x04\\n\\xf8\\x04\\x04\\x00\\xfc\\x01\\x14\\xf2\\x01\\xf7\\x08\\x00\\xf1\\xf8\\x03\\x07\\xeeA\\xf4\\x04\\x14\\x10\\x15\\xbe\\xf2\\x05\\x07\\xef\\xf5\\xfd\\n\\x1e\\x03\\r\\x02\\xff\\xed\\xf0\\xfa\\xf9\\xf9\\xef\\x02\\xf3\\xfb\\n\\x00\\xf0\\xfe\\xfa\\x06\\xee\\x0b\\xf2\\x06\\xfc\\xfc\\x0e\\r\\x03\\x0e\\x0b\\xf1\\xf8\\xf7\\xfe\\x19\\xee\\x0e\\xde\\xf6\\xf7\\xfc\\x06\\n\\xf4\\x05\\xfb\\xf8\\x0b\\x0b \\x01\\x07\\x17\\xfb\\x1b\\xe4\\xeb\\xf8\\x00\\t\\xfa\\xf9\\x03\\x0b\\x0b\\x11\\xf9\\xfb\\x0b\\x11\\xef\\x0b\\x04\\x00\\x0c\\x0b\\xee\\xf7\\x05\\xfb\\xf6\\t\\xf3\\x05\\x0b\\x00\\xf7\\xfd\\xef\\xf8\\x0b\\xfe\\xf5\\xec\\xee\\xf1\\xf2\\xf7\\'\\xfd\\xec\\x19\\x17\\xff\\xee\\xe2\\xfc\\x14\\xf8\\n\\x04\\xf7\\xeb2\\xd1\\x06\\xd3\\xf8\\x06\\xc5\\x03\\t\\x12\\x0f\\xf1\\x01\\xf9\\xe6\\x12\\x07\\t\\x0b\\xfe%\\x0f\\xfe\\x10\\x00\\xec\\t\\x03\\t\\xc5\\t\\x0e\\x04\\xe4\\xf4\\xd0\\xfd\\x04\\xec\\xfc\\t\\x1c\\x06\\x0c\\xdf\\xf2\\x12\\x11\\r#\\x0f\\xe8\\x04\\xee\\x04\\x00\\xf7\\x0e\\xfb\\x0b\\x0e\\xfd\\x07\\xfd3\\n\\xf5\\xfc\\xf2$\\xe6\\x04\\xff\\x07\\x01\\x11\\xe7\\x02\\xde\\x0f\\n+\\x0b\\xee\\x0c\\xf3\\x02\\xfa\\x00\\xfa\\x0b\\x04\\x10\\x11\\x00\\n\\x07\\n\\xfd\\xee\\t\\x8b\\x0e#\\t\\x02\\xec\\x03\\xe8\\xfd\\xff\\xf9\\xf8\\xdf\\xff\\xff\\x9e\\xf0\\xff\\xff\\x04\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc5\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x05\\x00\\x00e\\x03\\x00\\x00Y\\x03\\x00\\x00\\x00\\x00\\x00\\x00V\\xff\\xff\\xff\\x96\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x9e\\x04\\x00\\x00c\\xff\\xff\\xff\\xbc\\xff\\xff\\xff*\\x02\\x00\\x00\\x94\\x03\\x00\\x00\\xf0\\x01\\x00\\x00\\xfa\\x06\\x00\\x00\\x88\\xff\\xff\\xff0\\x01\\x00\\x00\\x1f\\x04\\x00\\x003\\x01\\x00\\x00\\xcb\\x01\\x00\\x00B\\x04\\x00\\x00\\xca\\x03\\x00\\x00\\x0f\\xff\\xff\\xff\\x96\\x02\\x00\\x00\\xa7\\x02\\x00\\x00 \\x03\\x00\\x00\\xe1\\x03\\x00\\x00\\xf8\\xff\\xff\\xff\\xd8\\xff\\xff\\xff\\x15\\xff\\xff\\xff*\\xf1\\xff\\xff\\x04\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\xf1\\xf0\\xfa\\x00\\x0f\\xef\\xf1\\xfa\\xf6\\xf2\\xf5\\xf7\\xef\\x12\\xf4\\x03\\xf0\\xb3\\xf1\\x00\\xd4\\x06\\xf8)\\n\\x16\\xf1\\xf5\\t\\xfb\\t\\x10\\xfb\\xfb\\xfd\\xf1\\x10\\x0e\\xfb\\x0f\\x0c\\xef\\xf5\\xf9\\xf1\\xf6\\r\\xfd\\xfd\\x81\\xf3\\x1a\\xec\\x0c\\xf3\\x0e\\x17\\xfe\\xf8\\xf5\\xe9\\x08\\xe7\\xf0\\x08\\xec\\xef\\x08\\xef\\x11\\x10\\x01\\x05\\x18\\xef\\x0c2\\xfa\\xfb\\xfa\\xe4\\xea\\xf4\\x11\\xfe\\x05\\x05\\xd9\\r\\xff\\xf1\\x07(\\x0f\\x06\\xff\\xfe\\x03\\xfa\\xf4\\xf4\\xff\\xfe\\x07\\xf3\\x02\\x0b\\xf3\\xfc\\xf3\\xf4\\x00\\xfd\\xfa\\x03\\x08\\n\\xf0\\xfe\\xf9\\xf1\\x06\\xfa\\xee\\xf4\\x02\\r\\x04\\xf4\\xff\\t\\x12\\xf1\\x08\\x02\\xe3\\xf7\\x0e\\xfd\\x07\\xd8\\x11\\x11\\x11\\xf4\\x00\\x10\\x03\\x03\\xf7\\xfe\\xe4\\x0e\\n\\x12\\t\\x04\\x10\\xf3\\xf5\\x0c\\xec\\x01\\x1e\\xf1\\x12\\x08\\xe5\\x16\\x02\\xf0\\x05\\x11\\x05\\xf8\\x0f\\xe7\\xf7\\x05\\xfc\\xfe\\x03\\x10\\n\\x00\\x0b\\x03\\x05\\xf3\\t\\xf4\\r\\xf4\\xfe\\x04\\x0b\\xfa\\xf5\\xf6\\xfd\\x03\\xee\\x0e\\xfa\\xfe\\x01\\xf3\\x10\\x1f\\x08\\x0f\\xdc\\n\\xfa\\xf7\\x11\\xd9\\xe0\\x13\\x01\\xf2\\x01\\xfe\\xfe\\xfa\\x03\\x03\\x12\\xfc\\xf3\\x10\\xfc\\r\\xf0\\r\\xf3\\x1a\\n\\x11\\x06\\xde\\xf3\\xfd \\xe4\\x06\\x07\\x1c\\x07\\xfc\\xed\\x10\\'\\x02\\x08\\x03\\xc5\\xdb\\xf0\\x14\\xfe\\x01\\xf0\\xf2\\x13\\xfa\\xff\\xfc\\xf2\\x16\\xe7\\xf2\\xf5\\x11\\x0c\\xec\\x15\\x0b\\xfe\\x00\\xec\\n\\x02\\x0f\\xf3\\x04\\x12\\x05\\xf0\\x17\\x12\\xf8\\x0c\\xf9\\x08\\xad\\xfa\\xfa\\x0e\\xf8\\'\\x0e\\x01\\xee\\xe8\\xd6\\xf8\\x1a\\xdc\\x10\\xee/\\x0e\\x12\\xea\\x12\\x01\\xfa\\xf8\\x04\\x17\\x02\\x0f\\xfc\\x06\\r\\x0c\\x11\\x08\\x0e\\xfd\\xff\\xc8\\xf3\\x06\\x02\\x0f\\xeb\\x11\\n\\x17\\x0b\\xe9\\xde\\xef\\xf9\\xff\\xdf6\\x00\\xe6\\x06\\xfe\\xd2\\n\\x15\\xdb\\x1f\\xe3\\n\\x10\\x04\\xf5\\xee\\xfb\\x01\\xe0\\x05\\x1b\\xdb\\x04\\x07\\xf7\\x0f\\xed,\\x16\\xf9\\xfc\\xfd&\\x0c\\x06\\x02\\xef\\xee\\xef\\xfd\\x01\\t\\xff\\x08\\x02\\x00\\xf8\\x04\\xfa\\xec\\xf2\\xff\\xe2\\xce\\xf3\\x1a\\xe0\\r\\xf9\\x0b\\x17\\x11\\xf4\\x15\\x10\\x10\\x08\\x01\\xdd\\x0b\\xf8\\x07\\xfb\\x0f\\x05\\xf6\\x12\\x04\\xf3\\x06\\x03\\xfc\\x15\\xff\\x07\\xf6\\x0b\\r\\x04\\x02\\xfe\\n\\x0e\\xfa\\xee\\x07-\\n\\x05\\xf2%\\xf2\\xf8\\x03\\x0b\\x0f\\xeb\\x08\\xfb\\x00\\r\\xe9 \\x05\\xea\\xf4\\t\\x07\\x0f\\xfa\\xf1\\xef\\r\\xff\\x0b\\x14\\x0f\\x04\\x01\\xf8\\x0c\\xff\\xbd\\x06\\xf4\\x14\\xf5\\x15\\xfe\\n\\x04\\x16\\xf1\\x04\\xf8\\n\\x13\\x04\\xfb\\xfc\\xfc\\xfe\\x0f\\x02\\xfd\\x07\\xf0\\xed\\x03\\r\\x00\\xec\\xfb\\xfd\\x94\\xe2\\xff\\xff:\\xf3\\xff\\xff\\x04\\x00\\x00\\x00@\\x00\\x00\\x00\\xea\\x00\\x00\\x004\\xf4\\xff\\xff\\x00\\x00\\x00\\x00j\\x0f\\x00\\x00\\xa6\\xfc\\xff\\xff\\xdf\\n\\x00\\x00\\xda\\xf4\\xff\\xffS\\n\\x00\\x00Q\\n\\x00\\x00I\\x03\\x00\\x00G\\xf9\\xff\\xff\\xa3\\xfd\\xff\\xffm\\x19\\x00\\x00\\xb5\\t\\x00\\x00\\xc5\\xf1\\xff\\xff\\x00\\x00\\x00\\x00\\x86\\xf3\\xff\\xff\\x04\\x00\\x00\\x00 \\x00\\x00\\x00\\xae]\\x08e\\xaa\\xc0\\x18\\xdf\\rx\\'\\xeeN@)\\xa1&\\xe5_\\xe2\\x13p]\\n\\x81\\x12#;f\\x18\\xfa\\xae\\x10\\xe3\\xff\\xff\\xb6\\xf3\\xff\\xff\\x04\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\xc8\\x03\\x00\\x00\\xa4\\x07\\x00\\x00\\xca\\xf3\\xff\\xff\\x04\\x00\\x00\\x00 \\x00\\x00\\x00\\xf9[\\xb3\\xd0>k\\xba\\x07&(\\xc5\\x15\\xad\\xb9\\xd3Z\\x99\\xeb\\x1a\\xf4M\\x13\\xf8V\\x0e\\x81\\xb6o\\xe8L\\xbe\\x10T\\xe3\\xff\\xff\\xfa\\xf3\\xff\\xff\\x04\\x00\\x00\\x00@\\x00\\x00\\x00Y\\xff\\xff\\xff\\xc0\\xff\\xff\\xff\\n\\x00\\x00\\x00\\xb1\\xff\\xff\\xff\\xc9\\xff\\xff\\xff\\x06\\x01\\x00\\x009\\xfe\\xff\\xff\\x88\\x01\\x00\\x00\\xde\\x00\\x00\\x00\\xc0\\xff\\xff\\xffz\\xff\\xff\\xff\\xfb\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x8f\\xff\\xff\\xff\\xa0\\xff\\xff\\xff\\xab\\x00\\x00\\x00F\\xf4\\xff\\xff\\x04\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\xfd\\xfb\\xfb\\x04\\xff\\x01\\x04\\x01\\x01\\xfc\\x03\\xfe\\xfe\\x00\\xf0\\t\\x02\\x08\\xfb\\x06\\x03\\x04\\x07\\x03\\xff\\xfb\\xfd\\xf6\\x03\\xff\\xfa\\x02\\xfa\\xf9\\xf9\\xff\\xf9\\xfc\\xfb\\x08\\x01\\xff\\xfd\\x00\\xfb\\xfa\\x02\\xfc\\xfb\\t\\x05\\xfb\\xf8\\x02\\xf9\\xfa\\xfa\\x01\\x05\\xfb\\xff\\x01\\xfe\\xfb\\x00\\x06\\xfc\\x01\\x04\\x0b\\x01\\xfe\\x01\\x02\\x00\\xff\\x02\\x04\\xfb\\xfe\\xfe\\x04\\xfe\\n\\x02\\x06\\x05\\t\\n\\xfe\\x01\\xee\\xfe\\x05\\xfc\\x04\\xfd\\xf8\\xfc\\xfd\\xfa\\xfb\\xff\\x04\\xfc\\xfe\\x00\\xfe\\xfd\\x01\\xfa\\xf8\\xfc\\xf9\\xfe\\xfc\\x02\\x06\\xf7\\x04\\xfe\\x01\\x03\\xfe\\xfb\\xfb\\x01\\x02\\xfd\\xfe\\xfb\\xf8\\x05\\xfa\\x04\\x00\\xfe\\xf9\\x04\\xf9\\xfb\\x01\\x02\\xfa\\xfa\\xfe\\x00\\x00\\xfd\\xf9\\xfa\\xfd\\x00\\x00\\xf7\\x01\\xfd\\xf8\\xfb\\xff\\x06\\x08\\xf7\\xfc\\xfa\\x01\\xfe\\xfa\\x02\\x04\\x08\\xf9\\xfd\\xf8\\xf4\\x02\\xf8\\x08\\x05\\t\\xfd\\n\\xff\\x03\\x08\\x03\\xf9\\xee\\xfa\\x06\\x01\\x07\\xf9\\xf6\\xf9\\xfa\\x00\\x05\\xff\\x00\\xfb\\xff\\x08\\xfd\\x03\\x02\\xef\\x01\\x04\\x02\\x06\\xfc\\xfb\\xfc\\xfe\\x00\\x04\\x00\\xfb\\x06\\xef\\x02\\x02\\x01\\xf8\\xf9\\x07\\x01\\xfc\\xfd\\x03\\xee\\x06\\x04\\xfc\\xf8\\xe8\\xfd\\x03\\xf5\\xfa\\x17\\x01\\xea\\xf9\\x06\\x02\\xfd\\xf7\\x07\\xfe\\x03\\xfc\\xfe\\xfa\\xff\\x04\\xfb\\x03\\x06\\x08\\x05\\x06\\x03\\x04\\xfb\\xfa\\x08\\x04\\n\\t\\x00\\x01\\xfe\\xfa\\xf3\\xff\\x04\\xfb\\xf8\\x04\\xfa\\x06\\n\\x08\\x01\\x06\\xfb\\xde\\x0c\\xfa\\t\\x01\\x07\\x8a\\xf9\\xff\\xfd\\x07\\x00\\x08\\x05\\xff\\xd0\\x04\\xfb\\xff\\xfe\\x81\\x07\\xff\\x04\\xff\\xfa\\x02\\xfa\\x01\\x07\\x07\\x02\\xfc\\x04\\xfa\\x01\\xf8\\xfe\\x01\\xfa\\x00\\xfd\\x02\\xf7\\x00\\xf7\\xfd\\xfc\\x04\\xfe\\x05\\x01\\xfd\\xfe\\x02\\x03\\xf7\\x02\\xfd\\xfd\\xfe\\xff\\x06\\xfe\\x02\\xf8\\xfb\\x05\\x04\\x04\\x00\\xf7\\xfe\\x01\\n\\x04\\xfe\\x05\\x00\\x0c\\x02\\x08\\xfa\\xfb\\x07\\x05\\xfe\\x01\\xfd\\xfa\\x03\\xfe\\x02\\x08\\x06\\x02\\xfb\\x06\\xfd\\x04\\x00\\xff\\xe7\\xfb\\xfd\\xee\\x04\\xf9\\xfa\\x03\\x02\\x01\\xf8\\x00\\xfc\\xff\\xf0\\x04\\xfd\\t\\xfa\\xfd\\xfe\\x04\\x04\\xfc\\x05\\x03\\x01\\xf7\\xfc\\x02\\xf9\\xfb\\xff\\x00\\xff\\xfd\\x02\\xfc\\x05\\xfe\\xf8\\xfe\\x00\\xf9\\xff\\xfd\\x00\\x02\\x01\\xfc\\x05\\xfb\\xfc\\x03\\xfd\\xf7\\x06\\xfa\\xfa\\xfb\\xfe\\x05\\xf9\\x02\\xfd\\x05\\x03\\xfc\\xf9\\xfe\\x02\\xf8\\xf9\\x04\\x04\\x01\\xfa\\xfe\\x01\\xfd\\xf9\\xfd\\xff\\x00\\xfe\\xfa\\x03\\xfe\\xff\\xf5\\x01\\xfc\\x01\\x03\\x05\\x04\\x01\\x02\\x01\\x07\\xfe\\x05\\x00\\r\\x0b\\xfb\\x06\\xfd\\xfc\\xf2\\x03\\x04\\x03\\xfb\\xfa\\xf9\\xfb\\x10\\x0e\\x04\\x0b\\xfb\\xb0\\xe5\\xff\\xffV\\xf6\\xff\\xff\\x04\\x00\\x00\\x00\\x80\\x00\\x00\\x00Z\\x00\\x00\\x00\\x9e\\x00\\x00\\x00&\\xfe\\xff\\xff\\xa1\\x03\\x00\\x00\\x8f\\x00\\x00\\x00 \\x01\\x00\\x00a\\x00\\x00\\x00J\\xfd\\xff\\xff\\x7f\\xff\\xff\\xffV\\x01\\x00\\x00\\x14\\x01\\x00\\x00r\\x00\\x00\\x00\\xc3\\x00\\x00\\x00\\x04\\x02\\x00\\x00\\x89\\x01\\x00\\x00Y\\x00\\x00\\x00T\\xff\\xff\\xff\\xcc\\xff\\xff\\xff\\'\\xfe\\xff\\xff\\x84\\x00\\x00\\x00\\x98\\x00\\x00\\x00\\xae\\x00\\x00\\x00E\\xff\\xff\\xff\\x86\\x00\\x00\\x00\\xaf\\xfc\\xff\\xffL\\x00\\x00\\x00f\\xff\\xff\\xffs\\x02\\x00\\x00\\xc1\\xff\\xff\\xff\\x8b\\x02\\x00\\x00v\\x01\\x00\\x00\\xef\\x00\\x00\\x00\\xe2\\xf6\\xff\\xff\\x04\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x02\\x02\\x01\\xfe\\xf9\\xff\\x07\\x01\\xf6\\x00\\x00\\xfa\\xfd\\x02\\xfe\\x01\\xfd\\x05\\x05\\x04\\xfd\\xfe\\xfd\\xfd\\x05\\x03\\x00\\xfa\\xfc\\x02\\xfc\\x05\\xfc\\xff\\xfb\\x06\\t\\x03\\x04\\xff\\x07\\x04\\x00\\x03\\x02\\xff\\xfe\\x05\\x02\\xf7\\x03\\xfb\\xff\\x05\\x04\\x06\\x00\\xf2\\x03\\xfd\\x07\\xfe\\xfe\\x04\\xfe\\xff\\xff\\x01\\x01\\x03\\x03\\x04\\xe7\\x01\\x01\\xfc\\xfd\\x05\\x05\\x04\\x00\\x01\\x02\\x00\\x01\\x01\\x04\\xfa\\x01\\x05\\xfd\\x00\\xfe\\x02\\xfb\\x00\\xf3\\x02\\xff\\x00\\x00\\xff\\x04\\xfe\\xf5\\x00\\x03\\x01\\xd1\\x02\\xfd\\x00\\x01\\xfe\\x05\\x04\\xfa\\xfd\\xfb\\x00\\xde\\x81\\x03\\x00\\xfe\\x04\\xfd\\xfe\\x04\\xff\\x06\\xfc\\xfd\\x02\\x00\\x02\\x00\\x02\\x04\\xee\\xfe\\x00\\x00\\xfe\\x01\\x05\\xfd\\x00\\x04\\x07\\xfd\\xf9\\xf7\\x04\\x03\\x02\\x01\\xf9\\xee\\x00\\x00\\x02\\x05\\xff\\xf1\\xfe\\x05\\x00\\x02\\x01\\x03\\xfe\\x01\\xf7\\x02\\xf8\\x00\\x01\\x00\\x01\\x08\\x03\\xf5\\xfc\\t\\xfc\\xfe\\x0b\\x03\\xfb\\xfe\\x04\\x03\\x05\\xfb\\xfe\\x07\\x04\\xfb\\x01\\xba\\xf4\\xfe\\xf1\\x05\\x04\\x03\\x03\\x05\\xfc\\xfa\\x02\\x08\\xfa\\xfc\\x05\\xf9\\xfd\\x00\\xfe\\x05\\xed\\xf9\\xf4\\xf7\\xfd\\xfc\\x06\\x05\\x02\\x07\\x03\\xff\\xff\\x0b\\x01\\xdf\\x02\\xff\\x02\\xfc\\xfe\\xf6\\x00\\xff\\xfd\\xfa\\xfd\\x02\\xfc\\x01\\xfe\\x06\\x00\\x03\\xfd\\x02\\x02\\x01\\xfe\\x04\\x07\\xff\\xfd\\x04\\xfe\\x01\\xfa\\x00\\x03\\x08\\x04\\x02\\xfe\\x03\\xff\\x03\\x04\\xfb\\x06\\xfe\\xff\\x04\\x04\\x01\\x04\\x0b\\xfc\\xfe\\xfc\\xff\\x03\\xff\\x00\\x02\\x03\\x04\\x02\\x03\\xfa\\xfd\\x04\\xfd\\xfe\\x00\\xfd\\xfc\\x00\\xfc\\xfc\\x01\\x00\\x03\\x02\\x03\\xfe\\x01\\x03\\x00\\x00\\xfd\\xff\\xfd\\x06\\x05\\x0b\\x04\\x05\\xfd\\xf9\\t\\x00\\xfc\\xff\\x01\\x01\\x06\\xfd\\xfc\\x07\\t\\x04\\x05\\x03\\xff\\x02\\xfa\\x01\\x06\\x05\\x07\\xff\\x04\\x03\\xfe\\n\\x0f\\x04\\x02\\x01\\t\\x02\\x00\\x00\\xf6\\x05\\xff\\n\\x02\\x00\\xfd\\xff\\xfd\\xfd\\x06\\xfd\\xf9\\x05\\xfe\\xf5\\xfd\\x02\\x02\\xfe\\x06\\x05\\xfd\\x01\\xf4\\x00\\x08\\xff\\xeb\\n\\xff\\xf9\\xfc\\xfe\\xff\\xff\\x01\\x0b\\r\\t\\xfe\\xfd\\xfc\\xfd\\x08\\x02\\x00\\xf4\\x02\\x07\\xe9\\r\\x02\\xfa\\xf8\\x05\\x14\\x05\\x00\\xfb\\xfc\\t\\xfe\\x02\\xfd\\xfa\\xfb\\x0c\\x10\\xf0\\xfe\\xf9\\x04\\x0c\\x03\\x0b\\x02\\xf0\\x07\\xfe\\x0b\\x01\\xf6\\n\\xff\\x00\\x02\\x00\\x01\\xfd\\x01\\xfc\\xfb\\xfe\\x00\\xf2\\xf6\\xfd\\xff\\xfc\\xfe\\x01\\x01\\x02\\xff\\xfe\\x03\\x08\\x06\\xfb\\xfc\\xff\\xfe\\x05\\x14\\x07\\x00\\x02\\xfa\\xff\\r\\x03\\xfc\\x00\\xfe\\x04\\xfc\\x02\\x06\\x02\\xfc\\x08\\x07\\xf8\\x01\\x05\\xfd\\xfa\\x0c\\x00\\xfd\\xf9\\x03\\xfb\\x01\\xfb\\x04\\xfe\\xfc\\xfe\\xfc\\xfd\\x00\\xfb\\xfb\\xfc\\x03\\x00\\xfd\\x00\\x02\\x01\\xfb\\x03\\xfb\\xff\\x04\\xff\\xfa\\xff\\xfb\\x02\\xfc\\x00\\xfd\\xfb\\xfc\\xfc\\xfb\\x02\\xfd\\xf9\\xfb\\xfb\\xfd\\x04\\xfa\\x01\\xfc\\x03\\x00\\x02\\xfc\\x03\\xfb\\x00\\x05\\x00\\x00\\xfb\\xfa\\xf8\\xfd\\x00\\xfa\\xfd\\xfb\\xf8\\xfc\\x01\\x01\\xff\\xfd\\x01\\x04\\xfd\\x00\\xfa\\xfe\\xff\\x04\\xff\\xfe\\x00\\xfd\\xff\\xff\\xff\\x02\\x04\\x03\\xff\\x05\\xfe\\xfd\\xfb\\xfa\\xff\\xfc\\x01\\xfe\\xfe\\xfe\\x04\\x00\\x00\\x02\\xff\\x02\\xfd\\xfd\\xfb\\xfc\\x03\\x00\\xfe\\x03\\x02\\xfc\\x04\\x00\\xfe\\xff\\x00\\x04\\x02\\x06\\x02\\xf5\\x04\\xfe\\r\\x01\\x00\\xfb\\xff\\xf6\\x01\\x01\\xfa\\xfc\\xfd\\x06\\x06\\x01\\xd7\\xfc\\x04\\xfa\\xf8\\x02\\x00\\x03\\xff\\xfd\\xf5\\xff\\xfa\\xff\\xf5\\xfe\\xff\\x05\\xfd\\x03\\xeb\\xd8\\xeb\\x05\\x03\\x02\\xf5\\xce\\xff\\xff\\xfa\\x05\\x00\\xfb\\xef\\xb6\\xfd\\x02\\xf1\\xe6\\x00\\xfa\\x03\\n\\x0b\\xf3\\x05\\xfe\\x05\\x06\\xff\\xfd\\x03\\xf1\\x03\\xfd\\x04\\xff\\x02\\x01\\xfc\\x01\\x04\\x01\\xfe\\x01\\xfd\\x01\\xff\\xfe\\x03\\x03\\xfd\\x01\\xfc\\x00\\xfb\\x04\\xfe\\x02\\x00\\x02\\xfd\\x05\\x00\\x07\\x00\\xfe\\x00\\xfe\\x03\\x00\\x04\\x01\\xff\\x00\\xfc\\x02\\xfe\\xfe\\x00\\x03\\x01\\xfe\\x05\\xfc\\x01\\xf8\\x00\\xfe\\xfe\\x05\\x01\\x00\\x00\\xfd\\x04\\x01\\xfd\\x02\\x00\\x05\\x03\\x01\\x01\\xfd\\xf8\\x98\\xff\\x02\\xfc\\x02\\x02\\x00\\xfe\\x02\\xfe\\xfe\\xff\\x02\\xfe\\xfe\\x06\\x03\\x03\\x02\\x01\\x02\\xff\\xe6\\x02\\xfb\\xfc\\xff\\x05\\x00\\x03\\xff\\x02\\xfd\\x02\\x05\\xfd\\xfc\\x02\\x03\\x01\\xff\\x02\\xff\\xff\\xfb\\xfe\\xfe\\x01\\x04\\x00\\xfa\\x07\\xfe\\x00\\x06\\x01\\xff\\xfd\\xfd\\x0b\\x01\\xff\\xfd\\x02\\xff\\xde\\x05\\xef\\x00\\x05\\x03\\x04\\x03\\xfe\\xfd\\x05\\x05\\xfc\\xff\\x02\\x02\\x03\\x00\\xfc\\x04\\xf9\\xf9\\xf8\\x01\\x01\\xfe\\n\\x01\\xff\\x06\\xfe\\x03\\xfb\\x05\\xfc\\xaa\\x01\\xfc\\x08\\xf9\\x00\\xff\\x02\\xfb\\x03\\t\\xff\\xfd\\xff\\x02\\x01\\xfd\\x03\\xe7\\x01\\x02\\x02\\x03\\xfe\\xf6\\x03\\x06\\x01\\xff\\xea\\x06\\xef\\x03\\xfc\\xf7\\xff\\x05\\xfe\\xff\\xfe\\x02\\xfe\\xfc\\xff\\x03\\x01\\x01\\x03\\xfc\\xf7\\xf1\\xf0\\x04\\xfb\\x04\\x00\\xff\\xff\\x03\\x01\\xfd\\xff\\x03\\xfa\\xe0\\xff\\xfe\\xfe\\xe9\\x02\\xfa\\xfd\\x08\\x02\\xf6\\x02\\xff\\x00\\x00\\x05\\xf9\\x03\\xf3\\x02\\x04\\x06\\xfe\\x01\\xfa\\xff\\x05\\x03\\xf9\\xfa\\x01\\xf3\\xff\\xff\\x01\\xff\\xfd\\x06\\x08\\x05\\xfe\\xfe\\xfb\\x01\\x08\\x02\\x00\\xf7\\x02\\t\\x0b\\x02\\xfd\\xfb\\xf8\\t\\x0e\\x01\\xff\\xfc\\x07\\x03\\x03\\x04\\xfa\\xfc\\xfd\\x07\\x03\\xf0\\xfe\\xfe\\xf7\\xfa\\t\\xf7\\x02\\xf5\\x04\\xfd\\x07\\xfb\\x04\\xfa\\xfb\\x02\\xff\\x02\\xfe\\xf9\\xfc\\xff\\xf9\\x01\\x06\\x02\\x01\\xfe\\x02\\x03\\x04\\xfe\\x03\\xfd\\x04\\xfd\\x03\\x02\\xfb\\xfe\\xff\\xff\\x02\\xfb\\xf3\\xfe\\x04\\xfd\\x01\\x00\\xfe\\x01\\x03\\x01\\x04\\x00\\xfe\\xfe\\xfd\\x02\\x03\\x04\\xf6\\x03\\x00\\x00\\x03\\x02\\xfd\\x02\\xfe\\x04\\x02\\xfd\\xfd\\x02\\xfe\\xfe\\xf9\\xfa\\x02\\xfe\\xf1\\xfd\\x00\\xfb\\xf9\\x06\\x04\\xf5\\x01\\xfc\\xff\\x02\\x05\\x02\\xfb\\xfe\\xfc\\x05\\xfc\\xff\\x06\\x01\\xfd\\xf7\\xfe\\x07\\x18\\x07\\xfc\\xf2\\xfa\\x07\\xfc\\x01\\x01\\x00\\xf6\\xfa\\xfe\\xed\\xf5\\xfd\\x01\\x02\\x0b\\xf3\\xf5\\xf7\\xff\\xfd\\x04\\xff\\xfd\\xfc\\xe8\\x04\\x08\\xf3\\xf0\\x00\\xf8\\x04\\x06\\xfc\\xfa\\xff\\xfd\\x01\\t\\x05\\xff\\x00\\xf6\\xfd\\x00\\x03\\x01\\x03\\x03\\xfc\\xfc\\x01\\x01\\xfc\\xfb\\xfd\\xfd\\xfe\\xfe\\xff\\x01\\x04\\x02\\xff\\x02\\x07\\xfa\\xfc\\xff\\xfd\\x01\\x00\\x02\\xfc\\x08\\xfd\\xff\\xfd\\x04\\xfe\\x01\\x03\\x03\\x04\\x07\\x01\\xf9\\x05\\x01\\x08\\x00\\x04\\xf4\\x02\\xfc\\x02\\x01\\x06\\x00\\xfb\\xfc\\x03\\xe4\\x00\\x08\\xf5\\xfe\\x02\\x04\\x03\\x00\\xef\\xf9\\xfe\\x06\\xfd\\x02\\x00\\xfc\\x00\\x08\\xfc\\x01\\xff\\xfd\\xee\\xf8\\x00\\x03\\xf6\\xff\\x03\\xfe\\x02\\xf2\\xf8\\xfe\\xf6\\xfd\\x01\\x03\\x03\\xfb\\x00\\x06\\x03\\x02\\xf7\\xf7\\xfe\\xee\\xa6\\x04\\xfd\\xf9\\x02\\xfd\\xfb\\x06\\x06\\xff\\x00\\xf4\\x00\\x08\\x00\\xf6\\x05\\x00\\xf5\\xfc\\xfd\\xfe\\x00\\x00\\x0c\\x0c\\t\\xfe\\xfd\\xfc\\xfe\\x07\\x01\\xfe\\xf4\\x01\\x0b\\xfe\\x0c\\x02\\xf9\\xf8\\x08\\x12\\x05\\xff\\xfb\\x02\\t\\x00\\x02\\xfb\\xfa\\xfc\\x0b\\x10\\xf1\\xff\\xf7\\t\\x11\\x06\\x0b\\x02\\xf0\\x06\\xff\\x0b\\xfe\\xf9\\n\\t\\xfa\\xfd\\x04\\xfc\\x00\\xfa\\xfd\\xfd\\x05\\x02\\x05\\x06\\xfb\\xff\\x04\\x02\\x05\\x02\\x05\\x01\\x07\\xfe\\xfb\\xf7\\xfe\\xfd\\xf8\\xff\\x05\\x16\\x07\\xff\\xfd\\x00\\r\\xfd\\xf6\\x01\\x00\\xf5\\x00\\xfd\\x04\\x01\\xf7\\x02\\x08\\x04\\xfb\\x05\\x02\\xfa\\x01\\t\\xfb\\x01\\xf0\\xfb\\x03\\x02\\x02\\xf9\\x02\\xfb\\xfe\\xfb\\x01\\xfc\\x02\\xfd\\x05\\x06\\x08\\x01\\x00\\xff\\x03\\xff\\x03\\x03\\x00\\x06\\x01\\x00\\x06\\xfb\\t\\xf9\\xff\\x08\\xff\\x06\\x04\\xf2\\x04\\xfd\\x04\\x03\\xf7\\xfe\\x00\\x02\\xfd\\x05\\x01\\x01\\x02\\x02\\xfb\\xfe\\xfe\\x01\\xf6\\xfc\\x02\\xfc\\x02\\xf0\\xff\\x03\\xfb\\x02\\xff\\x02\\xfd\\xfb\\x00\\x04\\xfd\\x02\\x03\\xfc\\xfd\\xff\\xfe\\xff\\x06\\x02\\x01\\x01\\xff\\x01\\xfe\\x05\\x02\\x04\\x03\\x02\\x01\\xfc\\xfb\\xfa\\x04\\x00\\xfe\\xfc\\x06\\r\\x05\\xf4\\xfc\\x03\\x05\\xfc\\xfa\\x05\\xfe\\xfe\\xfe\\x01\\x04\\x03\\xfc\\x01\\x07\\x05\\xfd\\x03\\x03\\xff\\xfc\\x06\\xff\\xfb\\xed\\xfd\\x08\\x00\\x04\\x03\\x00\\xf5\\x07\\xfd\\x00\\xfd\\x03\\x04\\xff\\x05\\x08\\xfe\\x06\\x04\\x00\\x05\\xfc\\x01\\xfd\\x01\\xfe\\xfe\\xff\\x04\\xff\\xf7\\xff\\x02\\xfc\\x08\\x03\\xf8\\n\\xff\\x01\\x01\\x08\\xff\\xfb\\x00\\xfd\\xff\\xfd\\x04\\xf9\\x08\\x01\\x02\\x01\\xff\\x01\\xfc\\x01\\x00\\x00\\xfe\\x04\\xfd\\x02\\x02\\x02\\xfc\\x04\\x01\\x02\\x06\\xfd\\xff\\x01\\xf6\\x06\\xf5\\xff\\xf8\\xfe\\x04\\xf5\\x00\\x01\\xf7\\xff\\x06\\x03\\x02\\x05\\x01\\xf9\\x06\\x03\\xfb\\x03\\xfd\\xf0\\x06\\x02\\x12\\x03\\xfa\\xfd\\xfa\\x01\\x00\\xf9\\x02\\xfa\\xf9\\x01\\xfb\\x02\\x03\\x01\\xfa\\t\\x02\\xfa\\xfe\\xfa\\xfc\\x07\\t\\x06\\xfb\\xfb\\x00\\xfd\\x01\\xfb\\xf2\\t\\x02\\x00\\xff\\x01\\xfc\\x03\\xf8\\x02\\x04\\x02\\x01\\xff\\xfd\\x00\\x01\\x02\\x06\\x01\\x01\\x06\\x05\\xfe\\xfb\\xfb\\xfd\\x03\\x06\\xfb\\x01\\xfe\\t\\x00\\x06\\x03\\x03\\x12\\xfe\\x01\\x01\\x00\\x02\\xfe\\x01\\xf5\\xfc\\xfc\\x05\\t\\xfe\\xff\\x00\\x00\\x01\\x01\\x00\\xff\\xfd\\xfc\\xf7\\x00\\x00\\x02\\xfe\\x02\\x00\\x01\\x07\\xff\\x00\\x07\\xf6\\x01\\x04\\xdd\\x00\\xf8\\xfb\\xfd\\xfa\\xfc\\x01\\xfd\\xfe\\xfc\\x03\\x00\\xfe\\xfd\\xe3\\xff\\x01\\xf9\\x01\\xee\\xe4\\xeb\\xff\\xfe\\x05\\x01\\xe0\\x04\\x00\\xfb\\x02\\x01\\x00\\xf6\\xdf\\xff\\xff\\xf5\\xd7\\xfc\\xff\\x02\\x10\\x06\\xf2\\x04\\xfc\\xff\\xfb\\n\\xfd\\x03\\xe6\\x01\\xfc\\xfe\\x00\\x00\\x01\\x00\\x01\\x01\\x00\\xed\\x02\\xec\\xfc\\x07\\x04\\x02\\x01\\x00\\x00\\x02\\x04\\xfc\\x00\\xf9\\x02\\x03\\x02\\x00\\x01\\xee\\xe7\\xfd\\x04\\xfe\\xff\\xfc\\xfe\\xff\\x05\\x00\\x02\\x02\\x03\\xf4\\xcb\\x02\\xff\\x01\\xfc\\x01\\x00\\x02\\x03\\xfa\\xf7\\x00\\x03\\xfe\\x05\\x02\\x01\\x04\\xf2\\x01\\xfc\\xfc\\x04\\x05\\x02\\x04\\xfb\\xfe\\x05\\x03\\x03\\x07\\x05\\x00\\x00\\x00\\x02\\xfe\\xfd\\x04\\xfc\\x00\\x03\\x08\\xfb\\x02\\xf9\\xfd\\xff\\x03\\x05\\xfe\\xfe\\xfe\\x01\\x07\\x08\\x00\\x01\\xff\\x00\\xfb\\x08\\x07\\x02\\x01\\x03\\x03\\x00\\x02\\x00\\x01\\xfe\\xfc\\x01\\x00\\x00\\xff\\x03\\xfe\\xfb\\xfe\\xff\\x00\\xfa\\x04\\x00\\x05\\xfe\\x04\\x06\\x06\\x00\\xc8\\xff\\xf1\\xff\\xff\\x07\\x06\\x04\\xfc\\x01\\xfc\\x08\\xff\\xff\\x05\\x02\\x05\\x0b\\x00\\x04\\xd0\\xea\\xef\\xfd\\x04\\x02\\t\\x00\\x08\\xf9\\xfe\\x01\\x02\\x00\\xeb\\xbc\\xff\\x02\\xfa\\x04\\xff\\xf8\\x02\\xf7\\x02\\xfe\\xfe\\xff\\x05\\xff\\xf2\\xfc\\xfe\\xe4\\xff\\x00\\xfd\\x03\\x02\\xf8\\x00\\x01\\xfc\\x01\\t\\xfe\\xfd\\x06\\xff\\xfe\\x01\\xfe\\xfe\\x00\\x00\\x02\\xfd\\xfb\\x00\\x02\\xff\\x04\\x00\\x05\\xff\\x05\\x06\\xfd\\xfb\\xfc\\x05\\t\\x05\\xff\\xfe\\x00\\x01\\x04\\xfa\\x03\\xfe\\xfd\\x00\\xfc\\xfe\\x02\\x00\\xfc\\x06\\x07\\xfe\\xff\\x00\\x01\\x00\\x06\\xfd\\x01\\xfdL\\xee\\xff\\xff\\xf2\\xfe\\xff\\xff\\x04\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\xd6\\x1e\\x00\\x00\\xf2\\xea\\xff\\xffO\\xf7\\xff\\xff\\xd9 \\x00\\x00\\xf3\\x0e\\x00\\x00=\\xe3\\xff\\xff2\\xe1\\xff\\xff\\xff\\xea\\xff\\xff\\x17 \\x00\\x00}\\xd1\\xff\\xffI\\x03\\x00\\x002\\xbd\\xff\\xff\\xe7\\x0e\\x00\\x00\\x9e\\xf0\\xff\\xff\\xa7\\xf5\\xff\\xff\\xe0\\xe8\\xff\\xff\\xa6\\t\\x00\\x00=\\n\\x00\\x00\\x1e\\xe2\\xff\\xff\\x8f\\xf4\\xff\\xff\\xfe\\xc9\\xff\\xffe\\xe6\\xff\\xffs\\xf8\\xff\\xff\\t\\xf8\\xff\\xff\\xb0\\xfb\\xff\\xfft\\x05\\x00\\x00\\x93\\x01\\x00\\x00<\\xed\\xff\\xff\\xad\\x06\\x00\\x00\\xc5\\xda\\xff\\xff\\x9c\\xea\\xff\\xff\\x1a\\xdd\\xff\\xff\\n\\xed\\xff\\xffO\\xe8\\xff\\xff%\\xf6\\xff\\xff\\xa8\\xfc\\xff\\xff\\x80\\xf4\\xff\\xff\\x90\\xe5\\xff\\xff\\xbc\\xed\\xff\\xff\\xe8\\x0f\\x00\\x00\\x05\\xcd\\xff\\xff\\xe8\\xe7\\xff\\xff_\\x0c\\x00\\x00\\xaa\\xfb\\xff\\xff\\x86\\x02\\x00\\x00\\x86\\xe9\\xff\\xff>\\xfa\\xff\\xff\\xe3\\x01\\x00\\x00\\xde\\xe6\\xff\\xffe\\xfb\\xff\\xff\\xa5\\x11\\x00\\x00\\xcd\\xfa\\xff\\xffr\\n\\x00\\x00\\x95\\x15\\x00\\x00q\\xfb\\xff\\xff\\xfb\\x16\\x00\\x00\\x8f\\x0f\\x00\\x00\\xa7\\xfb\\xff\\xff\\x12\\xe6\\xff\\xff\\x8d-\\x00\\x00K\\x06\\x00\\x00\\x82\\x01\\x00\\x00#\\x17\\x00\\x00\\xe0\\x01\\x00\\x00\\x00\\x00\\x06\\x00\\x08\\x00\\x04\\x00\\x06\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x10\\x00\\x00\\xe4\\xea\\xfb\\xf1\\xfe\\x0b\\xf7\\xfa\\xf9\\xe0\\xf0\\xf7\\xed\\xfd\\x08\\x05\\xcc\\xe2\\xf1\\xf8\\xfc\\x05\\x13\\x10\\xf9\\xdb\\x08\\x01\\xe7\\xdf\\x16\\xf8\\xf9\\xf2\\t\\x01\\xf4\\xdb\\x1f\\xb8\\x11\\x11\\xfd\\xed\\x04\\xf7\\x11\\xde\\x16\\x1d\\x1e\\xe3\\xf3\\xf9\\xeb\\xe5\\t\\xf2\\xf3\\xe5\\x07\\xfc\\xe8\\xda\\x15\\xff\\xec\\xf7\\xff\\x0c\\x0f\\x01\\xfa\\xd6\\xf7\\xe1\\x11\\x16\\xfd\\x00\\xf9\\xef\\xe8\\x12\\x02\\xfe\\xf4\\xf0\\xe8\\x00\\t\\x08\\xf6\\xf7\\x00\\r\\x00\\xed\\x0c\\xe6\\xd3\\xf5\\x1d\\xfa\\r\\x1a\\t\\xfb\\n\\x1c\\xf3\\xff\\x13\\x1a\\'\\xee\\xfd\\x04\\xf2\\xee\\x19\\x00\\x01\\xe7\\xef\\xe1\\xe3\\xe8\\x18\\x04\\x06\\x0c\\x13\\x06\\xf5\\x13\\x0f\\x07\\x0c\\t\\x02\\xf1\\x06\\x07\\xf5\\x08\\x06\\xff\\xec\\xe8\\x06\\x02\\xfb\\xfe\\x14\\xfd\\xe9\\xf2\\x04\\x14\\x15\\x0c\\x02\\xe7\\xe2\\xf5\\x0c\\n\\xef\\xff\\x0c\\x08\\xf9\\x0e\\x0e\\x14\\xec\\xf9\\xfa\\x0b\\x16\\x10\\xfc\\x0b\\xfc\\xe5\\xda\\x04\\xf6\\xf9\\xf7\\xfb\\x0b\\xf5\\xf7\\xfd\\x01\\x05\\xf9\\xf7\\xe4\\xf7\\x01\\xfa\\xfc\\xf2\\x00\\x08\\xfb\\x07\\xfe\\xf7\\xf7\\xf6\\xff\\x02\\xfb\\xfa\\x01\\xeb\\xf2\\x04\\x0f\\xf4\\xf3\\xfa\\x0c\\xfc\\xff\\r\\xf9\\xe8\\x08\\xfe\\xfe\\x06\\x0e\\x05\\xea\\xf2\\x12\\x11\\x08\\x02\\xfd\\x03\\xff\\xf9\\x00\\xfd\\xf9\\n\\xf5\\x02\\n\\xfe\\x00\\x17\\x0c\\x05\\x04\\x02\\x05\\x05\\xf5\\xff\\xe2\\xf7\\n\\xff\\x13\\x0f\\xfe\\xff\\xe7\\xf5\\x18\\xe8\\x18\\xf8\\xed\\xe4\\xcb\\xea\\x1d\\xf0\\xfa\\xea\\xe9\\xea\\xe1\\x05\\t\\xbf\\xc8\\xe8\\x02\\x0b\\x1a\\x1e\\xff\\xce\\xe6\\xff\\xe7\\r\\x11\\x0b\\xfd\\xe3\\xdd\\x16\\xf4*\\x15\\xb6\\xf0\\xc5\\xdd\\xf8\\xfa\\x00\\x08\\x01\\x04\\xf7\\x02\\xfd\\x04\\t\\x0c\\x15\\x03\\x01\\xf8\\xfb\\x05\\x00\\x10\\xf6\\xf7\\r\\x0e\\xfa\\x15\\x0b\\xfc\\xf3\\x03\\x06\\r\\xff\\x01\\x13\\x04\\x05\\x00\\xf3\\x08\\x0e\\xea\\xff\\x07\\xf9\\x01\\x00\\x0b\\x03\\xf8\\xf8\\x00\\x01\\x0c\\x0f\\x06\\x04\\x03\\xf8\\xfd\\r\\x08\\xf9\\x08\\xfe\\xe4\\xf9\\xf7\\x10\\xfd\\x00\\x00\\xf2\\xee\\xe0\\xf4\\x04\\xf3\\x08\\x17\\x02\\xe5\\xf9\\xfd\\x03\\xfb\\x13\\x16\\x07\\xd7\\xfa\\r\\xff\\xe0\\xf9\\r\\x0b\\xf2\\x18\\x02\\xe9\\xbf\\xd6\\x14\\xf1\\x1c\\x1f\\xf1\\xef\\xf3\\x12\\x05\\xf7\\x1a\\x1a\\x00\\n\\x04\\xfd\\xea\\x08\\xf5\\x11\\x19\\x03\\x0b\\xe8 \\x0c\\x15\\x07\\xfd\\xfe\\xfd\\n\\x0c\\n\\x17\\xf7\\xf4\\xf4\\xfc\\xf8\\x07\\x13\\xec\\xee\\xf5\\xff\\xff\\xe9\\x01\\xf9\\x0b\\xfd\\xfd\\x11\\r\\x05\\x13\\xff\\xf9\\x0e\\x16\\x0e\\x0e\\x0f\\n\\xff\\xfb\\xff\\t\\x0f\\x15\\t\\xf4\\xf2\\x17\\xef\\t\\xf7\\r\\xe1\\xe7\\x0b\\x06\\x00\\xfc\\xf4\\x06\\xed\\xfd\\xff\\x07\\x1a\\xe9\\xf9\\xea\\xd9\\xe8\\x1a\\xed\\xef\\xe2\\xe7\\xe1\\xea\\xe0\\xee\\x0c\\x06-\\x1b\\xff\\x17\\xf2\\xd9\\r(\\x0c\\xce\\xe3\\x1c\\x1a\\x03  +\\xfb\\xe8\\x1e\\x1b\\x1e\\xe7\\x07\\t\\xe7\\r#\\xf8\\xdb\\xe1\\xc7\\xc1\\xac+\\xff\\xdd\\xf9\\x0c\\xbc\\xa9\\xa4\\xda\\x00\\'\\x1d\\x08\\x0f\\x10 \\x1a\\xe0\\xe3\\xb4\\xf2\\xf9\\x08\\x00\\x0e\\xf9\\xda\\x83\\x1a\\xfe\\xde\\xe88\\x00\\xba\\xaa\\xda\\xe3\\xb4\\xda\"\\xe2\\xdb\\x0e\\xf9\\xeb\\xf4\\x15\\x07\\xb2\\xc7\\xe4;\\x0c\\x03*\\x04\\xd4\\r>\\xf5\\x10\\x17\\x1a\\xf3\\x04\\x18\\x17\\xeb\\xe6\\xf0\\xe7\\xe3\\xd0\\xf6\\x13\\n\\xfe\\xfb\\x16\\x04\\xfc\\x03\\n\\x07\\x08\\x03\\x04\\xed\\x0c\\x00\\n\\xfe\\xff\\x04\\xf9\\xff\\xf8\\x13\\x01\\xfe\\x14\\xf8\\xf4\\xf8\\xff\\x10\\x14\\x11\\x0c\\r\\xff\\t\\x01\\x02\\x0f\\xf9\\t\\x01\\t\\x06\\xfd\\xf6\\xfa\\x07\\x03\\xf8\\x13\\x02\\x03\\xfb\\n\\x06\\xf8\\x02\\xfb\\xf5\\xf5\\x00\\x03\\xee\\xba\\xe6\\xea\\xfe\\x08\\x0b\\xf0\\x08\\x0e\\x06\\xfb\\xf6\\x02\\xff\\xf9\\x0e%\"\\x14\\x1f\\x14\\x18\\x08\\x13\\n\\x00\\xe2\\xf9\\x08\\x16\\xfb\\x03\\xf2\\xd5\\x9f\\xf0\\x16\\x04\\xed\\x02\\xec\\xc6\\xfb\\x12\\t\\x04\\xdb\\xdb\\xb2\\xe0\\x12\\x06\\xf4\\xda\\xf6\\xeb\\xe9\\xf9\\x10\\x00\\xf7\\x01\\xfe\\x1a\\x07\\xfa\\x04\\xfb\\xfc\\xff\\xf4\\x14\\x0e\\t\\x02\\x06\\xfb\\x05\\xf8\\xf5\\x03\\xe3\\xef\\r\\xe8\\xf8\\x17\\xf8\\x17\\x02\\x10\\x18\\xff\\xf5\\xf8\\xf3\\x05\\xfc\\xf4\\xfc\\xf8\\x19\\x08\\xfb\\xf6\\xec\\xf8\\xe9\\xe3\\x1b\\x19\\xf9\\xf6\\xff\\xff\\xea\\xf8\\t\\x07\\xfc\\xf0\\x07\\x04\\x0e\\x15\\x0b\\xf6\\t\\x02\\xfc\\xef\\xf5\\xfd\\xfc\\t\\x10\\xfc\\x06\\x03\\xf1\\x08\\xff\\xf9\\x06\\x05\\x0f\\xf7\\xfe\\x00\\x07\\xfd\\x06\\x00\\xf9\\x01\\x0e\\xfe\\x02\\xf5\\xfd\\x06\\x02\\x17\\x12\\x0e\\xed\\x00\\xf4\\xf2\\xf3\\xff\\xff\\x04\\xf6\\xf7\\x04\\xf9\\x0c\\xf0\\xf6\\n\\xf6\\x04\\xf8\\xff\\x0e\\x11\\x15\\x19\\xf7\\xe4\\r\\xef\\xf8\\x08\\x04\\x08\\xfb\\x01\\r\\x00\\x00\\xff\\x19\\x0c\\t\\x05\\xff\\xf1\\xfb\\x06\\xf8\\xfd\\xf8\\x06\\xf2\\xf8\\x05\\n\\x0c\\x14\\xf2\\xf4\\x0e\\xff\\xff\\x01\\r\\xf4\\xfc\\x03\\xf2\\xfa\\x15\\x0f\\xfb\\x06\\xf6\\xf9\\xf0\\xfa\\xfc\\x04\\x02\\x05\\x00\\xf7\\xf3\\x0e\\x15\\x08\\x00\\xf1\\xf6\\x02\\x07\\xf1\\xfe\\xef\\xfd\\x04\\xfa\\t\\xf9\\xfb\\xf2\\x03\\x06\\x06\\n\\xfe\\n\\x08\\x08\\t\\x07\\x00\\x0c\\xfc\\x0e\\x01\\xfa\\xfe\\xfa\\xff\\x00\\x1c\\n\\xfc\\xf9\\x16\\x06\\x01\\xf8\\x08\\x0b\\x02\\xfd\\xfe\\x03\\n\\x00\\x01\\xfb\\xff\\x04\\x0f\\x11\\x05\\xfb\\x16\\x01\\xfc\\x05\\x0c\\x04\\xfd\\xf6\\x04\\x04\\x02\\r\\t\\n\\x05\\xfe\\x06\\x13\\x0f\\x05\\x08\\xfa\\xfb\\x03\\x00\\xf9\\x03\\xfd\\x00\\x06\\xff\\xf9\\x01\\xfa\\xf8\\x05\\x00\\x04\\x01\\xfe\\x11\\xfa\\xff\\xfc\\x01\\xf8\\x01\\xff\\x18\\x06\\n\\x01\\xfb\\xff\\xf1\\x0e\\x1a\\xf4\\xfd\\x10\\x03\\x00\\xf8\\x11\\x11\\x02\\x06\\x02\\xf8\\x00\\x07\\x08\\xee\\xfa\\x0c\\xe1\\xf8\\x01\\x16\\x10\\xf8\\xfb\\xf6\\xe3\\xf3\\xfe\\x02\\x05\\t\\xff\\xf9\\xe9\\x00\\x1b\\xf7\\x0e\\x0e\\xd5\\xf6\\x04\\x0b\\xd5\\xe2\\x08\\x07\\xf6\\xf4 \\xe7\\xed\\xf9\"\\t\\x1d!\\x0f\\xf6\\x06 \\x0b\\x15\\x1d%*\\x18\\xfe\\x01\\x01\\x12\\x16\\xf6\\xde\\xc6\\xe8\\xf1\\xfe\\r\\x0c\\x1a\\x08\\x11\\n\\xed\\xf1\\xe4\\xea\\x10\\x0b\\x1c\\x1e\\r\\xed\\x00\\xfb\\n\\x11\\xf4\\n\\x0b\\xf0\\x00\\xe8\\xd9\\xd6\\xb5\\xe9\\xf8\\x03\\xf2\\xf8\\xcf\\x00\\xeb\\xee\\xdb\\xf0\\xe5%\\x00\\x1e\\x15\\x01\\xfa\\xfc\\x04\\xfe\\x0c$\\r\\x04\\n\\x18\"\\x0f\\x0f\\x04\\x02\\xfd\\xff\\xf7\\xff\\n\\x03\\xfd\\xe3\\xe8\\xf6\\xf0\\x17\\xeb\\xf8\\xeb\\xf7\\x0b\\x0e\\x08\\x06\\xe2\\r\\x08\\x14\\xfa\\x00\\xfb\\x04\\xf5\\x17\\xfd\\xfb\\xe7\\xfc\\xfc\\x1b\\xfc\\x06\\x01\\xc5\\xcb\\xe0\\x01\\r\\x17\\x1f\\xfb\\xc0\\xe5\\xda\\x0f\\xfb\\x1a\\x0c\"\\xff\\xf9\\r\\xf8\\xfd\\xec\\x03\\xfb\\x1c\\r\\xf3\\x07\\x00\\xe9\\x05\\x05\\xfc\\xfd\\x03\\xfd\\xfe\\xfe\\x01\\n\\xfa\\x03\\x05\\x04\\x00\\xfd\\x05\\x01\\xf2\\x01\\xfd\\x0b\\x02\\xff\\x0f\\xfb\\x0c\\x07\\n\\x05\\xfa\\x02\\x16\\t\\x14\\x0c\\x05\\x04\\x04\\n\\x04\\n\\x15\\x03\\xfd\\x05\\x08\\x03\\n\\x11\\t\\x00\\x07\\x0f\\x05\\x05\\xff\\xfe\\x05\\x0b\\x07\\x07\\xfc\\x0e\\xfb\\r\\xfe\\x10\\xfd\\x03\\x06\\x14\\r\\x08\\x03\\x11\\xff\\n\\x00\\xfe\\x03\\xfa\\xfb\\x06\\xf5\\xf9\\x03\\xeb\\x08\\xfc\\x0c\\x11\\x07\\xf2\\xfd\\x04\\x06\\xf4\\xff\\x13\\xfe\\xf2 \\xc9\\xee\\xef\\x07\\n\\xea\\xf4\\x16\\xf1\\xfb\\x01\\t\\xf6\\xec\\xf2\\xee\\x02\\n\\x19\\x1a\\x10\\xfe\\xe2\\xf7\\xeb\\t\\xe3\\xf7\\xfb\\xe0\\xf8\\x0c\\xd4\\xf0\\x01\\x05\\x01\\x00\\x10\\n\\xeb\\xff\\x04\\x14\\xe5\\xfc\\x1c\\x15\\x01\\xeb\\xf2\\r\\xea\\x14\\x07\\xff\\xee\\xe3\\xf4\\x02\\x00\\xe2\\xf2\\x08\\xf7\\x01\\x1f\\x01\\x18\\xd4\\xe5\\xf0\\xfa\\x00\\x15\\xf9\\t\\xdf\\xe6\\xfe\\xf6\\x11\\x11\\x13\\xfb\\xef\\xe3\\xf1\\x19\\xf5\\x05\\x02\\xfe\\xf9\\xfb\\x03\\x06\\x0c\\r\\x07\\xfe\\x01\\x01\\x00\\x03\\x02\\x03\\x00\\xf5\\x03\\xf8\\xf5\\x06\\x04\\x04\\x08\\x07\\xfe\\xf9\\xf6\\xf9\\x05\\x04\\x03\\xf1\\xff\\x11\\n\\xf8\\xfa\\x01\\xed\\xe3\\x06\\r\\x10\\x05\\x01\\x06\\xfc\\xfa\\x10\\x11\\x07\\xfe\\x03\\x08\\x10\\x18\\x10\\xf0\\xf0\\xfb\\x01\\x0f\\xf1\\t\\xf3\\n\\x13\\xde\\xfd\\xf1\\x0f\\xf4\\x1d\\x05\\n\\xd4\\xef\\x0b\\xfa\\xff\\xff\\x0c\\x1a\\xe8\\xf8\\x05\\xed\\xee\\xe3%\\x01\\xf2\\xfa\\xfe\\xc9\\xc5\\xdf\\x0b\\xfb\\r\\x11\\xfc\\xd1\\xdd\\x02\\x19\\xf0\\x0b\\x10\\x1b\\xf8\\xf2\\xfe\\xdd\\x03\\xf1\\x01\\x0e\\x10\\xf2\\xf5\\xfb\\xf5\\x01\\x07\\xf8\\x03\\xfb\\xf8\\x0b\\xf4\\xeb\\t\\x03\\x03\\x03\\r\\xfd\\xfa\\xfa\\t\\x04\\x01\\r\\n\\x03\\xf7\\xfb\\x04\\x01\\xfb\\xff\\x00\\x03\\x05\\x00\\xf3\\xfa\\x01\\x04\\xf9\\x01\\xff\\x03\\r\\x05\\x06\\t\\x00\\xff\\xfc\\x06\\x10\\x03\\x0b\\x11\\x0f\\xfd\\x00\\x12\\x02\\xf8\\xf7\\xf6\\xfa\\x02\\t\\x14\\x04\\x0b\\x05\\t\\x08\\xfd\\xeb\\xed\\x07\\xfe\\xfd\\x1d\\xfa\\xf1\\xd0\\xfb\\x04\\xf8\\xfd\\x19\\xec\\xd3\\xe4\\xf0\\xfb\\x04\\x02\\x05\\xe6\\xf9\\xe8\\xfe\\xf4\\xfc\\r\\x06\\xee\\xfe\\xfd\\x03\\t\\xfc\\x11\\x04\\xfb\\xfa\\xfd\\xee\\xf5\\xfe\\x14\\xfa\\r\\xfd\\x11\\n\\xf4\\xf3\\x02\\xf9\\xfc\\x03\\t\\x08\\xf3\\x03\\xed\\x07\\xfe\\xfb\\x0e\\x1b\\x16\\x0b\\x13\\r\\xf7\\x04\\xfb\\x12\\x06\\xf1\\xeb\\xff\\xef\\x04\\n\\xe7\\xf5\\x08)1\\x13\\xd5\\xda\\xda\\xde\\xee\\x0b\\r\\x00\\x01\\xfc\\xd9\\xe0\\xfe\\xf2\\xd1\\xf0\\xfe\\xf8\\xe8\\xf3\\xf4\\xe5\\xf6\\xf1\\xfa\\xfa\\x0b\\x16\\x155\\x19\\x0b\\x04\\xe6\\xff\\x04\\xfd\\x04\\t\\x00\\x0c\\xf6\\xf7\\xfb\\xfb\\x04\\xf9\\t\\xfc\\x08\\x03\\xfb\\xfd\\xfb\\x04\\x08\\x00\\xfe\\xe7\\xec\\x01\\xfe\\xfa\\xfa\\xfc\\x05\\xfb\\xf7\\x0c\\xfa\\x05\\x03\\n\\x02\\x16\\t\\x07\\x05\\xfc\\x14\\xfa\\n\\t\\t\\x07\\r\\x0e\\r\\x04\\x08\\x06\\xff\\xf4\\xed\\xf8\\xfb\\xf9\\x13\\n\\x12\\x16 \\x1b\\x0f\\xec\\xdc\\xdc\\x08\\x0c\\x0e\\xff\\r\\xf4\\xf0\\x14\\xf6\\xf9\\xe3\\xe3\\x14\\xf6\\xd9\\xd3\\xa7\\xa7\\xba\\xe7\\x04\\xfd\\x11\\xe0\\xec\\xf8\\t\\x01\\xf7\\xdd\\xf60\\x1c\\x17\\x17\\x13\\x03\\x00\\x19\\x07\\x16)\\xfe\\x05\\x01\\x1d.\\t\\x05\\xee\\xc5\\xc7\\xf8\\xff\\x06\\x1a\\xd1\\xff\\x11\\x00\\x11\\xe6@\\'\\xf5\\x14\\xe0\\xec\\xe4\\xfe\\r\\xf1\\x13\\x11\\x02\\xef\\x05\\x1d\\x1c\\xce)\\xc8\\xdc\\x08\\xf0\\xefI\\x16\\x17\\xc6\\xfb\\xc8\\xca\\xbc\\'\\xfd\\x11\\xde\\xc7\\xe3\\xde\\xeb\\x1f\\xe0\\xf1\\x0f\\x12\\x13\\x00\\x17-\\x10\\xf5\\xe7\\xb9\\xc7\\xea\\x18\\xe8\\xd4\\x1c\\x17\\x18\\x1c\\xfc\\xdc\\xef\\xe8\\x05\\x12\\x0c\\x02\\x10\\xfa\\xf9\\x11\\xfa\\xf0\\xde\\xf2\\x13\\t\\xfa\\xa7\\xba\\xa2\\xcb\\xea\\x01\\xff\\x13\\xe8\\x01\\x08\\x15\\xff\\xef\\xce\\xe33%\\x08\\t\\x1a\\xf3\\xf4)\\x0f (\\x05\\x0f\\x01\\x14@\\x02\\xec\\xeb\\xd2\\xce\\xec\\x03\\xfe\\xff\\x01\\xf9\\x0b\\xfe\\xeb\\xf7\\xfa\\xf1\\xf3\\xfa\\xf9\\xec\\xe5\\x03\\xf1\\xf0\\xf0\\x0f\\x13\\xeb\\xf4\\x00\\xed\\xdc\\x14\\x1a\\xe7\\x03\\x03\\x19\\xf9\\x06 \\x16\\x01\\x0f\\x0c\\x15\\n\\xda\\x01\\xfe\\xfb \\xf9\\x0b\\x0e\\xd3\\xc0\\xdb\\xf4&\\xf7\\xf6\\xf2\\xec\\xd3\\xe2\\xd8\\x04\\xe6\\xce\\x12\\x0f\\x02\\x01\\n\\x0b\\xfa\\xee\\xe6\\x16\\x06\\x0b\\x11\\x10\\xfb\\xfa\\xf8\\xfd\\x05\\xf3\\xfa\\x03\\xe7\\xeb\\x02\\xea\\xfa\\xf4\\x16\\x12\\xea\\xd5\\xdd\\xed\\xee\\xf6\\xff\\xf4\\xfe\\xfe\\x05\\x04\\xee\\xea\\xeb\\xf8\\x15\\x0e#\\r\\xfd\\x0c\\xf8\\x0e\\x0b\\x1c\\x15\\x17\\xf5\\t \\x1d\\x0f\\x0c\\xe5\\x02\\xef\\x02\\xe2\\xf8\\xfd\\xf3\\x10)\\xf8\\xfd\\xe8\\x02\\r\\x1a\\x0b\\x0f\\x0c\\x0e\\x13\\x06\\x02\\t\\x00\\x1a\\x00\\x02\\xfb\\x07\\x11\\xed\\xf5\\x01\\x10\\x01\\x11\\t\\xf8\\xf0\\xf5\\xee\\xe2\\x04\\x02\\xfd\\xeb\\xe1\\xf1\\xe4\\xdc\\xd5\\x07\\xe4\\xce\\xf2\\xe8\\xfc\\x03\\x1c\\x1f\\x0b\\t\\n\\xe8\\xf1 \\xfa\\xf6\\x01\\xf5\\xec\\x14\\xfe\\xe5\\x05\\xe3\\xdb\\x00\\xf5\\xf6\\xe6\\x1c\\x0f\\xf6\\x11\\xf6\\xe5\\xf2\\x10\\xee\\x07\\x19\\xf9\\t\\xe7\\xcc\\xf8\\xce\\xe3\\x0c\\xe2\\xdd\\xf2\\x1e\\x0f\\xff\\xed\\xdb\\xe6\\xf7\\xea\\x1b\\x17\\x0f\\n\\xf5\\x01\\xea\\x14\\x01\\x02\\xf0\\xf6\\xf0\\xf0\\xfe\\x156$\\x07\\xf8\\x04\\x0b\\xfd\\x03!\\xd7\\xf9\\xf1\\x14\\x0f\\x15\\xff\\xef\\xd1\\xf8\\x11\\x05\\xfd\\x0f\\x0e\\xfb\\xe7\\x04\\x0b\\xdc\\xd0\\xe6&\\x01\\xf1\\xfc\\xf8\\xce\\x9c\\xb2\\x17\\r\\x1d\\x1d\\r\\xc0\\xc4\\xf3\\x13\\x05\\x0c\\n\\x1d\\xe5\\xec\\t\\x04\\r\\x13\\r-()\\xf2\\xe5\\x02\\xfb\\xfe\\xea\\xf9\\n\\x1b\\x0c\\x08\\xf9\\x03\\xe7\\x01\\x08\\x02\\x03\\xf7\\x02\\xf1\\xfe\\x1a\\x02\\xfe\\xee\\xf1\\xe5\\x07\\x0f\\x0c\\xf0\\xee\\x11\\x04\\x00\\x06\\x12\\xe4\\xef\\x08\\x1f\\xf4\\xff\\x08\\x15\\xfa\\xe9\\x11\\r\\x03\\x0f\\x06\\x17\\x15\\r\\xff\\xe6\\xee\\x13\\x02\\xe8\\xe6\\xf4\\xe9\\xde\\xd7\\xfa\\xfd\\x04\\x0b\\x08\\x03\\x05\\xfc\\xf9\\xff\\x01\\x02\\n\\xf9\\n\\xff\\xfe\\x07\\x05\\x04\\xfb\\x08\\x06\\x13\\xfe\\x0b\\xfa\\x0b\\xf8\\x02\\x07\\xf5\\x03\\x05\\xf9\\xff\\t\\x0f\\x02\\x10\\xff\\x00\\x07\\t\\n\\xff\\x05\\x17\\xff\\xfa\\x05\\x06\\xfe\\x07\\x04\\t\\t\\xff\\xfe\\xf5\\xfb\\xfe\\xf9\\x13\\r\\xe5\\xd4\\xe9\\r\\x01\\n\\t\\xf6\\xe2\\xf0\\xff\\x0e\\x08\\x1a\\xfc\\xf8\\xff\\xee\\xef\\xf9\\xef\\xf8\\x07\\xf8\\x00\\xeb\\t\\x04\\xfa\\xf1\\x08\\xda\\xf5\\xf0\\x02\\xfa\\xe2\\xf1 \\xec\\xe8\\x06\\x0f\\xee\\xdd\\xe5\\x1a\\xe0\\xf1\\x0f\\x06\\xf8\\xe3\\xe8\\xe9\\xfa\\x1d\\x19\\x0e\\x06\\xef\\r\\xfc\\x12\\xf0\\xfe\\xe5\\xf7\\x15\\x02\\xfe\\x0c\\xf6\\xf7\\xe8\\x0b\\xfd\\xfa\\xea\\xf5\\r\\xff\\x10\\x0b\\x04\\x02\\xfc\\xf0\\x1c\\x15\\xf8\\xe6\\x03\\x14\\x0e\\x16&\\x11\\xf3\\xf9\\x11\\x1c\\x07\\xe2!\\x02\\xfc\\x08\\x11 \\xfe\\xd3\\xfa\\xe5\\x01\\x0f\\xf8\\x01\\xd4\\x00\\xec\\xef\\xe2\\xdd\\xf7\\xfe\\x13\\xeb\\x04\\xe5\\xef\\xff\\x07\\t\\x01\\xf2\\xde\\xef\\xf7\\x11\\x07\\xf4\\xe8\\xdc\\xd9\\xec\\x0f\\x06\\x0e\\xef\\xdb\\xb1\\xdb\\x08\\xf8\\xd0\\xe8\\x0f\\x01\\xf2\\xf7)\\xec\\xe2\\xe9\\x10\\x0e\\x08!\\x1e\\xe5\\xee\\x00\\x07\\xf1\\x17\\x054\\x07\\xfa\\xf8\\xeb\\xe2\\x0c\\xeb\\xef\\xfa\\xeb\\xf2\\xff\\xfe\\xff\\xf9\\x08\\xf8\\x00\\xfe\\x00\\x04\\x03\\xfe\\x0b\\x11\\x0e\\t\\x0c\\x05\\x02\\n\\r\\r\\x00\\xfb\\x06\\x07\\x01\\x03\\x0b\\xff\\xf8\\xfc\\x02\\x04\\xf6\\xfe\\x01\\xfa\\xfc\\x02\\t\\xff\\xfa\\x03\\xfb\\xf6\\xf9\\x0c\\x12\\xef\\x08\\x05\\x01\\xf3\\xf7\\x07\\x07\\xee\\xf4\\xfc\\x00\\xfd\\x14\\r\\x05\\xf9\\xff\\x03\\x0e\\x10\\x12\\xfa\\x02\\xf0\\xe9\\t\\xfe\\x0f\\x07\\xf8\\x12\\xef\\x0b\\xf1\\xf2\\xde\\xe8\\xf9\\x12\\xf4\\x8d\\x81\\xb8\\xd9\\xf5\\xf1\\x14\\xd9\\xfe\\t\\xff\\x06\\x0e\\xf6\\xf1\\xfc,#\\t\\x1b\\x15\\xfe\\xfe\\x18\\t\\x10\\x0f\\xf9\\xed\\x00\\n\\x17\\x0e\\x05\\xed\\xd0\\xe4\\xf6\\x0b\\xf8\\x1c\\xf2\\xfc\\xf8\\x0e\\xd5\\xfd\\xd7\\x16\\xf4\\xfc\\n\\xfb\\xc9\\xcd\\xc9\\xfd\\x01\\xdd\\xf95\\xe2\\xa8\\x1b\\xe6\\xed\\xdb\\xf74\\xe5\\xda\\t\\xf5\\xd0\\xb4\\xe5\\x1d\\xcc\\xe1\\n;\\xd7\\xf3\\xe9:\\xc4\\xe3\\xd8\\xfa\\xc7\\xe0\\xde9\\xec\\xcf\\xda\\xe6\\x0e\\xb8\\xc55\\xe3\\xfd\\x10\\xfc\\x05\\x06\\r\\x00\\xfa\\x00\\x07\\x0b\\x0e\\xfe\\n\\x02\\xf7\\x19\\t\\x10\\x01\\x07\\x02\\xee\\x03\\n\\n\\xf6\\x05\\x16\\x12\\x02\\x15\\xfb\\x04\\xff\\x06\\x06\\x0e\\xfc\\x07\\n\\xf5\\xe4\\xed\\x01\\xea\\x01\\xfb\\xf0\\x0e\\xf4\\x01\\xea\\xd8\\x06\\xf4\\x02\\xff\\x01\\t\\x1a% \\n\\xf8\\xe4\\n\\x0e\\x11\\n\\x13\\t\\x0b\\xed\\x18\\x0b\\r\\x10\\x12\\x03\\x00\\xfd\\xfa\\x02\\x00\\xf7\\x0b\\x01\\x03\\x08\\x03\\x07\\xf6\\x14\\x1e\\xff\\xfe\\xec\\x00\\xf6\\xef\\xfe\\n\\x0f\\xf5\\t\\xe2\\xea\\xee\\x03\\xf9\\xe8\\x11\\r\\xe8\\xea\\x05\\xfb\\xf2\\xf6\\xfb\\xfd\\xfa\\xfd\\x12\\x1c\\x02\\xfb\\xfc\\xf5\\x19\\xf9\\x04\\x06\\xf7\\xf5\\x01\\x04\\xdf\\xf9\\x11\\x06\\xf3\\x00\\x0b\\x02\\xec \\x0b\\xfb\\xf9\\x06\\x14\\x01\\xf4\\x17\\xf0\\xe6\\xfc\\xef\\x08>\\x00\\x18\\xcd\\xf5\\xe1\\xb7\\xf6&\\x12\\x1d\\xf2\\xde\\xd5\\xd3\\xfb\\x0f\\x1f\\x19\\xeb\\xf8\\xfe\\x07\\x08\\x10\\xfd\\x08&\\x16\\x1a\\x04\\xf5\\xe9\\x01\\x01\\r)\\x17\\xfd\\xee\\x15\\xe5\\x08\\x17\\x00\\xfd\\n\\xe9\\x0b\\x00\\t\\xf7\\xd3\\xeb+\\xf0\\x03\\xd0\\t\\xba\\xd3\\xfb\\x00\\xe7?\\xc5\\x05\\xd2\\x04\\xe4\\xc4\\xad\\x02\\xca\\n\\xfd\\xfb\\xf5\\xcd\\xd6\\x1d\\xd7\\x07\\x1f\\x0e\\x0b\\x0e!1\\x08\\xfc\\xfa\\x06\\xf6\\x06\\xe9\\xf3\\x01\\xf9\\xf0\\xfb\\xed\\xee\\xfa\\x05#\\x1a\\x17\\x02\\x17\\x0e\\x06\\xf5\\x10\\t\\x0f\\xed\\xff\\xfc\\xf6\\x03\\n\\x0c\\x01\\x1b:\\x1b\\xe6\\xc7\\x06\\xe8\\xec\\x1a\\x19\\x04\\xe5\\xd4\\xc4\\xcc\\xe2\\x02\\xeb\\xf5\\xef\\xef\\xe6\\xec\\xe4\\xdd\\xe6\\x06\\x0f\\xee\\r\\x103C*\\x13\\x04\\xd2\\x03\\x02\\xfd\\x07\\xf9\\x04\\n\\x04\\x03\\xf5\\xf7\\xf6\\xf3\\xff\\x0b\\x0c\\xda\\x08\\xf8\\xf2\\x05\\t\\x0e\\x08*\\xfb\\xf5\\xf7\\x07\\xfc\\x00\\x04\\xff\\xfe\\x00\\x07\\x02\\xfa\\xf8\\xfa\\xf1\\x0b\\t\\x15\\xf6\\xd1\\xe9\\x00\\xfb\\x14\\x1e\\x01\\xe5\\xed\\x04\\x03\\x08\\r\\x00\\xef\\xe3\\xef\\xf4\\x16\\x01\\x00\\xff\\xf6\\xec\\xf7\\x04\\x0c\\t\\x02\\xf6\\x00\\x04\\x0b\\x19\\n\\xfb\\xfb\\x0f\\x08\\xfb\\x0b\\x0c\\x05\\x0f\\x00\\x0c\\r\\x12\\x13\\xf8\\xe6\\xf0\\xe1\\xef\\x17\\xff\\xf5\\xeb\\xdb\\xdf\\xd3\\xf0\\x0c\\xee\\xe1\\xe1\\xe5\\xe9\\xee\\xee\\xf6\\xe7\\xed\\xdb\\xd8\\x10\\x10*&\\x07\\xf4\\xf9\\xe0\\x1c\\n\\xf3\\x1e\\x10\\xf4\\xfd\\xea\\xef\\x01\\t\\x10\\xda\\xdf\\xe9\\xdd\\x07\\x0e\\x14\\x03\\xd3\\xec\\xf7\\xfc*\\x1a\\x06\\xe9\\x07\\x05\\x0e\\x00\\x08\\n\\xfb\\xe8\\x01\\xed%\\xff\\xee\\n\\x1e\\x06\\x01\\xf5$\\n\\xee\\xda\\xfd\\x02\\x06\\n\\t\\xf0\\x08\\xf2\\xdb\\xb5\\xac\\xea\\x17\\x1d\\x05\\t\\xfe\\t\\x1c\\r\\x13\\xfa\\xea\\xe6\\xf2\\x05\\xf9\\xc7\\xde\\xe5\\xfd\\xdc\\x14\\x17\\xde\\xd3\\xde\\xe7\\x11\\xf1\\t\\xff\\xf2\\xf8\\x0c\\x05\\xf8\\xf7\\x15\\x00\\xfe\\x02 \\x0f\\xe9\\x01\\x17\\x12\\xfc\\xf9 \\x06\\xe4\\xdd\\xff\\x0c\\t\\x0c\\x04\\xfa\\xf7\\xf0\\xc9\\xa6\\xcd\\xf1\\x13!\\x1e\\xed\\x03\\x03\\x0f\\xee\\xec\\x10\\xe2\\xed\\x06\\x06\\xf2\\x07\\xf8\\xfd\\xf9\\x13%\\xf5\\xf3\\x15\\x1d\\x03\\xf7\\x13\\xef\\xd8\\xd2\\xfb\\x0e\\x1a\\x08\\x00\\xfa\\xe2\\xb7\\xcd\\xf0\\n\\n\\x1e\\x0b\\xc3\\xdc\\xdb\\x11\\x14\\x08\\xf7\\x04\\x05\\xde\\x15\\x12\\x1a\\xfd\\xe6\\xf8\\xf8\\'\\x10\\x07\\x1a\\xd3\\x13\\x02\\n\\x07\\x0b\\xfd\\xf5\\n\\x03\\xfa\\x0b\\xeb\\xde\\xf2\\t\\x13\\x02\\x01\\xf9\\x05\\xf1\\xdb\\x06\\x17\\xe9\\xf2\\xfc\\xfe\\x07\\xea\\xf2\\xff\\xe2\\x14\\x0f!\\x07\\xfa\\xed\\xf1\\xfa\\x1d\\x12\\x16\\x0c\\x02\\xfa\\t\\xf9\\x08\\x0f\\x14\"\\t\\x0b\\xfe\\xf4\\xe2\\xaf\\xdd\\xf7\\n2\\xfd\\xea\\xdf\\x06\\xec\\xf1\\xf2\\xe7\\x01\\x0e\\x16\\x00\\x03\\xec\\xe0\\xf6\\xf5\\x13\\x14\\xf7\\xf3\\xfd\\xf3\\xed\\x1e\\x1b\\x11\\xd9\\xe2\\x03\\xff\\xf8\\x05\\r\\x19\\xfb\\x0b\\r\\t\\xfc\\xe8\\xf3\\xee\\x13\\x13\\xe2\\x1c\\xfb\\x13\\xd4\\xd2\\x00\\xdc\\xed\\x18\\x11\\xee\\xd9\\xd0\\xf1\\x189\\x0f\\x01 \\xd7\\xd3\\xc3\\xd2\\xdd\\xea\\xfe\\xf7\\x13\\x03\\x03\\xea\\xf2\\xe5\\xf7\\x16\\x1d,\\x1f\\xf8\"\\x01\\xea\\x1f/)\\xc4\\xcb(\\t\\xf5\\x03.&\\x01\\x037\\x0e\\x02\\xf1\\xe8\\xd3\\xf0?\\xe6\\xe8\\xcd\\xe6\\xa3\\xac\\x0b\\xde\\xe5\\xef\\xda\\xdd\\xc5\\xb7\\x08\\xff8\\x01\\xea\\xef\\x16\\xe6\\xef\\xd4\\xff\\xfe\\xff\\x04\\xf2\\xec\\xe8\\xfa\\r\\x15\\x0c\\n\\xe0\\xe4\\x0b#\\x0c\\x11\\r\\xdd\\xe7\\x10\\x1a\\xf8\\xec\\x0e\\xff\\xf4\\x00\\x0f\\xf5\\xf1\\x0b\\x1f\\xfd\\x18\\x1b\\xeb\\xe5\\t\\x16\\xf9\\xfb\\x1d%\\x11\\x00\\x0b\\xed\\xe7\\x05\\x1d\\x05\\xf8\\xeb\\xe8\\xce\\xfc\\xed;$\\xf0\\xd6\\xe5\\xf4\\xfb\\xfa\\xff\\x08\\x04\\xf1\\xfd\\xf8\\xf8\\xef\\xfb\\x14\\x10\\xf2\\x04\\x08\\xf6\\xec\\x13*\\xf3\\xdf\\x01\\x03\\x0f\\x10\\xff\"\\x04\\xfb\\xf1\\x0c\\x04\\x05\\xfc\\x0f\\x01\\x00\\xea&\\x10\\x03\\xe2\\xdf\\xdb\\xc8\\xd8\\x16\\x04\\xf7\\x01\\xd6\\xa2\\xbb\\xde\\x18\\x12\\x0c\\x02\\n\\x04\\xff\\xef\\x01\\xfc\\t\\x11\\xfc\\xf6\\xf0\\xf4\\xf3\\xf8\\x1b\\x03\\x01\\xeb\\xf9\\r\\xf6\\xf8\\x1a\\xfb\\n\\xee\\x1f\\x03\\xed\\xeb\\x0b\\xf1\\xe7\\x02\\x0e\\x10\\t\\xe1\\xf9#\\t\\xfb\\x17\\x19\\x00\\xd5\\x17\\x17\\t\\xfe\\xfb\\x10\\x00\\xf9\\x02\\xfe\\xe3\\xc4\\xb5\\xd4\\xff\\x11\\x13\\xf2\\x01\\xfb\\xf0\\xf9\\x05\\x00\\xf7\\x10\\xfd\\x08\\x02\\r\\x06\\x06\\x05\\xf3\\xfe\\xfc\\xfd\\xf7\\xf6\\x0b\\x05\\xea\\t\\x12\\x00\\x13\\x07\\xfc\\xed\\xf1\\x05\\x11\\x0f\\x17\\x0c\\x01\\xf5\\xf6\\xf8\\xfe\\x01\\xf9\\xfa\\xfa\\xf6\\xed\\xea\\xf4\\xef\\xfc\\xf0\\xf8\\xf3\\xf1\\xfd\\xfe\\x0c\\x15\\x15\\xf8\\xf9\\x1c\\xf6\\x06\\xfc\\x11\\x00\\x02\\x01\\xf4\\xdb\\xed\\xf3\\x01\\xee\\xef\\xd8\\xde\\xf3\\xee\\xd9\\x19\\xfa\\xd7\\xef\\x04\\xff\\xe3\\xdf6\\xe5\\xea\\xde\\xe4\\xe8\\xfd\\x03\\x08\\xc3\\xf3\\x0f\\xf2\\xf3\\xf5!\\xf7\\xce\\xfd\\xf8\\xfd\\xdf\\x07\\xf9\\xeb\\xd5\\xf1\\x01\\x0c\\x0c\\xfd\\xec\\x1a\\xcc\\xbb\\xe4\\x06\\x14\\r\\x18\\x10\\x08\\xf3\\xf3\\xed\\x01\\x0b\\xfa\\xfd\\xf0\\xe3\\x05\\x19\\x02\\xf3\\xfe\\xf6\\xf1\\xd0\\xf8\\x07\\xdb\\xea\\x01\\xe5\\x08\\xf3\\x07\\xf7\\xc8\\x16\\x16\\x01\\xf9\\xfe\\xed\\xe8\\xeb\\x12\\x15\\x10\\x1d\\xf9\\xf2\\xfd\\x0e\\x00\\x1b\\x08\\x04\\x0b\\xf1\\x01\\x01\\xca\\xc7\\xce\\xf1\\n\\np\\xff\\xff\\xfft\\xff\\xff\\xff\\x0f\\x00\\x00\\x00MLIR Converted.\\x00\\x01\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x18\\x00\\x14\\x00\\x10\\x00\\x0c\\x00\\x08\\x00\\x04\\x00\\x0e\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\xf4\\x01\\x00\\x00\\xf8\\x01\\x00\\x00\\xfc\\x01\\x00\\x00\\x04\\x00\\x00\\x00main\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x9c\\x01\\x00\\x00T\\x01\\x00\\x00\\x1c\\x01\\x00\\x00\\xe4\\x00\\x00\\x00\\xac\\x00\\x00\\x00t\\x00\\x00\\x00<\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x92\\xfe\\xff\\xff\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x10\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x04\\x00\\x04\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x18\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x15\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\xc6\\xfe\\xff\\xff\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x10\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\xb6\\xfe\\xff\\xff\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x15\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\x13\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\xfa\\xfe\\xff\\xff\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x10\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\xea\\xfe\\xff\\xff\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x11\\x00\\x00\\x00.\\xff\\xff\\xff\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x10\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x1e\\xff\\xff\\xff\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\r\\x00\\x00\\x00\\x0e\\x00\\x00\\x00b\\xff\\xff\\xff\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x10\\x00\\x00\\x00\\x14\\x00\\x00\\x00R\\xff\\xff\\xff\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\t\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x96\\xff\\xff\\xff\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x10\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x86\\xff\\xff\\xff\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\xca\\xff\\xff\\xff\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x10\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\xba\\xff\\xff\\xff\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x16\\x00\\x00\\x00\\x10\\x00\\x0c\\x00\\x0b\\x00\\x04\\x00\\x0e\\x00\\x00\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x18\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x08\\x00\\x07\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x18\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x00\\x00\\x00P\\r\\x00\\x00\\x98\\x0c\\x00\\x004\\x0c\\x00\\x00\\x88\\x0b\\x00\\x00\\xe4\\n\\x00\\x00\\x88\\n\\x00\\x00\\xdc\\t\\x00\\x008\\t\\x00\\x00\\xdc\\x08\\x00\\x000\\x08\\x00\\x00\\x8c\\x07\\x00\\x000\\x07\\x00\\x00\\x84\\x06\\x00\\x00\\xe0\\x05\\x00\\x00\\x84\\x05\\x00\\x00\\xd8\\x04\\x00\\x004\\x04\\x00\\x00\\xd8\\x03\\x00\\x00,\\x03\\x00\\x00\\x88\\x02\\x00\\x00,\\x02\\x00\\x00\\x80\\x01\\x00\\x00\\xdc\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x04\\x00\\x00\\x00(\\xf3\\xff\\xff\\x18\\x00\\x00\\x00 \\x00\\x00\\x00@\\x00\\x00\\x00\\x19\\x00\\x00\\x00\\x00\\x00\\x00\\tT\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff@\\x00\\x00\\x00\\x14\\xf3\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xa8\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xd7\\x92\\xc5;\\x19\\x00\\x00\\x00StatefulPartitionedCall:0\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00@\\x00\\x00\\x00N\\xf4\\xff\\xff\\x14\\x00\\x00\\x004\\x00\\x00\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x02<\\x00\\x00\\x00|\\xf3\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x8d\\xc5\\xc98\\x0c\\x00\\x00\\x00dense_7/bias\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00@\\x00\\x00\\x00\\xa6\\xf4\\xff\\xff\\x14\\x00\\x00\\x000\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x80\\x00\\x00\\x00\\xd4\\xf3\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x87\\xf1\\x16<U\\x00\\x00\\x00model/quant_dense_7/MatMul;model/quant_dense_7/LastValueQuant/FakeQuantWithMinMaxVars\\x00\\x00\\x00\\x02\\x00\\x00\\x00@\\x00\\x00\\x00 \\x00\\x00\\x00\\x98\\xf4\\xff\\xff\\x18\\x00\\x00\\x00 \\x00\\x00\\x00<\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x84\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff \\x00\\x00\\x00\\x84\\xf4\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x00\\x00\\x001\\x1a+<O\\x00\\x00\\x00model/quant_dense_6/MatMul;model/quant_dense_6/Relu;model/quant_dense_6/BiasAdd\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00 \\x00\\x00\\x00\\xee\\xf5\\xff\\xff\\x14\\x00\\x00\\x004\\x00\\x00\\x00\\x15\\x00\\x00\\x00\\x00\\x00\\x00\\x02<\\x00\\x00\\x00\\x1c\\xf5\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xa1\\x1al9\\x0c\\x00\\x00\\x00dense_6/bias\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00 \\x00\\x00\\x00F\\xf6\\xff\\xff\\x14\\x00\\x00\\x000\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x80\\x00\\x00\\x00t\\xf5\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xcb\\xc3\\x87<U\\x00\\x00\\x00model/quant_dense_6/MatMul;model/quant_dense_6/LastValueQuant/FakeQuantWithMinMaxVars\\x00\\x00\\x00\\x02\\x00\\x00\\x00 \\x00\\x00\\x00 \\x00\\x00\\x008\\xf6\\xff\\xff\\x18\\x00\\x00\\x00 \\x00\\x00\\x00<\\x00\\x00\\x00\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x84\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff \\x00\\x00\\x00$\\xf6\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x00\\x00\\x00\\xbb\\x99^<O\\x00\\x00\\x00model/quant_dense_5/MatMul;model/quant_dense_5/Relu;model/quant_dense_5/BiasAdd\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00 \\x00\\x00\\x00\\x8e\\xf7\\xff\\xff\\x14\\x00\\x00\\x004\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\x00\\x00\\x00\\x02<\\x00\\x00\\x00\\xbc\\xf6\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x9cf\\x869\\x0c\\x00\\x00\\x00dense_5/bias\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00 \\x00\\x00\\x00\\xe6\\xf7\\xff\\xff\\x14\\x00\\x00\\x000\\x00\\x00\\x00\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x80\\x00\\x00\\x00\\x14\\xf7\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xcb6\\xa2<U\\x00\\x00\\x00model/quant_dense_5/MatMul;model/quant_dense_5/LastValueQuant/FakeQuantWithMinMaxVars\\x00\\x00\\x00\\x02\\x00\\x00\\x00 \\x00\\x00\\x00\\x10\\x00\\x00\\x00\\xd8\\xf7\\xff\\xff\\x18\\x00\\x00\\x00 \\x00\\x00\\x00<\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x84\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x10\\x00\\x00\\x00\\xc4\\xf7\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x00\\x00\\x004\\x1bT<O\\x00\\x00\\x00model/quant_dense_4/MatMul;model/quant_dense_4/Relu;model/quant_dense_4/BiasAdd\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00.\\xf9\\xff\\xff\\x14\\x00\\x00\\x004\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x02<\\x00\\x00\\x00\\\\\\xf8\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xd8\\x16\\xe38\\x0c\\x00\\x00\\x00dense_4/bias\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x86\\xf9\\xff\\xff\\x14\\x00\\x00\\x000\\x00\\x00\\x00\\x0e\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x80\\x00\\x00\\x00\\xb4\\xf8\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1c\\x15\\xd8;U\\x00\\x00\\x00model/quant_dense_4/MatMul;model/quant_dense_4/LastValueQuant/FakeQuantWithMinMaxVars\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x02\\x00\\x00\\x00x\\xf9\\xff\\xff\\x18\\x00\\x00\\x00 \\x00\\x00\\x00<\\x00\\x00\\x00\\r\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x84\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x02\\x00\\x00\\x00d\\xf9\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x00\\x00\\x00\"\\x85\\x86<O\\x00\\x00\\x00model/quant_dense_3/MatMul;model/quant_dense_3/Relu;model/quant_dense_3/BiasAdd\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xce\\xfa\\xff\\xff\\x14\\x00\\x00\\x004\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x02<\\x00\\x00\\x00\\xfc\\xf9\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00%\\xe0\\x199\\x0c\\x00\\x00\\x00dense_3/bias\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00&\\xfb\\xff\\xff\\x14\\x00\\x00\\x000\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x80\\x00\\x00\\x00T\\xfa\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x94Q\\xb0;U\\x00\\x00\\x00model/quant_dense_3/MatMul;model/quant_dense_3/LastValueQuant/FakeQuantWithMinMaxVars\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x18\\xfb\\xff\\xff\\x18\\x00\\x00\\x00 \\x00\\x00\\x00<\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x84\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x10\\x00\\x00\\x00\\x04\\xfb\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x00\\x00\\x00\\x1dj\\xdf<O\\x00\\x00\\x00model/quant_dense_2/MatMul;model/quant_dense_2/Relu;model/quant_dense_2/BiasAdd\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00n\\xfc\\xff\\xff\\x14\\x00\\x00\\x004\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x02<\\x00\\x00\\x00\\x9c\\xfb\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x007\\xff\\x93:\\x0c\\x00\\x00\\x00dense_2/bias\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\xc6\\xfc\\xff\\xff\\x14\\x00\\x00\\x000\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x80\\x00\\x00\\x00\\xf4\\xfb\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00=B=U\\x00\\x00\\x00model/quant_dense_2/MatMul;model/quant_dense_2/LastValueQuant/FakeQuantWithMinMaxVars\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x10\\x00\\x00\\x00 \\x00\\x00\\x00\\xb8\\xfc\\xff\\xff\\x18\\x00\\x00\\x00 \\x00\\x00\\x00<\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x84\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff \\x00\\x00\\x00\\xa4\\xfc\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x00\\x00\\x00,\\x0e\\xc3<O\\x00\\x00\\x00model/quant_dense_1/MatMul;model/quant_dense_1/Relu;model/quant_dense_1/BiasAdd\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00 \\x00\\x00\\x00\\x0e\\xfe\\xff\\xff\\x14\\x00\\x00\\x004\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x02<\\x00\\x00\\x00<\\xfd\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x007,\\x0e:\\x0c\\x00\\x00\\x00dense_1/bias\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00 \\x00\\x00\\x00f\\xfe\\xff\\xff\\x14\\x00\\x00\\x000\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x80\\x00\\x00\\x00\\x94\\xfd\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00x\\xe70=U\\x00\\x00\\x00model/quant_dense_1/MatMul;model/quant_dense_1/LastValueQuant/FakeQuantWithMinMaxVars\\x00\\x00\\x00\\x02\\x00\\x00\\x00 \\x00\\x00\\x00@\\x00\\x00\\x00X\\xfe\\xff\\xff\\x18\\x00\\x00\\x00 \\x00\\x00\\x00@\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x84\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff@\\x00\\x00\\x00D\\xfe\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00^\\xbdM<I\\x00\\x00\\x00model/quant_dense/MatMul;model/quant_dense/Relu;model/quant_dense/BiasAdd\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00@\\x00\\x00\\x00\\xae\\xff\\xff\\xff\\x14\\x00\\x00\\x000\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x024\\x00\\x00\\x00\\xdc\\xfe\\xff\\xff\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x16\\xc4\\x198\\n\\x00\\x00\\x00dense/bias\\x00\\x00\\x01\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x18\\x00\\x14\\x00\\x13\\x00\\x0c\\x00\\x08\\x00\\x04\\x00\\x0e\\x00\\x00\\x00\\x14\\x00\\x00\\x004\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x80\\x00\\x00\\x00<\\xff\\xff\\xff\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x16\\xc4\\x19<Q\\x00\\x00\\x00model/quant_dense/MatMul;model/quant_dense/LastValueQuant/FakeQuantWithMinMaxVars\\x00\\x00\\x00\\x02\\x00\\x00\\x00@\\x00\\x00\\x00@\\x00\\x00\\x00\\x14\\x00\\x1c\\x00\\x18\\x00\\x17\\x00\\x10\\x00\\x0c\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x14\\x00\\x00\\x00\\x18\\x00\\x00\\x00,\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\t\\\\\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff@\\x00\\x00\\x00\\x0c\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x04\\x00\\x0c\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x00\\x00\\x00\\x00\\x00\\x80;\\x19\\x00\\x00\\x00serving_default_input_1:0\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00@\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x0c\\x00\\x10\\x00\\x0f\\x00\\x00\\x00\\x08\\x00\\x04\\x00\\x0c\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\t'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_obj.quantized_tflite_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone and fine-tune pre-trained model with quantization aware training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_model_optimization as tfmot\n",
    "# model = model_obj.model\n",
    "# quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# # q_aware stands for for quantization aware.\n",
    "# q_aware_model = quantize_model(model)\n",
    "\n",
    "# # `quantize_model` requires a recompile.\n",
    "# # q_aware_model.compile(optimizer='adam',\n",
    "# #               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "# #               metrics=['accuracy'])\n",
    "# q_aware_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# q_aware_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model against baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.x_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_subset = model_obj.x_train[0:1000] # out of 60000\n",
    "train_labels_subset = model_obj.x_train[0:1000]\n",
    "\n",
    "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_accuracy = model.evaluate(\n",
    "    model_obj.x_test, model_obj.x_test, verbose=0)\n",
    "\n",
    "q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   model_obj.x_test, model_obj.x_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create quantized model for TFLite backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, you have an actually quantized model with int8 weights and uint8 activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "q_aware_model = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See persistence of accuracy from TF to TFLite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to evaluate the TF Lite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter, x_test, n=6):\n",
    "#   input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "#   output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "#   # Run predictions on every image in the \"test\" dataset.\n",
    "#   prediction_digits = []\n",
    "#   for i, test_image in enumerate(test_images):\n",
    "#     if i % 1000 == 0:\n",
    "#       print('Evaluated on {n} results so far.'.format(n=i))\n",
    "#     # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "#     # the model's input data format.\n",
    "#     test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "#     interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "#     # Run inference.\n",
    "#     interpreter.invoke()\n",
    "\n",
    "#     # Post-processing: remove batch dimension and find the digit with highest\n",
    "#     # probability.\n",
    "#     output = interpreter.tensor(output_index)\n",
    "#     digit = np.argmax(output()[0])\n",
    "#     prediction_digits.append(digit)\n",
    "\n",
    "#   print('\\n')\n",
    "#   # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "#   prediction_digits = np.array(prediction_digits)\n",
    "#   accuracy = (prediction_digits == test_labels).mean()\n",
    "#   return accuracy\n",
    "\n",
    "    quantized_model_predictions = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Prepare input data\n",
    "        # input_data = np.array(\n",
    "        #     [x_test[i]*(2**(BIT_WIDTH-1))], dtype=np.int8)\n",
    "        # input_data = np.array([(x_test[i] - input_min) / (input_max - input_min) * (2 ** (BIT_WIDTH - 1))], dtype=np.int8)  \n",
    "        input_data = np.expand_dims(x_test[i], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "        interpreter.set_tensor(\n",
    "            input_details[0]['index'], input_data)\n",
    "        # print(f\"input_data: {input_data}\")\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get output\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        # output_data = interpreter.get_tensor(\n",
    "        #     output_details[0]['index']) / (2 ** (BIT_WIDTH - 1))\n",
    "        # output_data = output_data * (input_max - input_min) / (2 ** (BIT_WIDTH - 1)) + input_min\n",
    "        # output_data = output_data / (2 ** (BIT_WIDTH - 1))\n",
    "        # print(f\"output_data: {output_data}\")\n",
    "        quantized_model_predictions.append(output_data)\n",
    "    accuracy = (input_data == quantized_model_predictions).mean()\n",
    "    \n",
    "    # quantized_model_predictions = quantized_model_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=q_aware_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# test_accuracy = evaluate_model(interpreter, test_images=model_obj.x_test, test_labels=model_obj.x_test)\n",
    "test_accuracy = evaluate_model(interpreter, model_obj.x_test, n=6)\n",
    "\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.q_aware_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_obj.q_aware_model.layers:\n",
    "  if hasattr(layer, 'quantize_config'):\n",
    "    for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "        quantized_and_dequantized = quantizer(weight, training=False, weights=quantizer_vars)\n",
    "        min_var = quantizer_vars['min_var']\n",
    "        max_var = quantizer_vars['max_var']\n",
    "        print(quantized_and_dequantized*(2**(BIT_WIDTH)))\n",
    "        # quantized = dequantize(quantized_and_dequantized, min_var, max_var, quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.quantized_tflite_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.interpreter.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.plot_float_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = model_obj\n",
    "# plot_quantized_model(obj, n=6)\n",
    "model_obj.plot_quantized_model(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See 4x smaller model from quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create a float TFLite model and then see that the quantized TFLite model\n",
    "is 4x smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model_obj.model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(model_obj.quantized_tflite_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_converter.get_tensor_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b75f63a51ab1e44c10e89cf3b718812d9c5e2447d39cb402b946ba7653bfcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
