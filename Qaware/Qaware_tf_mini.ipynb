{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIT_WIDTH = 8\n",
    "EPOCHS = 1\n",
    "Q_EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "MODEL_NAME = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=5, flat=True)\n",
    "data_zoom = mnist.MNISTData(size_initial=20, size_final=8, color_depth=8, flat=True)\n",
    "# todo: \n",
    "data_zoom.x_train = data_zoom.x_train*(2**(BIT_WIDTH))\n",
    "data_zoom.x_test = data_zoom.x_test*(2**(BIT_WIDTH))\n",
    "\n",
    "# x_train= data_zoom.x_train\n",
    "# y_train= data_zoom.y_train\n",
    "# x_test= data_zoom.x_test\n",
    "# y_test= data_zoom.y_test\n",
    "\n",
    "ax = plt.subplot(1, 1 , 1)\n",
    "plt.imshow(data_zoom.x_train[0].reshape(size_final,size_final), cmap='gray_r')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,160\n",
      "Trainable params: 4,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from callbacks import all_callbacks\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import os \n",
    "# import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=8, flat=True)\n",
    "\n",
    "class QAutoencoder:\n",
    "    def __init__(self, data: mnist.MNISTData, bit_width=8, EPOCHS=1, Q_EPOCHS=1, model_name='model'):\n",
    "        self.x_train = data.x_train\n",
    "        self.y_train = data.y_train\n",
    "        self.x_test = data.x_test\n",
    "        self.y_test = data.y_test\n",
    "        self.input_min = np.min(data.x_train)\n",
    "        self.input_max = np.max(data.x_train)\n",
    "        self.input_shape = (data.x_train.shape[-1],)\n",
    "        self.BIT_WIDTH = bit_width\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.Q_EPOCHS = Q_EPOCHS\n",
    "        self.MODEL_NAME = model_name\n",
    "        self.model = self.autoencoder_model_gen()\n",
    "        self.history = None\n",
    "        self.loss = None\n",
    "        self.float_model_predictions = None\n",
    "        self.path_to_model =    None\n",
    "\n",
    "        self.path_to_quantized_model = f'model/QAE_model{self.BIT_WIDTH}bits_mini/KERAS_check_best_model_mini.model'\n",
    "        self.q_aware_model = None\n",
    "        self.quantized_tflite_model = None\n",
    "        self.interpreter = None\n",
    "        self.input_details = None\n",
    "        self.output_details = None\n",
    "        self.quantized_model_predictions = None\n",
    "\n",
    "    def autoencoder_model_gen(self):\n",
    "\n",
    "        # Encoder\n",
    "        # encoder_input = Input(shape=self.input_shape)\n",
    "        # encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "        # encoder_l2 = Dense(32, activation='relu')(encoder_l1)\n",
    "        # encoder_l3 = Dense(16, activation='relu')(encoder_l2)\n",
    "        # encoder_output = Dense(2, activation='relu')(encoder_l3)\n",
    "        \n",
    "        # # Decoder\n",
    "        # decoder_l1 = Dense(16, activation='relu')(encoder_output)\n",
    "        # decoder_l2 = Dense(32, activation='relu')(decoder_l1)\n",
    "        # decoder_l3 = Dense(32, activation='relu')(decoder_l2)\n",
    "        \n",
    "        encoder_input = Input(shape=self.input_shape)\n",
    "        decoder_output = Dense(self.x_train.shape[-1], activation='linear')(encoder_input)\n",
    "\n",
    "\n",
    "        # # Encoder\n",
    "        # encoder_input = Input(shape=self.input_shape)\n",
    "        # encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "        # # encoder_l2 = Dense(50, activation='relu')(encoder_l1)\n",
    "        # # encoder_l3 = Dense(32, activation='relu')(encoder_l2)\n",
    "        # # encoder_l4 = Dense(16, activation='relu')(encoder_l3)\n",
    "        # # encoder_l5 = Dense(8, activation='relu')(encoder_l4)\n",
    "        # encoder_l6 = Dense(4, activation='relu')(encoder_l1)\n",
    "        # encoder_output = Dense(2, activation='relu')(encoder_l6)\n",
    "\n",
    "        # # Decoder\n",
    "        # decoder_l1 = Dense(16, activation='relu')(encoder_output)\n",
    "        # # decoder_l2 = Dense(32, activation='relu')(decoder_l1)\n",
    "        # # decoder_l3 = Dense(32, activation='relu')(decoder_l2)\n",
    "        # # decoder_output = Dense(y_train.shape[-1], activation='sigmoid')(decoder_l3) # classifier\n",
    "        # # decoder_output = Dense(\n",
    "        # #     self.x_train.shape[-1], activation='sigmoid')(decoder_l3)  # autoencoder\n",
    "        # # decoder_output = Dense(self.x_train.shape[-1], activation='linear')(decoder_l3)\n",
    "        # decoder_output = Dense(self.x_train.shape[-1], activation='linear')(decoder_l1)\n",
    "\n",
    "\n",
    "        # Model\n",
    "        model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "        # refactor the code above to use the functional AP\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # model.compile(optimizer='adam', loss='binary_crossentropy') # classifier\n",
    "        return model\n",
    "\n",
    "    def fit_data(self, batch_size=256, epochs=Q_EPOCHS):\n",
    "        \"\"\"Write the fit function for the autoencoder. \n",
    "        Storing the fit history in self.history to be able to plot the fitting scores.\"\"\"\n",
    "\n",
    "        callbacks = all_callbacks(stop_patience=1000,\n",
    "                                  lr_factor=0.5,\n",
    "                                  lr_patience=10,\n",
    "                                  lr_epsilon=0.000001,\n",
    "                                  # min_delta=0.000001,\n",
    "                                  lr_cooldown=2,\n",
    "                                  lr_minimum=0.0000001,\n",
    "                                #   outputDir=f'model/QAE_model{self.BIT_WIDTH}bits/callbacks')\n",
    "        # callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "                                outputDir=f'model/callbacks_mini')\n",
    "\n",
    "        self.history = self.model.fit(self.x_train, self.x_train,\n",
    "                                      validation_data=(\n",
    "                                          self.x_test, self.x_test),\n",
    "                                      batch_size=batch_size, epochs=epochs,\n",
    "                                      shuffle=True, callbacks=callbacks.callbacks)\n",
    "        # self.model = strip_pruning(self.model)\n",
    "        self.loss = self.model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "        path_to_model =    f'model/model_{self.loss}loss/KERAS_check_best_model_mini.model'\n",
    "        self.model.save(\n",
    "            # f'model/QAE_model{self.BIT_WIDTH}bits/KERAS_check_best_model.h5')\n",
    "           path_to_model)\n",
    "        self.history = self.history.history\n",
    "        self.convert_to_Q_aware()\n",
    "\n",
    "    def plot_float_model(self, n=6):\n",
    "        \"\"\"Plot the float model\"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        self.float_model_predictions = self.model.predict(self.x_test)\n",
    "        self.plot_model_predictions(\n",
    "            n,\n",
    "            self.float_model_predictions,\n",
    "            './images/float_model/reconstructed images {model_name}_mini.png',\n",
    "        )\n",
    "\n",
    "    def representative_dataset(self):\n",
    "        for data in self.x_train:\n",
    "            # Scale the data using min and max values\n",
    "            scaled_data = (data - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))\n",
    "            yield [np.array([scaled_data], dtype=np.float32)]\n",
    "\n",
    "    def convert_to_Q_aware(self):\n",
    "\n",
    "        # converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n",
    "        # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # converter.representative_dataset = self.representative_dataset\n",
    "        # converter.target_spec.supported_ops = [\n",
    "        #     tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        # converter.inference_input_type = tf.int8\n",
    "        # converter.inference_output_type = tf.int8\n",
    "\n",
    "        # self.q_aware_model = converter.convert()\n",
    "\n",
    "        # # Load the quantized model\n",
    "        # self.interpreter = tf.lite.Interpreter(\n",
    "        #     model_path='quantized_model.tflite')\n",
    "        # self.interpreter.allocate_tensors()\n",
    "        # ------------\n",
    "        quantize_model = tfmot.quantization.keras.quantize_model\n",
    "        self.q_aware_model = quantize_model(self.model)\n",
    "        self.q_aware_model.compile(optimizer='adam', loss='mse')\n",
    "        self.fit_data_Q_aware()\n",
    "        # loss = self.q_aware_model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "\n",
    "        # # Save the quantized model\n",
    "        # with open(f'quantized_model.tflite', 'wb') as f:\n",
    "        #     f.write(self.q_aware_model)\n",
    "\n",
    "        self.convert_to_tflite()\n",
    "\n",
    "    def fit_data_Q_aware(self, batch_size=256, epochs=Q_EPOCHS):\n",
    "        \"\"\"Write the fit function for the autoencoder. \n",
    "        Storing the fit history in self.history to be able to plot the fitting scores.\"\"\"\n",
    "\n",
    "        callbacks = all_callbacks(stop_patience=1000,\n",
    "                                  lr_factor=0.5,\n",
    "                                  lr_patience=10,\n",
    "                                  lr_epsilon=0.000001,\n",
    "                                  # min_delta=0.000001,\n",
    "                                  lr_cooldown=2,\n",
    "                                  lr_minimum=0.0000001,\n",
    "                                  outputDir=f'model/QAE_model{self.BIT_WIDTH}bits_mini/callbacks')\n",
    "        # callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "\n",
    "        self.history_Q_aware = self.q_aware_model.fit(self.x_train, self.x_train,\n",
    "                                      validation_data=(\n",
    "                                          self.x_test, self.x_test),\n",
    "                                      batch_size=batch_size, epochs=epochs,\n",
    "                                      shuffle=True, callbacks=callbacks.callbacks)\n",
    "        # self.model = strip_pruning(self.model)\n",
    "        \n",
    "        self.q_aware_model.save(\n",
    "            # f'model/QAE_model{self.BIT_WIDTH}bits/KERAS_check_best_model.h5')\n",
    "            self.path_to_quantized_model)\n",
    "        self.history_Q_aware = self.history_Q_aware.history\n",
    "        self.loss = self.q_aware_model.evaluate(self.x_test, self.x_test, verbose=0)\n",
    "        # self.convert_to_Q_aware()    \n",
    "\n",
    "    def convert_to_tflite(self):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(self.q_aware_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = self.representative_dataset\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "        self.quantized_tflite_model = converter.convert()\n",
    "\n",
    "        # Save the TensorFlow Lite model\n",
    "        with open(\"model_obj_tflite_model_mini.tflite\", \"wb\") as file:\n",
    "            file.write(self.quantized_tflite_model)\n",
    "\n",
    "        # Load the quantized model\n",
    "        # self.interpreter = tf.lite.Interpreter(model_path='quantized_model.tflite')\n",
    "        self.interpreter = tf.lite.Interpreter(model_content=self.quantized_tflite_model)\n",
    "\n",
    "        self.interpreter.allocate_tensors()\n",
    "\n",
    "        # Get input and output details\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        self.quantized_predictions()\n",
    "\n",
    "    def quantized_predictions(self, n=50):\n",
    "        quantized_model_predictions = []\n",
    "\n",
    "        for i in range(n):\n",
    "            # Prepare input data\n",
    "            # input_data = np.array(\n",
    "            #     [self.x_test[i]*(2**(self.BIT_WIDTH-1))], dtype=np.int8)\n",
    "            # input_data = np.array([(self.x_test[i] - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))], dtype=np.int8)  \n",
    "            input_data = np.array([(self.x_test[i] - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH))], dtype=np.int8)  \n",
    "\n",
    "            self.interpreter.set_tensor(\n",
    "                self.input_details[0]['index'], input_data)\n",
    "            # print(f\"input_data: {input_data}\")\n",
    "\n",
    "            # Run inference\n",
    "            self.interpreter.invoke()\n",
    "\n",
    "            # Get output\n",
    "            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "            output_data = self.interpreter.get_tensor(\n",
    "                # self.output_details[0]['index']) / (2 ** (self.BIT_WIDTH - 1))\n",
    "                self.output_details[0]['index'])\n",
    "            # output_data = output_data * (self.input_max - self.input_min) / (2 ** (self.BIT_WIDTH - 1)) + self.input_min\n",
    "            # output_data = output_data / (2 ** (self.BIT_WIDTH - 1))\n",
    "            quantized_model_predictions.append(output_data)\n",
    "            # print(f\"output_data: {output_data}\")\n",
    "        \n",
    "        self.quantized_model_predictions = quantized_model_predictions\n",
    "        # self.compute_mse()\n",
    "\n",
    "    def plot_quantized_model(self, n=6):\n",
    "        \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        self.plot_model_predictions(\n",
    "            n,\n",
    "            self.quantized_model_predictions,\n",
    "            './images/QAE/reconstructed images{model_name}_mini.png',\n",
    "        )\n",
    "\n",
    "    def plot_model_predictions(self, n, quantized_model_predictions, imgs_path):\n",
    "        img_size = int(np.sqrt(self.input_shape[0]))\n",
    "        for i in range(n):\n",
    "            ax = plt.subplot(2, n, i + 1)\n",
    "            self.plot_imgs(\n",
    "                self.x_test, i, img_size, ax\n",
    "            )\n",
    "            ax = plt.subplot(2, n, i + n + 1)\n",
    "            self.plot_imgs(\n",
    "                quantized_model_predictions, i, img_size, ax\n",
    "            )\n",
    "        if not os.path.exists(imgs_path):\n",
    "            os.makedirs(imgs_path)\n",
    "        plt.savefig(imgs_path.format(model_name=\" complete\"))\n",
    "        plt.show()\n",
    "\n",
    "    def plot_imgs(self, arg0, i, img_size, ax):\n",
    "        plt.imshow(arg0[i].reshape(img_size, img_size), cmap='gray_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    def compute_mse(self):\n",
    "        mse = mean_squared_error(self.float_model_predictions, self.quantized_model_predictions)\n",
    "        print(f'Mean Squared Error between floating point and quantized model predictions: {mse}')\n",
    "        self.mse = mse\n",
    "\n",
    "model_obj = QAutoencoder(data_zoom, bit_width=BIT_WIDTH, model_name=MODEL_NAME, EPOCHS=EPOCHS, Q_EPOCHS=Q_EPOCHS)\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32) # classifier\n",
    "model_obj.model.summary()\n",
    "# model.fit(x_train, x_train, epochs=EPOCHS, batch_size=BATCH_SIZE) #autoencoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "  1/235 [..............................] - ETA: 2:12 - loss: 20516.0957WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.\n",
      "228/235 [============================>.] - ETA: 0s - loss: 7802.3677\n",
      "***callbacks***\n",
      "saving losses to model/callbacks_mini\\losses.log\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3900.66602, saving model to model/callbacks_mini\\KERAS_check_best_model.model\n",
      "INFO:tensorflow:Assets written to: model/callbacks_mini\\KERAS_check_best_model.model\\assets\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3900.66602, saving model to model/callbacks_mini\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00001: saving model to model/callbacks_mini\\KERAS_check_model_last.model\n",
      "INFO:tensorflow:Assets written to: model/callbacks_mini\\KERAS_check_model_last.model\\assets\n",
      "\n",
      "Epoch 00001: saving model to model/callbacks_mini\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 7698.2793 - val_loss: 3900.6660 - lr: 0.0010\n",
      "INFO:tensorflow:Assets written to: model/model_3900.666015625loss/KERAS_check_best_model_mini.model\\assets\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "  1/235 [..............................] - ETA: 2:57 - loss: 11184.5840WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0040s). Check your callbacks.\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 7455.7334\n",
      "***callbacks***\n",
      "saving losses to model/QAE_model8bits_mini/callbacks\\losses.log\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4746.37646, saving model to model/QAE_model8bits_mini/callbacks\\KERAS_check_best_model.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits_mini/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits_mini/callbacks\\KERAS_check_best_model.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 4746.37646, saving model to model/QAE_model8bits_mini/callbacks\\KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00001: saving model to model/QAE_model8bits_mini/callbacks\\KERAS_check_model_last.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits_mini/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits_mini/callbacks\\KERAS_check_model_last.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to model/QAE_model8bits_mini/callbacks\\KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 7345.4736 - val_loss: 4746.3765 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits_mini/KERAS_check_best_model_mini.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/QAE_model8bits_mini/KERAS_check_best_model_mini.model\\assets\n",
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmpdkg3l1ws\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmpdkg3l1ws\\assets\n"
     ]
    }
   ],
   "source": [
    "model_obj.fit_data(epochs=EPOCHS)  # batch_size=BATCH_SIZE, epochs=EPOCHS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4746.3759765625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_obj.plot_float_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obj = model_obj\n",
    "# plot_quantized_model(obj, n=6)\n",
    "model_obj.plot_quantized_model(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\luisa\\\\OneDrive\\\\Documentos\\\\GitHub\\\\Autoencoder-for-FPGA\\\\Qaware\\\\model_obj_tf_model'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MODEL_DIR = os.path.abspath(\"model/model_obj_tf_model\")\n",
    "# ASSETS_DIR = os.path.join(MODEL_DIR, \"assets\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "# if not os.path.exists(ASSETS_DIR):\n",
    "#     os.makedirs(ASSETS_DIR)\n",
    "MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\luisa\\\\OneDrive\\\\Documentos\\\\GitHub\\\\Autoencoder-for-FPGA\\\\Qaware\\\\model_obj_tf_model\\\\tf_model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(MODEL_DIR, \"tf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "MINI = True\n",
    "\n",
    "if MINI:\n",
    "    mini = \"_mini\"\n",
    "else:\n",
    "    mini = \"\"\n",
    "\n",
    "def save_objects(model_obj):\n",
    "    # Save the Keras model\n",
    "    model_obj.model.save(f'model/model_obj_keras_model{mini}.h5')\n",
    "\n",
    "    # Save the .tflite model\n",
    "    with open(f\"model/model_obj_tflite_model{mini}.tflite\", \"wb\") as file:\n",
    "        file.write(model_obj.quantized_tflite_model)\n",
    "\n",
    "    # Save other attributes\n",
    "    attrs_to_save = {\n",
    "        'x_train': model_obj.x_train,\n",
    "        # 'y_train': model_obj.y_train,\n",
    "        'x_test': model_obj.x_test,\n",
    "        'y_test': model_obj.y_test,\n",
    "        'input_shape': model_obj.input_shape,\n",
    "        'BIT_WIDTH': model_obj.BIT_WIDTH,\n",
    "        'EPOCHS': model_obj.EPOCHS,\n",
    "        'Q_EPOCHS': model_obj.Q_EPOCHS,\n",
    "        'MODEL_NAME': model_obj.MODEL_NAME,\n",
    "        'history': model_obj.history,\n",
    "        'loss': model_obj.loss,\n",
    "        'float_model_predictions': model_obj.float_model_predictions,\n",
    "        'quantized_model_predictions': model_obj.quantized_model_predictions,\n",
    "        'input_min': model_obj.input_min,\n",
    "        'input_max': model_obj.input_max,\n",
    "        'path_to_model': model_obj.path_to_model,\n",
    "        'path_to_quantized_model': model_obj.path_to_quantized_model\n",
    "        # 'history_Q_aware': model_obj.history_Q_aware,\n",
    "        # 'mse': model_obj.mse,\n",
    "    }\n",
    "    with open(f'model/model_obj_attributes{mini}.pickle', 'wb') as f:\n",
    "        pickle.dump(attrs_to_save, f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the objects of model_obj\n",
    "save_objects(model_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_objects():\n",
    "    \n",
    "    # model_obj.model.save('model_obj_keras_model.h5')\n",
    "    # Load the TensorFlow model\n",
    "    loaded_model = tf.keras.models.load_model(os.path.join(MODEL_DIR, \"tf_model\"))\n",
    "\n",
    "    # Load the Keras model\n",
    "    loaded_keras_model = tf.keras.models.load_model('model/model_obj_keras_model.h5')\n",
    "\n",
    "    # Load the .tflite model\n",
    "    with open(\"model/model_obj_tflite_model.tflite\", \"rb\") as file:\n",
    "        loaded_tflite_model = file.read()\n",
    "\n",
    "    # Load other attributes\n",
    "    with open('model/model_obj_attributes.pickle', 'rb') as f:\n",
    "        loaded_attributes = pickle.load(f)\n",
    "    \n",
    "    # Create a new QAutoencoder object and set the attributes\n",
    "    loaded_model_obj = QAutoencoder(data_zoom, bit_width=loaded_attributes['BIT_WIDTH'], model_name=loaded_attributes['MODEL_NAME'],\n",
    "                                    EPOCHS=loaded_attributes['EPOCHS'], Q_EPOCHS=loaded_attributes['Q_EPOCHS'])\n",
    "    \n",
    "    loaded_model_obj.model = loaded_keras_model\n",
    "    loaded_model_obj.quantized_tflite_model = loaded_tflite_model\n",
    "    loaded_model_obj.input_shape = loaded_attributes['input_shape']\n",
    "    loaded_model_obj.x_train = loaded_attributes['x_train']\n",
    "    # loaded_model_obj.y_train = loaded_attributes['y_train']\n",
    "    loaded_model_obj.x_test = loaded_attributes['x_test']\n",
    "    loaded_model_obj.y_test = loaded_attributes['y_test']\n",
    "    loaded_model_obj.model = loaded_model\n",
    "    loaded_model_obj.history = loaded_attributes['history']\n",
    "    loaded_model_obj.loss = loaded_attributes['loss']\n",
    "    loaded_model_obj.float_model_predictions = loaded_attributes['float_model_predictions']\n",
    "    loaded_model_obj.quantized_model_predictions = loaded_attributes['quantized_model_predictions']\n",
    "    loaded_model_obj.interpreter = loaded_tflite_model\n",
    "    loaded_model_obj.input_min = loaded_attributes['input_min']\n",
    "    loaded_model_obj.input_max = loaded_attributes['input_max']\n",
    "    loaded_model_obj.path_to_model = loaded_attributes['path_to_model']\n",
    "    loaded_model_obj.path_to_quantized_model = loaded_attributes['path_to_quantized_model']\n",
    "    # loaded_model_obj.mse = loaded_attributes['mse']\n",
    "\n",
    "\n",
    "\n",
    "    # Recreate the TFLite interpreter\n",
    "    loaded_model_obj.interpreter = tf.lite.Interpreter(model_content=loaded_tflite_model)\n",
    "    loaded_model_obj.interpreter.allocate_tensors()\n",
    "    loaded_model_obj.input_details = loaded_model_obj.interpreter.get_input_details()\n",
    "    loaded_model_obj.output_details = loaded_model_obj.interpreter.get_output_details()\n",
    "    # loaded_model_obj.convert_to_Q_aware()\n",
    "\n",
    "    return loaded_model_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the objects and create a new model_obj\n",
    "loaded_model_obj = load_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_obj.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,562\n",
      "Trainable params: 10,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model_obj.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_obj.quantized_tflite_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone and fine-tune pre-trained model with quantization aware training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1c8c767b880>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_model_optimization as tfmot\n",
    "# model = model_obj.model\n",
    "# quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# # q_aware stands for for quantization aware.\n",
    "# q_aware_model = quantize_model(model)\n",
    "\n",
    "# # `quantize_model` requires a recompile.\n",
    "# # q_aware_model.compile(optimizer='adam',\n",
    "# #               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "# #               metrics=['accuracy'])\n",
    "# q_aware_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# q_aware_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model against baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.94140625, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.00390625, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj.x_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_subset = model_obj.x_train[0:1000] # out of 60000\n",
    "# train_labels_subset = model_obj.x_train[0:1000]\n",
    "\n",
    "# q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "#                   batch_size=500, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model_accuracy = model_obj.model.evaluate(\n",
    "#     model_obj.x_test, model_obj.x_test, verbose=0)\n",
    "\n",
    "# q_aware_model_accuracy = model_obj.q_aware_model.evaluate(\n",
    "#    model_obj.x_test, model_obj.x_test, verbose=0)\n",
    "\n",
    "# print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "# print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create quantized model for TFLite backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, you have an actually quantized model with int8 weights and uint8 activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model_obj.q_aware_model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# q_aware_model = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See persistence of accuracy from TF to TFLite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to evaluate the TF Lite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter, x_test, n=6):\n",
    "#   input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "#   output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "#   # Run predictions on every image in the \"test\" dataset.\n",
    "#   prediction_digits = []\n",
    "#   for i, test_image in enumerate(test_images):\n",
    "#     if i % 1000 == 0:\n",
    "#       print('Evaluated on {n} results so far.'.format(n=i))\n",
    "#     # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "#     # the model's input data format.\n",
    "#     test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "#     interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "#     # Run inference.\n",
    "#     interpreter.invoke()\n",
    "\n",
    "#     # Post-processing: remove batch dimension and find the digit with highest\n",
    "#     # probability.\n",
    "#     output = interpreter.tensor(output_index)\n",
    "#     digit = np.argmax(output()[0])\n",
    "#     prediction_digits.append(digit)\n",
    "\n",
    "#   print('\\n')\n",
    "#   # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "#   prediction_digits = np.array(prediction_digits)\n",
    "#   accuracy = (prediction_digits == test_labels).mean()\n",
    "#   return accuracy\n",
    "\n",
    "    quantized_model_predictions = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Prepare input data\n",
    "        # input_data = np.array(\n",
    "        #     [x_test[i]*(2**(BIT_WIDTH-1))], dtype=np.int8)\n",
    "        # input_data = np.array([(x_test[i] - input_min) / (input_max - input_min) * (2 ** (BIT_WIDTH - 1))], dtype=np.int8)  \n",
    "        input_data = np.expand_dims(x_test[i], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "        interpreter.set_tensor(\n",
    "            input_details[0]['index'], input_data)\n",
    "        # print(f\"input_data: {input_data}\")\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get output\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        # output_data = interpreter.get_tensor(\n",
    "        #     output_details[0]['index']) / (2 ** (BIT_WIDTH - 1))\n",
    "        # output_data = output_data * (input_max - input_min) / (2 ** (BIT_WIDTH - 1)) + input_min\n",
    "        # output_data = output_data / (2 ** (BIT_WIDTH - 1))\n",
    "        # print(f\"output_data: {output_data}\")\n",
    "        quantized_model_predictions.append(output_data)\n",
    "    accuracy = (input_data == quantized_model_predictions).mean()\n",
    "    \n",
    "    # quantized_model_predictions = quantized_model_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter = tf.lite.Interpreter(model_content=q_aware_model)\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # test_accuracy = evaluate_model(interpreter, test_images=model_obj.x_test, test_labels=model_obj.x_test)\n",
    "# test_accuracy = evaluate_model(interpreter, model_obj.x_test, n=6)\n",
    "\n",
    "# print('Quant TF test accuracy:', q_aware_model_accuracy)\n",
    "# print('Quant TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x1c8b21bc520>,\n",
       " <tensorflow_model_optimization.python.core.quantization.keras.quantize_layer.QuantizeLayer at 0x1c8b21bcc70>,\n",
       " <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapperV2 at 0x1c8af1ac880>,\n",
       " <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapperV2 at 0x1c8af254c10>,\n",
       " <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapperV2 at 0x1c8aeda0fd0>,\n",
       " <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapperV2 at 0x1c8b21bc400>,\n",
       " <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapperV2 at 0x1c8b1d82820>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj.q_aware_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-12.541695  -39.018612   48.773262  ... -12.541695   20.20607\n",
      "   60.618202 ]\n",
      " [ -9.754654  -13.238457    3.4838028 ...  13.238457  -49.470028\n",
      "   45.98623  ]\n",
      " [ -2.0902786 -18.812546  -46.682983  ...  27.870438  -58.52792\n",
      "   -4.877327 ]\n",
      " ...\n",
      " [ -7.6643677 -58.52792   -50.86355   ... -26.476913   48.0765\n",
      "   49.470024 ]\n",
      " [ -6.270851    6.967613   35.534813  ... -25.083393  -36.231567\n",
      "  -41.805656 ]\n",
      " [-50.86355    22.296349   59.92144   ... -24.386635  -41.805656\n",
      "   49.470024 ]], shape=(64, 64), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 34.464035  -55.272507  -42.26721   -28.611649 ]\n",
      " [ 57.873573   31.862976   51.37092   -13.005295 ]\n",
      " [ 24.710068   30.562447   -7.1529083 -48.11959  ]\n",
      " [-72.82965    56.573036   -5.202118   68.27781  ]\n",
      " [ -4.5518494  29.261917  -70.22859   -22.759266 ]\n",
      " [ 13.655563   34.464035  -39.66615    16.90689  ]\n",
      " [ 68.27781    -4.5518494 -68.2778     18.857681 ]\n",
      " [ 11.054504  -46.81906   -32.513237   47.469337 ]\n",
      " [ 30.562447   57.873573   40.966682  -73.47992  ]\n",
      " [-26.01059    81.283104  -52.671448  -59.824356 ]\n",
      " [-37.06509    76.080986  -12.355026   -7.803177 ]\n",
      " [ 35.764565   54.622246  -66.97727   -46.168797 ]\n",
      " [-16.906883  -27.961384    7.8031845 -78.68204  ]\n",
      " [  9.753975   18.20742    64.37621    41.61695  ]\n",
      " [ 17.557152   -9.103706   11.054504  -40.316414 ]\n",
      " [ 68.27781   -20.158203  -72.82965   -55.272507 ]\n",
      " [-68.2778    -42.917473  -26.660854  -52.671448 ]\n",
      " [ 23.409538  -32.513237   -9.103706  -67.62753  ]\n",
      " [-55.922768  -25.360325   22.109009   70.87887  ]\n",
      " [-53.32171    41.61695    -2.601059  -17.557144 ]\n",
      " [ 18.20742   -64.37621   -61.775154  -66.327    ]\n",
      " [ 42.26722   -48.11959    22.75927   -66.97727  ]\n",
      " [-76.08098   -44.868267  -37.715355   -7.803177 ]\n",
      " [-74.13018    42.91748     1.950798   63.07569  ]\n",
      " [ 72.17939   -14.305824   -1.9507904 -32.513237 ]\n",
      " [-35.114296  -43.567738   50.720665   -7.803177 ]\n",
      " [  5.2021255  57.873573  -20.808472  -14.305824 ]\n",
      " [ 53.321724  -45.51853   -48.769855   44.21801  ]\n",
      " [ 22.109009  -53.32171    20.80848   -30.562443 ]\n",
      " [  7.152916  -77.38151    46.1688     46.81907  ]\n",
      " [-36.414825    2.6010666  29.912186   71.52914  ]\n",
      " [ 25.360329   46.1688    -63.075684  -73.47992  ]\n",
      " [ 17.557152   44.86828    37.715363  -50.070385 ]\n",
      " [ 63.07569    73.47993   -16.256615   59.824364 ]\n",
      " [ 48.76986   -57.223297  -65.67674    23.409538 ]\n",
      " [ 65.02648    40.966682  -59.174095   47.469337 ]\n",
      " [-77.38151    45.518547    1.3005371   7.152916 ]\n",
      " [-13.005295   57.873573   48.76986   -49.42012  ]\n",
      " [ 18.857681  -55.272507  -68.2778     20.80848  ]\n",
      " [-77.38151    -9.103706  -44.868267   -3.25132  ]\n",
      " [-31.862972   32.513245  -23.40953    50.720665 ]\n",
      " [-57.223297   24.0598      5.2021255  31.862976 ]\n",
      " [-66.97727   -13.655556   28.611656   26.010597 ]\n",
      " [ 39.666153   22.109009   68.27781   -53.32171  ]\n",
      " [ 66.97727   -16.906883   70.2286     13.655563 ]\n",
      " [ 57.873573  -22.759266  -22.109001  -25.360325 ]\n",
      " [ -2.601059  -48.769855   -5.202118   11.054504 ]\n",
      " [-70.22859    68.92808   -42.26721   -28.611649 ]\n",
      " [-34.46403    81.283104   27.311127  -65.67674  ]\n",
      " [ 28.611656    7.8031845 -53.971977   29.912186 ]\n",
      " [-46.168797   52.021187  -16.906883   55.922783 ]\n",
      " [ 13.655563   46.81907    34.464035   51.37092  ]\n",
      " [-71.52912     1.950798   -5.202118  -26.01059  ]\n",
      " [ 14.305832  -14.956085  -57.223297   37.715363 ]\n",
      " [-18.857674   38.365623  -70.87886   -24.059795 ]\n",
      " [ 65.02648   -61.775154  -49.42012   -48.11959  ]\n",
      " [-36.414825   16.90689   -44.218002   65.02648  ]\n",
      " [-59.174095   82.583626  -71.52912    18.20742  ]\n",
      " [ 35.114304  -24.059795  -33.1635     25.360329 ]\n",
      " [ 70.2286     46.1688     -8.453438  -52.02118  ]\n",
      " [-51.370914   69.57833    -9.753967  -42.917473 ]\n",
      " [ 18.20742    52.671455  -52.671448   37.715363 ]\n",
      " [-60.474625   28.611656    4.551857  -16.256615 ]\n",
      " [-57.873566   71.52914   -61.124886  -68.2778   ]], shape=(64, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 187.649    -132.97961 ]\n",
      " [  81.265335 -175.8286  ]\n",
      " [ 101.951035  119.681656]\n",
      " [ 103.42857   152.18779 ]], shape=(4, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-121.90729     -6.360382  -104.94627     -5.3003235  122.96735\n",
      "    92.22551      2.1201172  -56.183365   -56.183365    68.90411\n",
      "   -49.822983   110.24658    114.48683     29.681763    73.14436\n",
      "   106.00633  ]\n",
      " [  86.925186   -80.56482     72.084305    57.243423    76.324554\n",
      "   -85.865135  -134.62805    -74.20444    -50.88305    -85.865135\n",
      "    -9.540573  -112.36672     23.321396    98.58589    -33.922035\n",
      "    -7.4204483]], shape=(2, 16), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-33.985977  -27.80671    14.418289  ...  52.52379    40.165253\n",
      "   60.762802 ]\n",
      " [-62.82257     0.          7.209152  ...  33.985977  -10.298782\n",
      "   19.567688 ]\n",
      " [ 21.627441  -47.374397   -3.0896378 ...  23.687195    1.0298767\n",
      "  -62.82257  ]\n",
      " ...\n",
      " [-16.47805     2.0597534 -46.34452   ...  60.762802   82.39024\n",
      "  -22.657318 ]\n",
      " [-26.776833   21.627441   41.19513   ... -45.314636  -14.418297\n",
      "  -23.687195 ]\n",
      " [  6.1792603 -60.76281   -41.19513   ... -46.34452   -44.28476\n",
      "   33.985977 ]], shape=(16, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for layer in model_obj.q_aware_model.layers:\n",
    "  if hasattr(layer, 'quantize_config'):\n",
    "    for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "        quantized_and_dequantized = quantizer(weight, training=False, weights=quantizer_vars)\n",
    "        min_var = quantizer_vars['min_var']\n",
    "        max_var = quantizer_vars['max_var']\n",
    "        print(quantized_and_dequantized*(2**(BIT_WIDTH)))\n",
    "        # quantized = dequantize(quantized_and_dequantized, min_var, max_var, quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_obj\u001b[39m.\u001b[39;49mquantized_tflite_model\u001b[39m.\u001b[39;49mlayers\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "model_obj.quantized_tflite_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.interpreter.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.plot_float_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = model_obj\n",
    "# plot_quantized_model(obj, n=6)\n",
    "model_obj.plot_quantized_model(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See 4x smaller model from quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create a float TFLite model and then see that the quantized TFLite model\n",
    "is 4x smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model_obj.model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(model_obj.quantized_tflite_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_converter.get_tensor_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b75f63a51ab1e44c10e89cf3b718812d9c5e2447d39cb402b946ba7653bfcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
